# ChatGPT Replacement - Mentat Enhancement

**Date**: 2025-11-22 06:08 AM
**Status**: in_progress (Phase 1-3 active, Phase 4-8 planned)

## The Idea

Stop paying for ChatGPT and GitHub Copilot. Keep Claude Code subscription (not ready to replicate). Redirect OpenAI/GitHub money to OpenRouter credits for frontier model access on-demand.

**Cost consolidation goal**:
- Cancel: ChatGPT Plus (~$20/mo), GitHub Copilot (~$10/mo)
- Keep: Claude Code subscription
- Add: OpenRouter credits (~$30/mo budget)

## Why It Matters

- Reduce subscription sprawl
- OpenAI web chat features can be replicated in Mentat
- OpenRouter provides access to multiple frontier models (Claude, GPT-4, Gemini, etc.) with pay-per-use
- Custom implementation = full control over features and data

## Current State (Mentat Chat)

- WebSocket-based real-time streaming
- sessionStorage only (current session, lost on refresh)
- Single conversation view
- Model selector (OpenRouter integration already exists!)
- RAG toggle for video content
- No conversation persistence
- No conversation history UI

## Implementation Phases

### Phase 1: Conversation History (MVP)
- Persist conversations to backend storage
- Auto-generate conversation titles (LLM summary of first message)
- Left sidebar with conversation list
- Search conversations (title + content)
- New chat button
- Delete/rename conversations

### Phase 2: Projects
- Group conversations into projects
- Per-project custom instructions (override global)
- Per-project file uploads (accessible to all project chats)
- Per-project memory (RAG scoped to project conversations)
- Ability to edit project settings from chat interface
- Context-aware: system knows it can modify project files/instructions

Reference: [ChatGPT Projects](https://help.openai.com/en/articles/10169521-projects-in-chatgpt)
- Pro: 40 files/project, Plus: 20 files
- Project-only memory toggle
- Custom instructions per project

### Phase 3: Canvas
- Right sidebar for document/code editing
- ChatGPT calls it "Canvas" - editable output area
- Direct editing without copy/paste
- Code-specific shortcuts: review, add logs, add comments, fix bugs, port language
- Writing shortcuts: adjust length, reading level, polish, etc.

Reference: [ChatGPT Canvas](https://help.openai.com/en/articles/9930697-what-is-the-canvas-feature-in-chatgpt-and-how-do-i-use-it)

### Phase 4: Writing Styles
- Style dropdown in chat header (Concise, Detailed, Formal, Technical, Creative)
- Style injection into system prompt
- Custom style text input option
- Persist style per conversation

### Phase 5: Global Memory
- Auto-extract preferences/facts from conversations
- Inject relevant memories into system prompt
- Memory management UI (view/edit/delete)
- Per-conversation "don't remember" toggle

### Phase 6: Web Search + Freedium
- Web search via existing flaresolverr MCP
- Freedium Docker service for Medium paywall bypass
- Chain: flaresolverr â†’ freedium for Cloudflare+paywall content
- Inline citation formatting in responses

### Phase 7: Code Execution Sandbox
- Docker-based Python sandbox with security limits
- Output capture: stdout, stderr, matplotlib plots, files
- Artifact-style results panel in frontend
- Security: no network, no host filesystem, timeout enforcement

### Phase 8: Image Generation (Later)
- API integration (DALL-E 3 or Stability AI)
- Image library sidebar
- Cost tracking per generation

**Full Phase 4-8 details**: See [phase4-8-implementation-context.md](phase4-8-implementation-context.md)

## Technical Considerations

### Storage Options (Phase 1)
- SQLite file (simple, no extra services)
- PostgreSQL (if we want full-text search, already have Docker)
- JSON files in compose/data/conversations/ (simplest, git-friendly for backup)

### Auto-naming
- Use cheap/fast model (Haiku or free tier) to summarize first user message
- Fallback: first N characters of first message

### Search
- Phase 1: Simple text search (LIKE query or grep on JSON)
- Phase 2+: Vector search with SurrealDB (native HNSW indexes)

### Context Optimization (Phase 2+)
- Only load relevant project context, not everything
- Lazy load files on mention
- Summarize old conversation context vs full history

## Open Questions

1. Storage backend for Phase 1? (leaning JSON files for simplicity)
2. How much conversation history to load in context? (last N messages + system prompt?)
3. Project file storage location? (compose/data/projects/<id>/files/)

## Decision Log

| Date | Decision | Rationale |
|------|----------|-----------|
| 2025-11-22 | Start with Phase 1 | Get persistent history working first |
| 2025-11-22 | Add Phase 4-8 | Research found missing features worth implementing |
| 2025-11-22 | Memory = auto-extract | ChatGPT style (vs Claude's explicit save) |
| 2025-11-22 | Search via flaresolverr | Reuse existing MCP, avoid new API costs |
| 2025-11-22 | Code sandbox = Docker | More powerful than browser-based Pyodide |
| 2025-11-22 | Phase 0 = test infra | Set up tests before adding features |
| 2025-11-22 | Test api.js + chat.py | Test foundation before building on it |
