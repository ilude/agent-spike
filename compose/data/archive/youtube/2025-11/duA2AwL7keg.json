{
  "video_id": "duA2AwL7keg",
  "url": "https://www.youtube.com/watch?v=duA2AwL7keg",
  "fetched_at": "2025-11-09T23:18:03.455968",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-09T23:18:03.455968",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "I think we're focusing on agents at their most underleveraged point. Let me explain what I mean. Fundamentally, we are focused on AI agents as executors. AI agents as doers writing emails, answering tickets, codegen demos, and we are spending ink and we are spending pixels and we are spending tokens figuring out as a community how to get AI agents to do stuff better. That is the lower leverage opportunity for Asians and we are almost never talking about the higher leverage opportunity and it's being used today by smart companies. The higher leverage opportunity is AI modeling agents as AI models. That is an exponential opportunity and this video is all about unpacking the idea that modeling beats doing. And there's a quiet AI revolution among companies that have figured that out. I want to show you why the next trillion dollar edge is not faster execution with a agents even though that's good. It's better simulation with agents. So the traditional agent conception is LLMs plus tools plus guidance. Pretty simple, right? You have an AI agent that has uh a large language model at the heart. That's we would call that the brains. It can call tools to do tasks and it's wrapped up by guidance or orchestration that gives it a policy that tells it what it should be doing and also constraints what it should not be doing. And a lot of our evaluations essentially measure how do these agents LLM and tools and guidance in a little trench code do at getting real work done. And so the KPIs that we brag about tickets closed, hours saved, cost per interaction, those all come from that idea of agents. agents as doing things with tools and policy guidance to constrain them. Networks of agents, communities of agents, meshes of agents to use the McKin McKenzie phrase, those all come from this concept that you need a swarm of agents or a team of agents doing work for you. That's great for automation. That's great for execution. Let's zoom out to the wider opportunity. Agents can be reality simulators. So the concept of a digital twin is something that was actually first brought out sort of and shown off in public earlier this year back in January when Nvidia launched manufacturing warehouse twins. This was in the same conference where CEO of Nvidia announced that, you know, Jensen is is gonna say this is the year of AI agents, right? And we knew that the VC hype was big for AI agents. Jensen coming out at the beginning of the year in January with a whole AI agent demo. People kind of slept on the warehouse part. People forgot that the idea Jensen had was that digital twins matter profoundly for long-term productivity and for maximizing the lever of AI leverage of AI agents. So just as we defined AI agents uh that are doers as LLMs and tools and guidance, I'm going to tell you that if you want to use agents as modelers, you add one thing more. You have agents that are LLMs with tools and guidance in a simulated world. That's the last part. And that's why the simulation matters so much with the warehouse that Jensen introduced. Every other example we have of model uh building simulates the world. Now, it might not be like a 3D video game world simulation. It might be a simulation that models the relevant constraints of the world in text in words. That can happen too. And all you have to do, like we we do this all the time, there are prompts that will set up your LLM to act as an agent within a reality simulator. And all you're doing is telling the agent to act in a certain way with this policy and guidance given these constraints around the world. And so when we talk about, hey, help me game out this situation with a difficult stakeholder. And people are having those conversations with their uh LLMs. They're having those conversations with Chad GPT. They're talking about breaking up with their ex and they're simulating that conversation with Chad GB to see how it goes. That is agents as reality simulators. Here's why it matters that we talk about this. We are spending most of our time talking about agents that execute. Those are linear time savings agents. They turn a 10-minute email into a zero minute email. which is fantastic. Imagine the difference when you have a reality simulator agent that helps you improve your decisioning as a business. Imagine an agent that allows you to simulate various business timelines and explore them. We often only have the chance for a simple PowerPoint presentation to the board with three options and here's our preferred one. AI gives us so much more power to work with and almost none of us are using these agents as reality simulators to think through different timelines in a structured way. In that world, if we did a little structured timeline exploration for a business, we could turn a 10-year market cycle into a 10-hour sim and come back with five or six different 10-hour sims and have a much more useful understanding of where the business was going. In a sense, we're taking all of these timelines that we've had to historically just look the next two or three steps on. We now have the compute to simulate a bunch of those different lines, bring them in and make smarter decisions. If that improves our decision-m as humans just a little bit, it will more than make up for the impact of all of the LLM agents that focus on execution. So, what are these exponential value levers? How do you know if you're doing this right? One, I talked about timelines. There's a huge alternate timeline advantage. You can run and simulate all kinds of different options, including not just for the business as a whole, but for particular scenarios. You can simulate customer response to product launches. You can simulate marketing campaign universes before you spend a buck. You can ship test all kinds of code permutations before you actually ship the code. Time compression is the second one I want to call out. So time compression is the idea that your competitor is on iteration three but you're on iteration 300 because you are not on wall clock time. You are on simulation time and you're able to simulate things so quickly and discard them. Now, I'm going to get objections for sure, right? People are going to say, \"Well, these simulations are not all accurate. So, why would we believe this alternate timeline or why would we believe this time compression concept?\" Well, one, it's being used by some of the biggest companies in the world already to deliver extraordinary value, and I'll get to that. But but two, even if it's not perfectly accurate, if it's significantly better than the option of not thinking about it at all, great. It can be 70% accurate and still be extremely useful. And yes, there are companies that are using virtual simulations to dramatically accelerate progression. Robotics is a good example. Robots are learning to walk without ever walking by being trained in virtual environments first where they can be trained extremely quickly. That saves the company a ton of time on training costs. Another example is Tesla and driving. Tesla trains driving AI on simulated courses and it helps because the car can have all of the edge case experiences without getting into very expensive accidents. Okay, so we talked about value levers like alternate timeline, time compression. There's one more I want to call out before we get to the real world here. Uh compounding is a big one. Every time you sim, you develop better priors. When you develop better priors, you get to nonlinear breakthroughs more easily. You can find pricing cliffs. You can find hidden segments. You can find breakthrough products. Things that you will not get with the smartest executing agents in the world. What I'm really trying to get you to take away from this is that you are on a linear value scale with AI agents as executors and you are on a nonlinear value scale with AI agents as model simulators. Let's get to a couple examples. Uh, and these are all vehicle examples. We're going to do some some cars this time. That doesn't mean this is the only place this is happening, but I think it's useful. With Renault, they cut vehicle dev time by 60% by having digital twins. The digital twin predicts crash outcomes pre-prototype, which really helps them to develop the car appropriately. BMW built a virtual factory with thousands of line change permutations overnight to simulate the best factory outcomes. Formula 1 has real-time pit strategy simulations that helps figure out what is the most efficient way to allocate energy in a pit crew change so that you can get that car back on the raceourse as quickly as possible. And one example that isn't a car situation ad networks can pre-est creative mixes for rorowaz uplift without spend. When you talk about sort of the idea of like a viral simulator and there are apps now that do this. What it's essentially doing is AI agents as world models. It's giving an an LLM or another machine learning algorithm a set of constraints, a set of tools, and a world to operate within. And it's asking it to come back with a response after it's modeled that world. Okay, I anticipate I'm going to get more objections. So, we're just going to be real honest about those objections. Garbage in, garbage out is the first one, right? If you put garbage in, you're going to get a bad simulation out and it's a waste of time. That's true. Maybe put in some proven calibration loops and calibrate what you put in. Maybe take pay attention. This is very controllable. And make sure that you back twist back test and keep yourself honest relative to performance. So if your digital twin is simulating a timeline and you're actually running that timeline on wall clock speed and you see that things are significantly diverging versus the scenario, be honest. Assess what went wrong with your simulation. You usually missed a constraint when you were projecting for the board and go back and fix it. Another push back. This gives you false confidence. Fair. I think we had false confidence when we didn't consider our options before, too. You should use your simulations to bound distributions, not run point projections. Does that make sense? You have distributions of timelines. you should be putting some constraints around them because you had a scenario that modeled out what was likely to happen. You don't want to make a point assumption. That's always been a weakness for humans is we overfixate on a particular point assumption and we don't think about the world as a series of distributions. Another objection, compute is super pricey. How can we afford this? Well, how can you not afford it? If it gives you breakthrough potential, seems like it would be worth it, right? I want to call out a fourth one. Culture change is hard. If we actually give people bonuses, if we give them rewards for decision quality, if we give people rewards for avoiding disaster, not just building something new, we are going to change corporate incentives. I know that is a hard one. I have no illusions. I've worked in the corporate world long enough to know that that there there's not a lot of companies that do that. But we have an opportunity to rethink how we do decision making to rethink how we do agentic utility in the business and we can bring compute into our decision-m and future forward thinking in a way we have never been able to do it before. I think that does imply culture change. I think it implies thinking more about how we think, how we make decisions, thinking more about avoiding disasters. So, you're like, \"Okay, this is a lot. How do I get started?\" Well, let me suggest picking one KPI to try and twin first. Something you think you know well enough that you can model, whether it's literally modeling with a long prompt in Chad GPT or building something custom. Maybe it's cost of acquisition, maybe it's churn, I don't know. Then you want to make sure you understand the data that you're feeding it. You want to understand how you refresh that data and you understand the feedback loops. Finally, you want to make sure that you have a tool stack that is dependable and solid. Now, if it's a big company effort, you may have a data lake with a lakehouse and a feature store and a simulation engine and a dashboard. That would be an example of an enterprise stack. If it's very small and you're trying to simulate breaking up with your ex or your soon-to-be ex, it's not nearly that fancy. You just have to have good data. You have to have a refresh cadence as you have that next date with the person that you're considering breaking up with and good feedback loops. And so I intentionally use a slightly humorous uh take from our personal lives because one, we do talk with Chad GVT about our personal lives and two, I think it helps make it tangible. Fundamentally, if you want to simulate a relationship, you have to give enough information about that relationship to make it a useful simulation. And then you have to change and update your priors for that agent to understand how it needs to adjust as reality continues to evolve. So the thing that I want to leave you with is this. If we have the capability to have clearer foresight and we choose not to use it, does this raise our moral responsibility, are we more responsible for future timelines because we have the compute to think about agents as worldbuilders? I think we are. I think we have a responsibility to think more deeply because we now have the compute to do so. And I want to call out again, there is a massive divergence curve opportunity here. If everyone else is obsessing about agents as doers and you are the one thinking about agents as ways to model future realities and make better decision-making, you are playing a different game and you are a first mover in that game. So stop asking how can AI do this task or I'm not going to say stop. AI is tremendously valuable as an exeutor, but 95% of what I see is that start asking how AI can show you different kinds of futures and help you improve your decision making. Where would a digital twin save you from your next big mistake? That's my question for you. Enjoy.",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api",
    "video_id": "duA2AwL7keg",
    "title": "We're Getting AI Agents Backwards\u2014Simulation Wins",
    "description": "My site: https://natebjones.com\nMy substack: https://natesnewsletter.substack.com/\nThe story: https://open.substack.com/pub/natesnewsletter/p/the-complete-141-page-guide-to-ai?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true\n\nTakeaways\n 1. Execution vs. Modeling: Most teams obsess over agents that do\u2014writing emails or closing tickets\u2014but the real leverage is in agents that model complex realities and guide better decisions.\n 2. The Missing Layer: Simulation: Add a simulated world to the classic \u201cLLM + tools + guidance\u201d stack and you transform an agent from a task-runner into a reality simulator\u2014think digital twins that explore futures before you spend a dollar.\n 3. Exponential Value Levers: Alternate-timeline exploration, time compression (iteration #300 while rivals hit #3), and compounding insights create nonlinear payoffs unattainable with pure automation.\n 4. Proof in the Wild: Renault cuts vehicle-dev time 60 %, BMW optimizes factories overnight, F1 refines pit strategy in real time, and ad networks pre-test creative mixes\u2014all by simulating before executing.\n 5. Handling Objections: \u201cGarbage in, garbage out\u201d is solvable with calibration loops; simulations bound distributions rather than forecast points; compute costs pale beside breakthrough potential; culture must reward decision quality, not just shipped features.\n 6. Start Small, Scale Fast: Twin a single KPI, feed clean data, refresh and back-test continuously, and layer dependable tooling. Early movers in modeling gain a first-mover edge while others chase incremental automation gains.\n\nQuotes\n\u201cWe\u2019re pouring tokens into agents that shave minutes, yet the trillion-dollar edge is compressing ten-year strategies into ten-hour sims.\u201d\n\u201cAgents in trench coats doing tasks are linear; agents in simulated worlds are exponential.\u201d\n\u201cIf we now have compute for clear foresight, choosing not to use it becomes a moral failure.\u201d\n\nSummary\nI argue that the community fixates on agents as doers\u2014LLMs plus tools closing tickets\u2014while the real opportunity is agents as modelers. By adding a simulated world, we turn agents into digital twins that explore alternate timelines, compress months of learning into hours, and compound insights over iterations. Examples from Renault, BMW, Formula One, and ad tech prove the payoff. Objections about accuracy, overconfidence, cost, and culture are addressable with calibration loops, probabilistic thinking, and incentive shifts. The playbook: pick one KPI, feed quality data, establish feedback loops, and iterate. Those who master simulation will outpace pure automation players.\n\nKeywords\nAI agents, digital twins, simulation, modeling beats doing, alternate timelines, time compression, compounding insights, decision quality, KPI twinning, calibration loops, Renault, BMW, Formula One, ad creative simulation",
    "published_at": "2025-07-16T13:01:19Z",
    "channel_id": "UC0C-17n9iuUQPylguM1d-lQ",
    "channel_title": "AI News & Strategy Daily | Nate B Jones",
    "duration": "PT15M33S",
    "duration_seconds": 933,
    "view_count": 34461,
    "like_count": 1776,
    "comment_count": 229,
    "tags": [],
    "category_id": "22",
    "thumbnails": {
      "default": {
        "url": "https://i.ytimg.com/vi/duA2AwL7keg/default.jpg",
        "width": 120,
        "height": 90
      },
      "medium": {
        "url": "https://i.ytimg.com/vi/duA2AwL7keg/mqdefault.jpg",
        "width": 320,
        "height": 180
      },
      "high": {
        "url": "https://i.ytimg.com/vi/duA2AwL7keg/hqdefault.jpg",
        "width": 480,
        "height": 360
      },
      "standard": {
        "url": "https://i.ytimg.com/vi/duA2AwL7keg/sddefault.jpg",
        "width": 640,
        "height": 480
      },
      "maxres": {
        "url": "https://i.ytimg.com/vi/duA2AwL7keg/maxresdefault.jpg",
        "width": 1280,
        "height": 720
      }
    },
    "fetched_at": "2025-11-15T19:25:11.730403",
    "all_urls": [
      "https://natebjones.com",
      "https://natesnewsletter.substack.com/",
      "https://open.substack.com/pub/natesnewsletter/p/the-complete-141-page-guide-to-ai?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true"
    ],
    "blocked_urls": [],
    "content_urls": [
      "https://natesnewsletter.substack.com/",
      "https://open.substack.com/pub/natesnewsletter/p/the-complete-141-page-guide-to-ai?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true"
    ],
    "marketing_urls": [
      "https://natebjones.com"
    ],
    "url_filter_version": "v1_heuristic_llm",
    "url_filtered_at": "2025-11-15T19:52:16.781195"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"Agents as Models: AI Simulation, Digital Twins, and Better Decision Making\",\n  \"tags\": [\"ai-agents\", \"simulation\", \"digital-twins\", \"decision-making\"],\n  \"summary\": \"The video argues that AI modeling agents\u2014their use as reality simulators and digital twins\u2014enable powerful simulations to improve decision making and accelerate forecasting beyond traditional execution-focused AI.\"\n}",
      "generated_at": "2025-11-09T23:18:14.802450",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}