{
  "video_id": "urymhRw86Fc",
  "url": "https://www.youtube.com/watch?v=urymhRw86Fc",
  "fetched_at": "2025-11-17T22:30:53.437751",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-17T22:30:53.437719",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "every day it becomes clearer that local llms are the future of generative AI why from companies to Indy devs using llm apis comes at the expense of privacy money and speed not everyone can afford or wants to torch 5 10 50 thousands of dollars to build test and use llms in production environments right now these are the costs we have to pay for the best models but local LMS are catching up fast the issues with speed Ram requirements GPU requirements and ease of access are being solved One Day at a Time by the incredible open-source llm Community we've seen local llms make huge strides in the past 6 months but how close are we to having a local on device llm providing real value for real use cases and if they're not ready how will we even know when they are let's discuss local llm viability of course the tldr is local llms are not ready for most use cases it takes a ton of work to set up a local llm and to get it into the shape in which you can use it in a production environment but by utilizing one of the most important engineering practices we can learn why local llms aren't ready yet and we can set up systems to monitor when they are ready so we can be ready to take advantage of the incredible opportunity so what is engineering practice it is none other than testing I've talked about prompt Fu in the past but by testing our prompts by testing llms we can know when they're ready for our specific use case and we can watch as local llms make progress and become more viable every single day so let's break down how you can test your prompts and local llms so you can unlock the value of local llms as soon as they're ready let's look at this local llm eval codebase Link's going to be in the descript description so in this test we have two sets of providers we have cloud llms and we have local on device llms we're going to look at how these two sets of llms cloud versus local performs against a natural language query to SQL prompt so the goal of the prompt is to take a natural language query and convert it to SQL we have some light instructions here on how to convert a natural language query into an SQL statement so every single provider you see here is going to be run using that prompt and we'll pass in the nlq and the database dialect in each one of these tests and of course the test specifies what the natural language query is and it'll assert the results we're looking for so let's go ahead and just run this right in promp Fu you first export your API Keys then we're just going to run prompt Fu eval so this is going to run all these tests so I have you know 10 tests here and for each provider it's going to run each one of the tests so we'll see about 40 results after this test runs okay fantastic so that finished you can see here we had four failures 36 successes you can see here we can run this promp Fu view command to view a local web viewer of the results so let's go ahead and do that we'll open up the browser and here we go so this is really cool so you can see for each prompt and for each combination of variables we can see the results of our tests so looking at each model you can see here we have the gpt3 we have GPT 4 and then we have Gemini Pro nothing really exciting here nothing kind of unexpected each one of our 10 test cases you know select user with ID and the SQL of course is Select star from users where ID equals 5 and you know the test passed for almost all these you can see here Gemini Pro performs a little worse than GPT 3.5 both gp4s performed perfectly they nailed every single SQL statement right so this is really cool this is the power of prompt Fu this is is the best way to test your llms right now the best way to test your prompts with different variables with different prompts I did a whole video on this I'm going to link that in the description as well for you to check out you know as expected the best models perform the best you know we pay for these we do expect you know top high-end performance out of these models and you can see here in the test. yamel file in the codebase and just to mention it here you know we're operating inside of this nlq tosql test Suite so essentially every directory here presents a test Suite you can you know play with these in the code base it's a great way to get started with prom Fu and start testing your specific use cases that you want to nail we need to move away from using llms as toy applications and really start building real valuable products with them and how do you build real valuable products you need to know that your code that your infrastructure that your product is working and of course the way to know that your product is working your application is working is to test but so let's go ahead and take a look at our on device local l lims I'm going to just go ahead and run the test first and then we can talk about how exactly it's set up the results of these tests are really really interesting so let's go ahead comment these out I'm going to run the top four here so I have an older mistro model I'm running the latest updated version of mistol and then I have both 5 2 and I have rocket 3B I'm not going to run tiny llama my computer started making weird sounds which it never has when I started running this I think there's something up with this model so just maybe a head up we're going to run these these are some of the best open source local LMS right now and we're just going to run them against the exact same test right so you can see here in this directory it points right to the prompt. txt in the same you know file here and it also points to test star. yaml so that's going to be all the tests and we can you know open these up one by one these are just going to be basically the exact same thing with different assertion statements and different SQL passed in right so you know this provides a really cool way for you to test individual cases for your application while you're changing up some of the variables right changing up some of the inputs that your user will likely input into your code right and some other additional variables right that you might have that maybe your backend is going to you know insert into your prompt right and then you can use this really just clean and simple syntax uh you know with curly braces to you know specify exactly what you want swapped out in your prompt so really cool I love how they set this up shout out to all the prompt food Developers they're building a really fantastic tool here without further Ado let's run the on device local llms and compare the results let's see how close they are to providing real value for real use cases so to run these we're just going to do the exact same thing promp Fu eval and here we go so right now under the hood a bunch of RAM and resources is being you know reserved and run to fire off these local llms for those of you that don't know I'm not using oama I'm using something called llama file I did a whole video on llama file definitely go check that out long story short if we open up the custom models directory here you can see we have a couple JS files that allow us to run custom local models but we also have a bunch of llama files and again the best place to go to learn about llama files is that video and then the original llama file code base the whole idea is you can run an entire local open source model in a single file and it's really really incredible I just want to point out something also here you know while these tests are running my M2 high-end MacBook Pro just started humming first time I've ever heard my M2 so I just want to point that out like these local llms are going to be putting your device to work if you can run it at all even some of these smaller models still really resource intensive but let's go ahead and just let this finish okay so it looks like we're getting stuck here on one of these models I'm going to go ahead and guess this is the rocket 3B I'm just going to comment this out we're going to rerun and hopefully we'll get everything through we are super early to using local llms in this type of way at this type of volume so I'm not surprised to see things going wrong here we're just going to rerun I'm almost confident that my machine is just maxing out and killing one of these programs and freezing and stalling everything else okay so that finished we did have to take out rocket 3B but let's go ahead and just look at the results you can see here right away we have 14 success 16 failure when we were using Cloud providers we had most of our cases succeed right so let's look at the results here right off the back you can see that our top performing model is going to be mistel 7B it looks like both of the previous version and the new version came really close to performing really well let's go ahead and just look at the exact cases right we have a couple cases here you know select five users where we are selecting from users limit five but we're looking at a specific column and not every column which is what most of the higher end models do and then we have you know select just five users it's trying to order randomly for some reason uh order by random here again and then we have select all user Fields you can see that this was working here um not working here and you can see here fi got that one right and then you can see uh fi just fully crashed um you can see that uh it looks like it probably just blew out of memory and you know this just kind of goes to show the point that you know these models aren't ready they're not really ready to be run on device at scale and you know to be fair I am pushing my machine pretty hard by running all these tests really fast back to back what it looks like llop file is doing is allocating a certain amount of memory to run the models so it's kind of setting it up mounting it and then it's running the prompts against it right so any local llm provider is going to have to do that type of work it needs to mount it needs to allocate memory a large chunk of memory in order to actually run any one of your prompts but you know all in all even with f crashing even with it looks like you know we had rocket 3B crashing in still not too bad right in a really closed environment you could feasibly see you know running one of these llms with decent success specifically the mistro model right like we could clean this up we could do some more prompt engineering to get this working in some capacity but you know why would I do that right now when I could just click on gpg 3.5 gbg4 and have all of these passing at 100 if we open up our eval runs here and just go back to the previous run you know the results look very different right gbg4 just killing it 100% everywhere with no issues gbg 3.5 basically still getting 100% it looks like it just pulled out every single one of the fields we can let that pass right it literally just selected every column from the user's table and we can look at the users table here again right if we look at nlq SL prompt you know it's looking at that users table it's got the ID it's got created updated you know so basically just manually selected all the fields this is passable that's fine so really GPT 3.5 also did an amazing job uh vertex doing okay got three failures here it looks like these are probably also passable you know but then we go to our local open source models and um you know not quite the same picture a lot more work needing to be done here to clean these up we have one model fully crashing and this is already after pruning a model completely there's a lot more to go here there's a lot more work to be done I just want to call something out this isn't all bad news there's something really magical about these local llms let me let me show you exactly what I mean so if we open up the promp fu configuration and let me just go ahead and get rid of um these models here and let me just run mistol I'm going to boost the repeat to five now mistol 7B is going to run 50 prompts using this prompt template it's going to run each one of these tests five times right so you know we have 10 tests here that's going to give us 50 tests I'm just going to let this rip let me show you something really cool something really magical about these local open source models so that's just finished you know Mill performed fairly well looks like we got 30 successes 20 failures and you know again I just want to call it out running the M2 64 GB memory it is officially powered on and it never does this my M2 stays dead quiet all the time let's go ahead and look at that test right you can see here we got 60% of the test correct this is another reason I like prompt Fu in reality using llms is going to provide variable results right they're non-et istic by Nature even with your temperature at zero so I like to use this repeat option to see how many times will something work and then you can get into the probability of your prompt and your model and How likely it is that you know for any user it will produce the right results so you can set that there with the evaluate options repeat 5 that's a really handy feature running this model over and over and over really lets you test the output it's local and this costs me absolutely nothing right that is the beautiful part the real setback here is we need smaller on device llms with higher accuracy before this becomes production viable it's amazing that I basically did this at no cost I ran 50 prompts at literally zero cost to me whereas when we were running the cloud providers you can see the cost of every run so you can see here 2 cents burned there 1 cent there and you know another great part about pron Fu it's doing the math for me here I could run this 50 times I do this for the production tests for talkr database but here for this demo this is enough we don't need to run this 50 times and you know really torch torch the cash there but I just wanted to call that out I think that's pretty incredible that we're able to run this prompt these prompts 50 times the really magical part here is I can run this as many times as I want with no costs incurred I spent Zero running these 50 prompts that's pretty awesome so real quick I want to talk about a couple things llama file and and how this codebase is configured so that you can run local models so you know first we have to talk about llop file LL file is this really incredible piece of technology basically it lets you run llms and a llm server out of a single file Link in the description of course but let me go ahead and just show you what I mean we can mount one of these models right now in the custom model directory we can open up the Shell let's open up a new terminal and I'll just type custom models and let's just go ahead and run that um we'll run rocket the one that failed so I'm just going to run literally just running that file it's going to load it into memory it's going to do everything it needs to run and then bam it's now running a clean chat like interface on Port 880 and you know we can run some prompts fix this SQL statement from user equal true and there you go where off status equals true and user ID is not null it's modifying some stuff here again these are local models so you know we we're running rocket 3B it's it's local you know it's running right in memory so of course it's not going to be perfect but it is pretty cool you know list five cool things about typescript let's see where it pulls that nice type inference type checking code reusability accessible type system so anyway just wanted to show that off promp Fu allows us to create these really simple classes that basically run arbitrary code and return the response so it has a really clean simple API we can go just look at that so yes so you can see here if you mirror this API you can basically run any code you can run any llm and return the right response which is the provider response here and you can see the provider response is going to be this object structure here and if you return that it's going to post your results in the UI and handle the response from your prompt test so that's what we do here we have this custom model Base Class which handles all the logic feel free to check out this code base dig into it if you want and then we have upper level classes that basically just override a couple key methods to make the local llm work and return the results right so we have versions for the mistol for five for rocket and for tiny llama so that's all here it's going to be in the code base Link in description once again so you know as you can see local llms aren't there yet but they are definitely getting there this is something I'm keeping a sharp eye on as many of you know I I am actively working on talk to your database a text to SQL to results desktop application that's going to reduce the time it takes for you and I to write SQL and get results out of your databases I can't ship this product in good faith to developers working in real production environments with real Stakes if I'm not testing the underlying technology you can't really productionize your software if you're not testing it if you're not proving it out so I'm actively testing cloud providers to make sure that they're producing the right outputs you know using promp Fu and I'm also testing and really excited and you know looking at um you know adding local llms to talk to your database long-term goal for that application and for all the applications I build is to run the llms right on the device right so we don't have that cloud dependency that latency and you know the privacy concerns and all that right just say specific use case I wanted to share I think in order to get real value out of local llms we need to monitoring how these perform against the real Cloud providers right for example you could take a fine tune mistro model that is you know hyper focus on your use case and compare it to you know like an out of thebox gbg4 right so that would be a great test to run right like just compare your local model to you know a gp4 model and this is the key I think really paying attention to this over time as new models get rolled out both on the cloud and local side you should have your test ready ready to quickly add new models new prompts new variables so that you can be right on top of it when local llms are ready for production that's where I'm going to be it's really clear to me that local LMS are the future the only question is when and how quickly can you and I move on the insane value that's coming out of local llms please use this codebase as a launching point to you know help you get all of your tests set up for your own personal use definitely for your you know production product client work I am hyperfocused on that nlq to SQL use case if you care ton about code generation you should have code generation tests right if you care ton about SEO marketing and blog generation you should have test Suites for blog generation tests right if you're a Trader and you need fast sentiment analysis on you know live video feeds you should be building test Suites so that you know when that next you know sentence that next phrase that next word comes in you're running your sentiment analysis and you're getting you know the information you need to make the decisions right on the spot so you know you get the point Point here I highly recommend you you know dig into this code base dig into prompt Fu if you're using AMA that's cool I've had great success so far with llama file so I'm going to be using that for a while it's all about setting up your tests for both cloud and local llms so that you are ready when they are ready right that's what it's all about and tests help you get there prompt testing is going to help you get there if you want to stay up to date explore use cases and discuss the value propositions and witness real products being built with local llms drop the like drop the sub and I'll see you in the next one",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "local-llms, on-device-llms, prompt-fu, nlq-to-sql, open-source-llm",
      "generated_at": "2025-11-17T22:31:04.215916",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}