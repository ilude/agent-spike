{
  "video_id": "KhINc5XwhKs",
  "url": "https://www.youtube.com/watch?v=KhINc5XwhKs",
  "fetched_at": "2025-11-17T22:32:52.900365",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-17T22:32:52.900336",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "how do you know if you've created a great prompt that will deliver repeatable results across hundreds of uses in your apps and tools if you've clicked on this video you've probably run into situations where you're running a prompt for an application or a tool and you've wondered how can I improve this prompt in speed accuracy and cost the tricky part about prompt engineering right now is that if you don't have a framework in place for measuring a prompt against another prompt or an llm provider against another llm provider you won't really know if you're adding value or removing value from your prompts and therefore you won't really know if you're improving or actually making your application worse I think for a lot of prompts especially personal prompts you can get by on just feel you can get by on just the couple results that you see out out of your prompt but when you want to kick things up to the next level when you want to scale your prompts into an application that runs hundreds thousands and maybe tens of thousands of times per day you need to ship your prompts with with confidence that's where test driven prompt engineering comes into play in this video I'm going to show you how you can get cheaper faster and more accurate prompts by utilizing a prompt evaluation and testing framework called prompt Fu prompt food lets you iterate on your llms faster and it helps you measure the quality and it prevents you from making your prompts worse it helps you catch regressions after this video you'll know how you can compare your prompts to find the most accurate fastest cheapest prompt for your tools and applications let's Dive Right In so I'm going to open up vs code this is a quick start prompt Fu code base that I built for you go ahead check it out Link in the description this is going to help you get ramped up very quickly and understand the Core Concepts of prompt testing if you open the read me you're going to see a set of instructions let's go ahead and go into preview mode here and the setup for this is extremely simple huge shout out to the promp fu Engineers for building this install promp Fu export your open AI API key when do that now and then we're going to CD into one of the test Suites here I'm going to go into our first minimal example and then we're going to run prompt Fu eval so I'm going to do that now PR Fu eval so this is what the standard output looks like in the CLI we'll get into the details in a moment here what I'm going to do is open up the Local web viewer and we can do that using prompt Fu view so we're going to do that now prompt Fu view great so this view is much better so let's start from the top here you can see here we have a description so this is just the description of what we're testing you can see here we have a variable which is nlq natural language query and you can see it's saying list jobs that have run longer than 30 seconds so this is a variable I'm passing into my prompt and if we inspect our prompt here you can see this is the prompt so is this following block of text a SQL natural language query nlq and then here I'm passing in this nlq variable and this nlq variable is exactly what gets replaced by this variable here right so you can see this is nlq if we open this up again we have this syntax here where it says nlq so that's getting replaced there you can see here we're running two models open AI gp4 turbo preview and open AI 3.5 turbo preview and already just by running this one simple test you can see something really interesting right I'm getting the exact same results but in a vastly different number of tokens and if we remove the cach which we'll do in the second this is a cached result um you'll see that gbt 3.5 is much faster than 3.4 right but let's go ahead and open this up gb4 is giving me a lot of additional information that's really cool do I need all that information in this case I don't and gbg3 is going to be more than enough but let's backtrack a little bit let me show you exactly what this test looks like so if we go back to vs code and we open up this directory we can see we have three files and this structure is repeated in every test here okay so let's open up the prompt Fu config that's where everything starts you can see the structure is very simple and this is something that I love about pron Fu you're able to really condense it down into really clean reusable testing structures as you can see we have a highle description we have our two providers we have a reference to our prompts and a reference to our tests so this is referencing this prompt file right here and you can see here we have this file glob pattern saying it's saying pull in every test with this format right so test and then any string and then yaml this is really cool prom Fu gives you a simple configuration file this is the starting point for all tests but if we go ahead and open up the prompt you can see here just like we saw in the UI we have this simple prompt so the idea is that you'll store all of your prompts in this type of test Suite or in some type of centralized location and then you can hook up promp Fu using this prompt syntax to go to that exact location you know wherever um you know wherever your prompts are right and then you can run arbitrary tests on them right so I think that's going to be like the right structure when you're setting up your test Suites so anyway let's continue so you can see here we have our prompt we have the variable in our prompt that's going to be dynamically changed based on our test okay so let's look at the last file here our test. yaml so I've commented out a couple items here let me just get rid of these for now and we can add them back in a moment so what does a test look like in pron Fu it's quite simple so we have a description so this is you know you describing what the test is testing and then we have two really important blocks we have variables and you can see here we have that nlq variable right this is what's going to be dynamically inserted into each test if we had another block like like this here and we updated nlq we're going to get two runs of our prompt with the two different versions of the nlq variable right and I'll show that off in a moment here the next most important block here is this assertion block and this is what really creates the value of your tests right this is where your tests really become valuable this enables you to build out assertions into your tests and if we open up the docs here so in the read me I've linked a few really important places you're going to want to hop to quickly in the pron fuit documentation the assertions is one of them so if we open up the assertions you can see here there's an entire slew of different ways you can assert truth after your test runs right so imagine you run your prompt and now you have your prompt output on the output you're going to run different assertions right so you can imagine you know for our natural language query prompt let's go ahead and open that up right so we're saying is this following block of text a SQL natural language query so in order to test if that's true you can see here inside the test I'm using this I contains any so this is ignore case contains anything right so this is saying ignoring case make sure that my prompt contains any one of these words right so any one of these items make sure it's in my prompt right and you can see that here I contains any they have a really nice set of assertion types that you can make right you can use simple ones like make sure it contains a certain string you can use regx matching you can use custom JavaScript custom python you can use leevin distance and then you can do some really cool things that we'll get to in a moment like my favorite so far the llm rubric which basically lets you run llms on on your llm responses right so you can say something like um make sure the sentiment is positive right so make sure that the output is positive you can say things like make sure this is a natural language query right which is what we're going to do in this example coming up here so that's what the test looks like and really after you've learned just these three simple components you have your providers you have your prompt and you have your tests you're actually good to go um and that's why I wanted to make this video just by knowing these three simple components and by knowing about promp Fu you've just unlocked massive value for your prompts for your tools for your applications and therefore your users right and that's what this is all about I've mentioned this on the channel before if you're a subscriber to the channel you know that we aim to use the best tool for the job the tool that allows us to build the best experiences for our users in the age of llms and AI technology you need to be able to test your prompts to maximize accuracy for your users and also for your business you want to be able to cut your costs to the bare minimum right by comparing different prompts you can know you know how long does that prompt need to be you can know do I need gp4 here can I use something simpler right um so you know those are just a couple of ways you can use this let's go ahead add these items back to our tests right so I'm going to add two additional test cases and you know just never let yourself get overwhelmed just take things one step at a time so if we collapse this you can see here we just have three items in our test and you know each one contains their own natural language query which then will get passed into our prompt and run three times right so what we end up with here is one prompt that runs three different times right and then we run that two times for each provider right so we're going to end up with six total tests here let's go ahead and run this again so I'm going to close that out I'm going to run the promp fu eval and I'm going to run this with the no cash parameter so no cache you can see here that it's returning cached items so that you say tokens on your testing I actually recommend always running with no cash so you can get fresh results every time as you're updating your tests so I'm going to run no cache and you can see here it's running those six items it's running the 3.5 turbo model and it's going to run gp4 there we go great so that's run you can see we have no failures so everything ran successfully let's go ahead and open up the viewer now the UI should make a little more sense here right we have our natural language query variable running three times and then we have our outputs which are running each different provider on our prompt and that's running for GPT 4 and then you have gpt3 doing the exact same thing here right and so couple things to look out for here right away right even just with this small test we have a test running three times with different assertions you can see here that gbt 4 took 11 seconds right 11,000 milliseconds that's going to be 11 seconds and gbt 3 took less than 1 second okay something else important to note here the results are the exact same you can see here in a lot more detail dpt4 is giving me a lot more information it's helping me build out the actual SQL which is great but you know that's not really what I want here I just wanted to tell me is it a natural language query or is it not and you can see here gpt3 is doing that in a tenth of the time at a fourth of the cost this is really really important to call out right I'm just going to say that one more time gbg 3.5 turbo is doing the exact same work gbg4 turbo is in a tenth of the time right at a fourth of the cost right we have 200 tokens here we have 50 tokens here right this is massive if I have this prompt in production which I do I'm going to be totally transparent here I have the gb4 prompt running this exact same prompt for a new tool called talk to your database you can check that out Link in the description if you're interested I want to call out how important this is right we have a huge prompt giving us a lot more information back that we don't need we don't want gpg 3.5 can do it for us right a tenth of the speed and a fourth of the cost even if this isn't always consistent you can imagine that some version of this ratio right uh 10:1 4:1 exists right and and and the whole point is with testing your prompts you can reveal this information okay I want to call out a couple more things here in our second prompt you can see we're using the llm rubric assertion type let me collapse the first test case and you can see here for list five users with Gmail and that's happening right here right so that that's this row okay we actually have another test validation right we're saying response should confirm that query is an nlq so this is one of many assertion types that you can use with prom Fu right there's there's tons of them here you're going to find a lot more of these other ones really valuable these are just some that I've gravitated toward to get up and running quickly I'm a huge fan of the llm rubric cuz you can just basically prompt on your prompt to confirm um you know the type of response happening so so that's fantastic I'm leave all the information you need to get started and to get running in the read me so there's going to be a lot of valuable information for you here Link in description it's all here ready for you I know I said I wouldn't make a video this week but I dug into this and you know I started thinking um you know it's really important for engineers for product Builders to see this save time save money speed up your prompts feel free to like feel free to sub if this was helpful for you there are a couple other tests in here that you can look at there's a lot more you can do with promp Fu right we're only using two llm providers here if you look in our PRP Fu config I'm just looking at open AI you know in the docs I've linked um to different providers and they have a whole slew of you know all the common providers uh and I'm sure they'll be adding uh Gemini coming out pretty quickly here I'm really excited to test that out see how it Compares and uh you know this kind of opens up a whole new realm of knowing and having certainty that your prompts are great so there's a lot more information here there's a lot more value feel free to check out the repo if you you know have interest if you you have this problem if you want to improve your props you want to cut your costs highly recommend you check this out just real quick I want to call out you know what why are we testing why are we evaluating our llms why are we evaluating our our prompts specifically right it really comes down to these three things and I'm going to gloss over these pretty quickly here I want to keep this video short for you guys save money save time ship with confidence and prevent regressions right once you have all your prompts in a centralized location you can prevent issues and you can detect them before they run into production right it's all about about setting up the right assertions for your tests and the Beautiful part about this is you know let's say you have um three or four more assertions one of them can fail and that can be okay for your prompt right prompt fuit will tell you you know three out of four test pass and maybe that's okay right maybe that's okay for this situation but you might get below threshold where you know if 50% of your test cases are failing for a specific prompt and a specific test then you know you need to give it some more attention or the change that you just make to your prompt is actually causing a regression right so that's really important to call out there save money save time I think that's pretty obvious at this point right by knowing how many tokens you need by knowing which llm you need to produce a prompt and produce it with consistent results you can save money and save time right one of the big things for me I've been working on a natural language query AI postgress data analytics tool I need to know and test different prompts against different prompt inputs with a couple different variables coming in right and prompt Fu gives you a really clean structure right you set up a bunch of prompts and you pass in specific tests and you can update these variables right I have two other examples here that are a lot more detailed that are going to give you a lot more information right you can actually pass in multiple prompts by using this triple Dash and then all your tests will run for each one of these prompts right so there's a lot more value here I just want to call out a couple things I think that there is this journey that Engineers go on where they first dislike testing because it burns so much time completely understand that I was that engineer once too but then they come around to it because they realize it actually saves them time and increases confidence and reduces bugs and lets them ship faster with confidence so I think carrying some of that mentality over to your prompts is going to be a wise move it's not out of the question to build test driven prompts you know to write the test first and then build a prompt to fit the test right so let the let your test kind of Drive how you write your prompt I'm a huge fan of prompt Fu due to the Simplicity and ease to get set up so again shout out to the devs highly recommend them I'm going to be sticking with them for a while they give a lot of Simplicity and customizability so you know it's a great tool it's going to be one that's in my toolbox right now especially in the age of llms the age of AI gpts Etc so that's all for this one drop a like and a sub for more agentic engineering tips tools principles and more on this channel we build full complete tools full complete products using the latest and greatest technology right now it's all about agentic software you're actively on the road road to building software that lives breathes and creates value for us while we sleep I'll see you in the next one",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "prompt-engineering, prompt-Fu, llm-testing, test-driven-prompt-engineering",
      "generated_at": "2025-11-17T22:32:59.241508",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}