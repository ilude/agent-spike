{
  "video_id": "6iEgJsqkdeM",
  "url": "https://www.youtube.com/watch?v=6iEgJsqkdeM",
  "fetched_at": "2025-11-10T00:30:23.711772",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-10T00:30:23.711772",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "one of the things I've been calling out is that we really need to get past this idea of short-term memory context windows in AI where you have a limited context window and the AI just forgets well Google has written a paper that helps us think about how to get past that uh it's called Titans and it's basically presenting a different architecture than traditional Transformer based architecture and large language models I'm going to try and explain it to very briefly we should probably do a longer video on this at some point but the paper just came out I'm still reading it so the takeaways that I have at the top right now Transformers use self attention to compute relationships between all of the tokens at a sequence so if you say the cat jumped over the dog that's a sequence it's Computing the relationship between the tokens in that sequence so self attention is going to have mathematically speaking what we would call quadratic complexity in other words it's very very expensive to compute for long sequences because you're multiplying across all of the relationships and Transformers struggle and don't explicitly distinguish between short-term and long-term memory it all works like that every token interacts with all of the others and so Titans is different because Titans introduces something that's closer to our own brains there's a dual memory system in Titans a Titans architecture apparently has a short-term memory which is very similar to how Transformers work today and it focuses on local dependencies it also has a long-term memory which is a separate net new neural module that's explicitly designed to store and retrieve information from past context now what's interesting is it apparently works over longer context windows so titans's long-term memory can handle context lengths exceeding 2 million tokens it does that by efficiently retrieving information without recomputing the dependencies for the entire sequence so it can look at Ultra long range dependencies like uh relationships between genes and a genome now the nice thing is it enables you to get to linear scaling versus sort of the computational cost of quadratic scaling and I know that sounds mathematical but basically if you're not Computing all the relationships all the time then you're able to scale farther and so that's really exciting I'm still digging in I'm still trying to figure out what all is in here but potentially it seems to enable long range needle in a hstack type memory retrieval that's what the authors claimed they did they claimed they tested it versus Baseline trans Transformer architecture on what's called a needle and a Hast stack task a classic example is you change one word in Moby Dick and you tell the Transformer to find it and you see if it can look through the entire context window and find it and they claim that their long-term uh Titan's memory architecture does better at that than Baseline um and they think that by explicitly differentiating what requires immediate attention in short-term memory versus long-term attention it's going to mimic human abilities better and allow us to exceed traditional context Windows that's what I've got so far I'm still reading the paper I think it's potentially very important uh and I wanted to share it with you and see what you guys think",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api",
    "video_id": "6iEgJsqkdeM",
    "title": "Google Research Paper: Titans Architecture Solves AI Memory?",
    "description": "My site: https://natebjones.com/\nMy links: https://linktr.ee/natebjones\nMy substack: https://natesnewsletter.substack.com/\nTitans paper: https://arxiv.org/pdf/2501.00663\n\nTakeaways:\n 1. Dual Memory System: Titans introduces short-term and long-term memory modules, mimicking human memory systems and improving over traditional transformer architectures.\n 2. Extended Context Windows: Titans handle over 2 million tokens, enabling ultra-long-range dependencies and surpassing transformer limitations.\n 3. Linear Scaling: By efficiently retrieving information without recomputing dependencies, Titans achieve linear scaling, reducing computational costs for long sequences.\n 4. Improved Needle-in-a-Haystack Tasks: Titans outperform transformers in tasks like identifying changes in large datasets, demonstrating better memory retrieval and accuracy.\n 5. Human-Like Efficiency: Differentiating short-term and long-term memory enables Titans to prioritize information like human cognition, improving task efficiency and focus.\n 6. Genomics Applications: Titans show potential in fields like genomics, handling relationships across vast datasets such as gene sequences.\n 7. Revolutionizing LLMs: Titans redefine context handling in AI, paving the way for more scalable and capable large language models.\n\nQuotes:\n\u201cWe need to move beyond the limited context windows of AI and embrace architectures like Titans that can truly remember.\u201d\n\u201cTitans\u2019 long-term memory module enables efficient retrieval without the heavy computational burden of transformers.\u201d\n\u201cThis architecture mimics human memory better, blending short-term focus with long-term storage to solve complex tasks.\u201d\n\nSummary:\nTitans introduces a revolutionary architecture that combines short-term and long-term memory modules, addressing limitations of traditional transformers. With extended context windows exceeding 2 million tokens and linear scaling efficiency, Titans excel at tasks requiring vast memory retention, such as genomics and needle-in-a-haystack problems. By explicitly distinguishing between immediate and historical data, Titans mimic human cognition and redefine what\u2019s possible for large language models, setting a new benchmark for scalability and performance.\n\nKeywords:\nTitans, transformers, dual memory system, short-term memory, long-term memory, context windows, linear scaling, needle-in-a-haystack tasks, large language models, AI scalability, genomics, AI efficiency, memory retrieval",
    "published_at": "2025-01-14T14:00:01Z",
    "channel_id": "UC0C-17n9iuUQPylguM1d-lQ",
    "channel_title": "AI News & Strategy Daily | Nate B Jones",
    "duration": "PT3M43S",
    "duration_seconds": 223,
    "view_count": 8542,
    "like_count": 436,
    "comment_count": 53,
    "tags": [],
    "category_id": "22",
    "thumbnails": {
      "default": {
        "url": "https://i.ytimg.com/vi/6iEgJsqkdeM/default.jpg",
        "width": 120,
        "height": 90
      },
      "medium": {
        "url": "https://i.ytimg.com/vi/6iEgJsqkdeM/mqdefault.jpg",
        "width": 320,
        "height": 180
      },
      "high": {
        "url": "https://i.ytimg.com/vi/6iEgJsqkdeM/hqdefault.jpg",
        "width": 480,
        "height": 360
      },
      "standard": {
        "url": "https://i.ytimg.com/vi/6iEgJsqkdeM/sddefault.jpg",
        "width": 640,
        "height": 480
      },
      "maxres": {
        "url": "https://i.ytimg.com/vi/6iEgJsqkdeM/maxresdefault.jpg",
        "width": 1280,
        "height": 720
      }
    },
    "fetched_at": "2025-11-15T19:24:12.397440",
    "all_urls": [
      "https://natebjones.com/",
      "https://linktr.ee/natebjones",
      "https://natesnewsletter.substack.com/",
      "https://arxiv.org/pdf/2501.00663"
    ],
    "blocked_urls": [
      "https://linktr.ee/natebjones"
    ],
    "content_urls": [
      "https://natesnewsletter.substack.com/",
      "https://arxiv.org/pdf/2501.00663"
    ],
    "marketing_urls": [
      "https://natebjones.com/"
    ],
    "url_filter_version": "v1_heuristic_llm",
    "url_filtered_at": "2025-11-15T19:52:12.913933"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"Titans architecture: long-term memory and linear scaling in AI\",\n  \"tags\": [\"long-term-memory\", \"transformer\", \"self-attention\", \"linear-scaling\"],\n  \"summary\": \"Explains Google's Titans architecture, a dual-memory AI model with a short-term local memory and a separate long-term memory module that enables ultra-long context and linear-scaling computation.\"\n}",
      "generated_at": "2025-11-10T00:30:43.495993",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}