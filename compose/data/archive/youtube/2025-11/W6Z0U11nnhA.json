{
  "video_id": "W6Z0U11nnhA",
  "url": "https://www.youtube.com/watch?v=W6Z0U11nnhA",
  "fetched_at": "2025-11-17T22:23:37.348666",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-17T22:23:37.348638",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "this is incredible right now I have llama 3.1 8B running on my M2 MacBook Pro it's scoring 72.8 on mbpp EV Val plus zero shot shout out big Zuck king of AI I don't even know what I'm saying anymore we have so much technology stacked on so much technology the abstractions are nearly endless it's been an incredible few weeks for large language model technology and for engineers like you and I building on top of this stuff Zuck and the Bros released llama 3.1 with the largest ever open model 405 billion parameters we're going to be using 405b and we're also going to be using my local llama 3.18 we're going to run them side by side and run some comparisons but a day after llama 3.1 was released we got the large enough post from mistal AI they are also cooking insane Val with mistol large 2 last week we also got the mistol Nemo model this is a collab between mistol and Nvidia we're going to take a look at this in this video and compare how mistel performs against llama 3.18 and then of course we picked up gbd4 mini earlier in this week we covered this on another video it's been an incredible couple weeks in the age of generative Ai and this video I want to ask and answer a question that touches every single one of these models what prompt format does your llm perform best with is it markdown is it XML or is it just a raw dog unformatted prompt we'll also look at an additional format that I've been curious about that I haven't seen used at all we'll validate this format in this video along with markdown and XML so if you're interested to see which format is really the best for your prompts and therefore your AI agents and your agentic workflows stick around and let's get started so so how are we going to determine if XML is really the best format for your prompts in order to know which format is the best we need to run the same prompt across these different formats and that's exactly what we've done in this code base right here we have eight tests we're going to run with four formats each and we're going to see who has the highest score at the end of it for these tests we're going to run two local models we're also going to run the brand new llama 3.1 45 B and then as a kind of control group we're running the Claude 3.5 Sonet and gbd 40 mini so let's go ahead and kick off the first test I'm going to run bun run bf1 and this stands for best format one so you can see that prompt running there if we open up the markdown version of this prompt we have a header which then has a couple bullet points underneath and then we just have our block of text I'm almost certain you've built this prompt out yourself in some format both you and I just can't be bothered to read the entire blog when we have an llm that can compress it and give us the information faster so we're just creating six bullet points here we want to start with a dash our tests have completed let's go ahead and see how they performed so we are running on top of prompt fum and it looks like so far um across the board we have 100% success rate this is fantastic but this is also a simple prompt right let's go ahead and look at what this prompt looks like in every format that we're looking at here right so you can see we have the raw format at the top here and the prompt is just kind of placed at the top and then we paste all of our text we have the markdown format where you can see we have a little bit more structure and then we have this XML format right now we're structuring the prompt in XML the tags make it really really clear where things start and end and we're able to consistently reference different parts of the prompt within the prompt and then here's the unusual format I've been wanting to look at it's the same thing as the XML form form except it's in raw Json so you can see here we have the bullet summary and if we go ahead and pull over the tests themselves we can see the actual results so we have six assertions for each test and it looks like across the board every raw prompt performed same with the markdown prompts same with the XML prompts but down here one of our assertions failed the test will still pass if more than half of the assertions were successful so this is still technically a pass but what we'll do is just to kind of keep score here we'll go ahead and remove one point so the new score here is going to be -1 for Json and then for XML markdown and raw we're just going to keep the score at zero so the best prompt format is going to be the score closest to zero this is a great way to test the prompt format because we have multiple models of different accuracy and power that we're running every format against so let's go ahead and continue that's our first prompt so just a simple bullet summary let's go ahead and kick off our second prompt here all right so let's look at that second prompt so now we're doing a YouTube chapters summary so this is a real prompt that I use and essentially what we have here is let's go ahead and use the markdown format so so the YouTube video chapter summary is as you imagine we're trying to get that chapter summary that you see in the description of YouTube videos that creates the chapters that you see on this video and other YouTube videos and in order to do this properly there's a you know several instructions here we have a couple of formatted examples and then of course we have our parsed out transcripts with the timestamps and the text within the time stamp right so this is how we're able to create a prompt that can automatically generate chapters for us but I've always wondered you know which format is ideal so we're going to find out right here this prompt is a lot more complex as you can see here by the success and failure rate so we have 15 failures and only five perer successes let's go ahead and look at the results all right so as you can expect we have high performance at of claw 3.5 son it gbg4 mini has 50% passing and then we have just some pure no outputs um I'm running on replicate so I think there's just something wrong with the way text might be formatted or something not giving us some responses here we're not going to count those scores we have three test felt here two fa here what we're really looking for here is left to right which formats seem to be continuously giving us outputs and already we can kind of see a hint of that right we have the raw format right we can take a look at this raw format putting down the generation rules we have these kind of markdown like headers but they're just you know capitalize texts and as we go down the line here we get to see something really really interesting right it looks like markdown format isn't actually performing that well in this specific test case but we do see the raw chapters performing well we see the markdown format not performing so well and then all of a sudden we have the XML format performing really really well if we go down to the Json format it looks like Sonet is actually able to pull something through for this it looks pretty bad as a format I will say this is a harder problem but this is also why I put this problem in so I'll tally up the score on the screen and let's keep moving let's now run our third test and our third test is a personal AI command selector so we'll use the markdown format just because it's the easiest to read so what we're doing here here is given a prompt and the idea is that this will come in via voice and what we want is a list of commands that the prompt is asking for right so for instance you can imagine you want your AI assistant to select some files for you to edit a file to open the browser based on the prompt we pass in we want it to automatically select specific commands and then we can take these commands and run arbitrary functions this is effectively how tool calling works so we're basically building our own personal AI assistant tool calling framework given a list of commands so let's go ahead and see how this prompt perform you can see this time we have 11 successes and nine failures let's go ahead and see the results so in this test we are looking to get this specific list of commands so the specific prompt that we use for our Command selection prompt we say select the index file readme file and select the utils directory read the diff and update the readme to describe the recent changes so you can see here that's going to activate one or more of these commands you can see that parsed properly right here but it's not all roses right if you look across the board looks like llama 3.18 is having some trouble with this and so does mistal Nemo this is to be expected the local models are as much as they're improving they still cannot follow instructions properly let's look for left to right results right so it looks like overall the raw prompt performed okay as I scroll here I'm going to tally up the scores it looks like our markdown format performed fairly well it looks like the XML format once again performed very well we actually have two successes here in mistel Nemo and we have one success here and then finally at the bottom here we have the Json format we can dig into what the test assertions look like exactly so you can see here we're running a is Json check so this just makes sure that the output is Json and they're running a couple JavaScript Checkers so we're parsing using json. and then we're checking the length we're checking the First Command second command third command so on and so forth right so this is the power of prompt testing I've made videos on this in the past I'll link one of those in the description but being able to test your prompts and do things like compare formats this is a really really powerful way to know and gain confidence in your prompts as you're engineering your agentic workflows so I'm going to move through the rest of these faster so we can get a solid score here let's go ahead and kick off our component generation so we're going to on BF4 and our component generation prompt looks like this so this is what this prompt looks like this is a classic nux 3 V3 component generation you can see here we're asking it to build a radio control this radio control is going to have a certain number of buttons it's going to have a diameter we have some rules for the prompt and of course we have an example view Nu component structure keep in mind this can be react this can be spelt this can be whatever you want it to be at the end here we have this piece of text that leads the LM to the right response so let's go ahead and look at the results here every single prompt format succeeded there let's go and take a look at what that looks like so all good across the board I'm going to go ahead and tally up the scores looks like we have one fail on llama 3.18 B and everything else was successful this is an easier prompt for every single prompt format even our new experimental Json format had really no problem completing this prompt you know at a high level looking at all these results you you can kind of pick up on the fact that for simple to even medium difficulty prompts your format doesn't matter so let's go ahead and continue let's see if our next results kind of back that so over on Bon r on bf5 let's look at the markdown prompt for code debugging and it looks something like this we have some setup here we have again some bullet points on how things should be structured and the output here needs to be a Json structure so imagine you're building your own AI code editor or you're building a code Checker this is going to given a block of code return a list of problem code an explanation for the issue and then an action to resolve the issue so let's go ahead and look at our tests these are still running you know at the bottom here we're going to specify the code that we want reviewed so this is where You' you know place a template variable instead right so if you're running this at scale inside of an application you'd have something like broken code and then you would paste in here based on your user input and then at the bottom here I'm using that leading technique teque where I kind of lead the llm to respond in a certain fashion let's go ahead and look at the results here so we have three failed 17 successes and let's see what this looks like so across the board just left to right I'll tally up the scores here as I scroll down okay so it looks pretty good overall as you can see we're starting to consistently see XML is pulling ahead not just for Claud 3.5 Sonet right because you would expect that right they literally say that in their documentation that using XML tags structure prompts will improve claws output right it it helps them from mixing up instructions with examples or context this is really really important the bigger the prompt gets and I think that's going to be a consistent theme that you see as you're working with your prompts I definitely see that theme the larger your prompt gets the more complex your prompt is the more prompting techniques you need and a lot of those prompting Tech techniques can be resolved by simply using a prompt format and being consistent with the way you're referencing in your prompts so you can see here it looks like our top performer once again is XML I am surprised to see how well the raw format is working even on Lower tier models you can see llama 3.5 8B is performing decently well same with mistl Nemo so you can see our code debugging prompt working pretty well if we you know want to dig into the issue you know there are several issues here we have a list here but we're actually passing into Tuple potential divide by zero issues we have index out of range they're definitely problems with this code let's go ahead and pull the 3.1 405b response and just see what that looks like right so we can dial in here this is one of the great Parts about promp Fu I definitely wouldn't say this is the you know most beautiful UI ever but I think it's because there's just a lot of information to fit in here but we can click in here and see the exact output let's go and just copy this output right doesn't handle the case when the list is empty just like I mentioned uh we should get a divide by yeah there's the divide by zero and then we should if we just search if we just search Tuple we'll get that issue here right so you know another cool prompt and another way to test and validate The Prompt format so let's go ahead and continue let's look at update file config so let's get this kicked off the update config file looks like this so we open up the markdown again markdown is so incredibly readable effectively all we're doing here is we're going to give this prompt a configuration file this is a configuration file from our previous video and it has some Flags you know it's classic Json stuff some bin Etc we're going to ask in natural language to operate on our config file and then return the config file with those changes completed so you can think of it as a Json editor you can leave it as an XML editor it's really any configuration file again again this is one of the powers of a great model I'm not really explicitly saying Json anywhere except for in the markdown TX here just for some formatting but this could be XML yaml or toml so let's tally up the results all right so this is little more interesting it's cool to see 405b instruct outperforming clae 3.5 Sonet and GPT 40 in at least you know one test case but if we just look left to right and T up the score here wow so it looks like XML performed really well we only have two issues here and then we have markdown with four total issues and then the default format with just three assertion failures so again really really great performance out of XML let's go ahead and continue and wrap up our final two prompts all right so the next prompt we have a question answer chat bot this is one of the original use cases for llm we're passing it a name we have a user question coming in my deployment isn't deleting what's the issue so the user is trying to delete a deployment of this service and then we have a bunch of documentation you can see in the bottom right this is going to be a 10K token prompt so on the higher side all right so for QA chatbot we have to kill our local models the 10K tokens running on my MacBook Pro is just not running local models are just not there yet so I'm going to go ahead and get rid of that and we can run this against these three remaining models let's go ahead and kick it off again all right so we have six successes let's check the results here all right so we're getting some weird results out of 405b instruct getting some kind of weird stuff there so I'm not not going to count anything from 405b here looks like it was really really close we actually had XML fail on one test assertion super interesting let's go ahead and bring these back and run our final test bf8 and this is script to key ideas so the idea behind this prompt is really simple you have a script or a piece of content that you're working with a piece of media maybe it's for a Blog maybe it's for a YouTube video maybe you're giving a presentation maybe it's a draft whatever it might be you want to take it and you want to get the high level takeaways you want to get the key ideas a couple title ideas and then some quotes from the script so we have an outline we want this in markdown format couple instructions here and then of course we have our script so that's what's running here and our local models should be able to run this okay so 12 success eight failures let's go ahead and tally up our final results so the local models just could not do this there's too much context inside of every one of these prompts it is interesting to see that at some level the format doesn't matter if the model itself cannot perform well enough but we can see here all of our top-end models performed every format perfectly this is really cool to see and we can dial into the prompt formats here just a little bit right so here's the raw prompt format just top to bottom nice and simple dump the script at the bottom here's the Markum format and you can see here there's some structure we make that look good the markdown format definitely beats out every other model on readability we then get into XML XML is the right format for the models I don't see the score rate here but you know just doing rough math keeping track of everything here um XML performed the best then we had raw marked down and then in the last place we had our experimental JSM format but you can see the results up here on the screen I don't actually know the real numbers here I'm going to count them when I edit I can almost guarantee you uh the XML format is the best format for your prompts when you want to maximize for prompt performance so we're talking prompt accuracy and we're talking reproducibility if you're running a model in a production environment where you have users where you have Revenue that is dependent on the success of your prompts I highly highly recommend you go with the XML format if you're building prompts for readability and you can sacrifice a little bit of performance I highly highly recommend the markdown format as I mentioned it is just the most readable format and we can just take a look right so this is that final prompt it's so easy to read this to see the structure with the header tags to see bullets to see lists to see bullet lists and then to also structure content and Nest content right versus our experimental format Json it looks pretty good until you have to start making edits to the content until you have to start nesting there's no real space in Json for nesting other content formats XML on the other hand gives you everything you need and it actually is a very kind of clean and beautiful format in its own way the readability of this format I think definitely goes down but again if you're building this at scale you're productionizing this you need the best performance I highly recommend using XML prompts all the power of XML really comes from having consistent references throughout your prompts whenever we say certain keywords so for instance we have script when we say given a script and then we have a tag that says script exactly the llm has zero confusion about where to find this information it seems the other formats have that same type of power but not as much as XML also surprisingly I thought the raw dog prompts would do worse but of course with our higher and high performing models llama 3.1 405b gbd 40 mini and Sonet it does seem like the prompt format doesn't matter so much so in summary we do have to agree here with anthropic overall if you want the best prompt format use XML tags if you want your prompts to be very readable I recommend using markdown it's cool to see the research and the Ingenuity that anthropic puts forth get validated in some local tests with the top end models and some of the lower tier local models I completely agree with their blog here using a prompt format specifically using XML format gives your llm Clarity accuracy flexibility and parability if you haven't read this definitely give this a read I'll link that's in the description I'm going to continue using markdown prompts because I love the readability of them but when I productionize and ship things into code I will be using XML tag format because as we saw in this small test set XML outperforms every other prompt format if we were to increase the size of our tests and add additional insertions I would expect that Trend to continue if you enjoyed this you know exactly what to do thanks so much for watching and I'll see you in the next one",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "llama-3.1, mistal-nemo, gbd4-mini, prompt-formatting, llm-benchmark",
      "generated_at": "2025-11-17T22:23:49.589556",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}