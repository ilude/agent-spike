{
  "video_id": "28zuliyLd5Q",
  "url": "https://www.youtube.com/watch?v=28zuliyLd5Q",
  "fetched_at": "2025-11-17T22:26:53.601905",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-17T22:26:53.601871",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "every second you can save your user is a second gain in user satisfaction and therefore your earnings potential there's a powerful use case for llms hiding in plain sight and the best part is this solution we'll discuss can be reused inside your tools and applications across every domain the autocomplete is a powerful use case for llms hiding in plain site since the solution stays close to the prompt it's likely whatever open AI has to announce today will only add value to your autoc complete here's exactly what that looks like we're not going to touch this yet let's talk about the Omni complete the autocomplete for everything domain agnostic Auto improving user autocompletes this is only possible because of llm Technology before llms we had powerful drop downs and autoc completes but nothing like this the ability to take a BAP to take a huge prompt and to ask your llm to autocomplete any any domain that you have sufficient information for is gamechanging and in this video I want to show you exactly what that looks like to help your users get accurate and fast autocompletes so let's break down the big Ideas why do we care about llm autocompletes why are they so different than traditional autocompletes LM autocompletes can self-improve with every single completion the code base I'm going to share with you has a simplistic yet effective way to on every completion your users make on your tool on your website it's going to reinforce the most popular autocomplete in doing that I'm going to show you exactly how you can build prompts that auto improve with each use we care about llm autocompletes because they are easily reusable across your tools and application there's one prompt and there are three variables inside the prompt that you swap in and out based on the context and based on the domain you're solving for that's it it's not a stretch to say that after you watch this video you will not have to write any more autocompletion code you'll only need the prompt I'm going to share with you in this video so the interesting thing about llm autocompletes imagine you have an FAQ on your website right it has um a couple of the most popular questions that users have directly reached out and asked you when you have an autocomplete powered by an llm the users can literally enter in any question they're looking for and that gives you actionable information about your prospects and users you have quantifiable data about what your users care about when it comes to your tool when it comes to your product this is gamechanging please do not miss the the importance of this bullet point from a business perspective this is the biggest takeaway of this video llm autocompletes reveal actionable information about your prospects users and customers visiting your websites and then lastly I'm just going to share the code base that I use to build this tool right here I don't always have time to share complete code bases that are digestible that are easily set up but when I do I I love to share them with you I'm really starting to dig into code bases that have prompt centered architecture so we have the client we have the server and then we need a concrete place for not only our prompts but a place to test our prompts and to validate our prompts and in this video I'm going to share with you a concrete client server architecture where we have a prompt centered architecture everything is built around the prompt so again we're going to demo this in just a second we're getting there um but first let's talk about the highle architecture if we don't understand the high level nothing else matters so here's the front end in the back end I was talking about this is a classic architecture we have a single front end and we have two backend end points that we're going to be discussing right now so what does it look like so imagine this prompt comes in from a user how does unoc CSS compare to tellin CSS Uno CSS is a atomic CSS engine uh feel free to look that up I'm using Uno CSS uh vue.js and uh tellwin CSS just as examples here for our domain specific knowledge for our autocomplete so imagine you're the owner of Uno CSS and you have an omni complete on your website so this is an llm powered autoc completion and they type how does unoc CSS and then based on all the information that you've had from previous visitors you create this Auto completion for them how does that look between our front end and backend so as soon as I type how does unoc CSS what happens is we hit this endpoint here slet autocomplete is going to do is look at all the previous results from users and the hit rates of each one of them and all of your domain specific knowledge for your tool or application and it's going to run a prompt that has this information inside the prompt so it has both the hit rates previous completions and it it also has your domain specific knowledge so that's all in the prompt over time that prompt will turn into a BAP just because as your tool or application grows your prompt will grow larger I'll link our Bap video in the description this endpoint is going to give us all of our autoc completions ranked by hit rate if there's no autoc completion we're going to fall back on our domain specific knowledge this is going to hit an llm provider what that is exactly doesn't matter in our Omni complete example here I'm using Gro on top of a llama 70b model Gro is fast so it's perfect for use cases like this where you need results fast so this gets you an auto completion right so so now our user has the compare to tell when CSS completion soon as I hit tab our second backend post request gets hit and that's going to be our Slash use autocomplete and what this is going to do is increment in store both the input and the completion and this simple architecture is all we need to build an omni complete an llm powered autocomplete that can change domains with minor tweaks to your prompt so let's go ahead and actually run this right how does this look in Productions we have this Omni complete here and remember we're just focused on the autocomplete functionality when we hit enter we don't really care what happens next that's up to your tool to figure out and solve right so let's go ahead and run this exact example right how does unoc CSS so go ahead and just run that so we'll say how does unoc CSS compare to tell when CSS okay if we hit tab here we can autocomplete and then we can hit enter here right Fant fantastic so whatever your application does it takes the prompt from there right let's go ahead and run another Omni complete example so let's say easiest way to set up so this is a good one so easiest way to set up un CSS in a vuejs project right and we get the auto completion it's really important to show expertise on your websites and in your tools right when you can autocomplete an llm power field for your users it tells your user consciously and subconsciously that you know what they're looking for this auto complete is spoton and it's something that someone might actually query on our unoc CSS website right just as an example right so this is awesome so we have autocompletes that are domain specific powered by our llm that are self-improving right every time that autoc completion happens it's more likely to happen again for additional users that visit the website so let's start by looking at the prompt the most important piece of this application so if I type prompt here you can see we have this simplistic prompt let's walk through this so I'm using a prompt in the markdown syntax so I have an H1 here and then I have a couple of H2S here my H2S represent important information for my prompt for the llm so you can see here we have a couple of rules so this is our generation rules you closely follow the generation rules I'm using the capitalize reference prompt variables anytime I refer to one of these variables it has a corresponding place as a markdown header item and this lets our llm have a place to concretely reference that information so you can see we have a bunch of rules here this is guiding our llm guiding the prompt to make the completion properly and then we have the important information down here right we have four variables we have the topic the topic is going to be the highle domain we're operating in so in our example we are operating in vuejs and Uno CSS the previous completions is going to be our list of previously completed items we'll dive into that in just a second here we then have our domain knowledge this is a plain text file that has all the information you need your them to know to answer questions about your tool or application and then finally we have the completion that we want our user to solve so in the previous example when we type something like how does uno CSS pair and then we wait a little bit and get that auto completion to tell when CSS our prompt in here our Auto completion will just be this right so and let's go ahead and take a look at what these previous completions look like so this is the most important piece this is how we're able to self-improve to Auto improve our application so let's go ahead and take a look at this and it's pretty simple let's collapse everyone except the top you can see here we have just like we looked at we have the input how does un CSS and we have our completions right we can return more than one completion we're only using the top so right now we just say compare to tell when CSS this has nine hits so I've been testing this I've been building this and I have this up to nine hits so let's go ahead and look at that full flow of what this looks like right so we'll refresh here and then we'll type how does unoc CSS bam so we have that completion come up I'm going to hit tab look what happened over here on the right side this incremented from 9 to 10 this is how we're able to constantly improve over time and this Json blob right all this information every single time our prompt runs this gets dropped in to our previous completion and then we're referencing our previous completions and a couple generation rules just saying hey if we have a completion use the top hit right use the always prefer the completion with the highest hits right and that's exactly what we have here so this is how we're able to take a Omni complete and just make it self improve on its own and notice there's nothing in here that is specific to VJs or oocss right this is the most important piece this is what makes the system an omni complete system right all you have to do is come in here switch out your topic switch out your previous completions and then add your domain knowledge and start firing off autocompletes right so this code is going to be linked in the description if you want to go ahead and check this out if you can understand how this can drive satisfaction for your users and you can see how you can take this prompt and really use it for your specific use case go ahead and hit the like hit the sub um you know I deeply appreciate every like and every sub that we've got on the channel uh it means a lot that we can focus so deeply on real technology solving real problems using llms in real use cases exploring different ideas and the fact that so many show up to really uh you know push what they can do with their engineering means a lot so if you would hit the like hit the sub I definitely appreciate that and let's go ahead and talk about the architecture of this application so like we looked at it's a simple client Ser application but there are a couple important pieces here that I want to highlight so on the top level I've kind of dropped in a vue.js application here and you know we just have like classic stuff here this is your react code this is your spell code your solid JS code whatever you like to use um if you ask me what frontend framework you use is completely irrelevant nowadays they've all essentially converged it's really about how quickly can you get to production and how quickly can you provide value to your users and every once in a while you need a really efficient front-end system that just run fast but again usually that's all about the cloud infrastructure running on anyway so we have our frontend file there we then have a simple python flask server and if we hop in here you can see that this is serving up those two routes use autocomplete and we have our get autocomplete right so just as you'd expect we have a simple switcher here if you want to change topics you can go ahead and just add another tupal and update the index and then we have something really interesting here we have prompt tests I've been experimenting with different ways to get prompt validation get prompt testing inside production code bases so basically created this pattern where we have a specific type of prompt that we want to solve and so this is the prompt file we were looking at before and then we have the knowledge based information here right so if we go into this you domain knowledge. txt you can see it's just a text file this file will grow in size over time eventually this will be a BAP it'll contain a ton of information about your specific tool or product but you notice here this is a knowledge base specific to the VJs problem solve and so this is where you can break things out right you can come in here and add another uh domain problem set so let's say we were talking about I don't know AER so in here you would add another domain knowledge and previous completions and then you would basically just go to Main and update topics to fit exactly what you were trying to solve and I think there's one more variable here on the front and I just use as a placeholder inside the app. viw component just topic right there right and then that's it after you change those couple items you have the Omni completion ready to solve your specific use case right the llm powered autocomplete but so the interesting thing about this architecture is that this actually integrates directly with prompt Fu I'll link previous prompt Fu videos in the description prompt fu is a prompt testing framework that can help you gain confidence in your prompts but you can basically see here I'm testing on llama 3B and 70b Via Gro I've got a simple assertion I want to make sure I'm always getting back Json and then I have some tests right so you can see here I'm actually routing my previous completion inside my test variables directly to my vue.js previous completions and domain knowledge text files so this architecture allows me to directly integrate my confidence in my prompts and we can go and just run this right now so so I open up a new terminal here just type yarn test and this kicks off a promp Fu evaluation so it's running all of these tests I've set up on my prompt and it's passing in the prompt variables specified in the test right so you can see here we have topic input value previous completions and domain knowledge and these are getting wired in Via a JSM file and a text file and that all gets placed inside these variables right for each one of these tests and it runs twice because I have two providers and if we go ahead and open up the prompt viewer view you can see that we have these completions running right so I wanted to share this architectural pattern with you just because I think it's really important to know that your prompts are working and to gain confidence in your prompts and the best way to do that is to literally share your prompt and the way that you you know replace and template out your prompts with the real information to have have them ready to go in your test our server is looking directly at this test file there's definitely better ways to configure this I think this is just the fastest most direct way to both validate your prompts and ship them to production let's go ahead and run one more example what I want to do here is look at what happens when we run a brand new query right so let's open up our previous completion let's say we've just created a new application we'll go ahead and actually clear it out and we don't have anything for our users um this is the beauty of the llm and this prompt in particular right if we don't have any previous completions our llm is going to fall back on our domain knowledge which is just a raw string of everything we want our llm to know to help answer questions solve problems for our users even with nothing we can come in here refresh and just start asking questions how to integrate want to CSS with view okay so we'll go ahead and hit Tab and there you go so we now have a brand new hit and I do want to call up that I'm just using a Json file here you might want something more intricate you might want to save this to a you know SQL database nosql database a KV store whatever and however you want to store this information is completely up to you it's all about the concepts this information is going to use in the prompt test and in our production server however you want to divide that up definitely go for it but so we have this automatically updating right this is essentially our database for this test right so let's go and run another one right so if we said something like how does CSS pair to tell and CSS okay classic one so you can see there we just added that hit let's ask another question do I use the same okay so I have another question do I use the same presets for window CSS as I do for wind dcss and you can see here if we just you know copy this and search when dcss this is in our domain knowledge so this is all getting pulled right from our domain knowledge as a fallback if I run this again though do I use the same and then autocomplete you can see here we have another hit for that autocomplete and we also have a sorted file so that's going to bump to the top so I think the Omni complete and the idea of the llm powered autocomplete really points out an interesting use case for llms if you have the knowledge and you have the information you can kind of merge the old way of doing things with just a bunch of raw code a bunch of business logic with the new way which is you have a prompt you have domain knowledge and you have information that fuels some type of use case you keep your domain knowledge and your information that FS a use case separate and then you put that into a prompt that can iterate on the variables that you're giving it right I think the next big phase of llms in 2024 and 2025 is really going to answer the question how can you use your unique domain specific data and knowledge to feel interesting prompts and user experiences to solve real problems there's a lot of talk about us being in an AI bubble or an llm bubble and frankly I don't think it matters either way if all develop stopped right now we would still have so many Incredible use cases so many incredible products and tools to build out so I just want to leave you with that idea think about how you can take your existing workflows your existing engineering development product workflows and enhance them with a self-improving auto improving prompt based on automatically updating data that your users drive this really starts to push us where we're going on this channel which is we're going to build software that is living software that builds you on our behalf while we sleep if that sounds interesting to you if you want to follow along the journey and even if you don't believe me and you think it's total BS um hit the like hit the follow join the journey I'm pushing on this I'm dedicating my career to this technology we're going to see how far we can push it thanks so much for watching and I'll see you in the next one",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "omni-complete, llm-autocomplete, prompt-centered-architecture, domain-knowledge, prompt-testing",
      "generated_at": "2025-11-17T22:27:01.360743",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}