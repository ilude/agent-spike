{
  "video_id": "Cy1Z8J0anKw",
  "url": "https://www.youtube.com/watch?v=Cy1Z8J0anKw",
  "fetched_at": "2025-11-17T22:29:23.017480",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-17T22:29:23.017451",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "llm benchmarks are really great and they're very important but they're hard to truly understand and really comprehend every llm provider comes out with some clearly a little biased slightly fine-tuned prompt comparisons and absolutely there's a lot of Truth in what they're sharing but you always have to take it with a grain of salt of course the llm provider wants to promote their own llm over others so that developers like you and I try their llms and spend our time and money checking out their tech for that reason I wanted to create a type of Benchmark that you can really feel so I built this simple benchmarking tool last llm standing wins it's really simple you select how many lives you want you drop in your prompt testing results and then you let your llms battle on their specific models so before we dive into this anymore I'm going to let these models duel we have a Gro model here we have vertex Pro we have GPT 3.5 turbo and of course we have the new claw Sonet so we're going to let these LMS battle and we'll walk through exactly what's going going on after so here we go so as you can see right away Gro is ripping grock is already done with the 10 prompts gbt 3.5 finishes Gemini Pro finishes and it looks like claw 3 Sonet doesn't make it to the end here so let's talk about what exactly just happened there right so each one of these squares represents a single prompt test and every llm runs the same test if we hover over one of these squares you can see here the prompt is this so given the natural language query select five users generate the SQL that's all this prompt is doing given a natural language query we want to have the llm generate the proper SQL statement for us you can see at the bottom GPT 3.5 generated the correct select statement select star from users limit 5 so that runs for every single model and every provider each llm is given specific Awards so you saw here Gro finished first but it also finished with the least cost it got the Frugal award we have two bullseyes so these llms completed with no mistakes and and we had claw 3 Sonet die and it also was the first to die first and only to die so it got the Deb award it got the first Blood award and it also got the Grand Theft Auto llm award which means it cost the most let's go ahead and run another test let's go ahead and bump our lives up to four and let's run this so this is a battle between the top two llms currently with their apis open to the public so this is open AI one of the latest preview versions and of course up against clog 3 Opus they have four lives each if you're excited for this llm battle and you're super stoked for the results here hit the like hit the sub and let's watch the two best models battle it out so right away you can see gbg4 quite a bit ahead claw 3 Opus lagging but still 100% accuracy love to see that for both of these models they have four lives so let's see how far they both get so far so good they're running 20 natural language query to SQL prompts and there we go so we got some errors okay so anthropic just bombed four in a row and we're we're going to have open AI GPT 4 okay so we got two errors from open AI gp4 but it did complete this was the fastest LM alive but it also got the Grand Theft LM award so it did cost us the most out of these two models of course claw 3 uh bombed four in a row here so it lost all of its Health it died it also got the first Blood award and the FR LM here doesn't mean much since it's you know just two models up against each other here is a really big exciting example here's the big final set way llm battle we have gro we have gbg4 we got vertex we got gpt3 we got three variants of claw in here but before we dive into this one I want to show you exactly how this works we're standing on top of a framework called prompt Fu we've discussed this on the channel before I'll make sure to link those guides and those videos in the description so you can get caught up on how you can write tests for your prompts to gain confidence in the value that your llms are bringing to your work to your products and to your tools it runs right in the CL lie and it also has a nice user interface for you to look and Visually see the results of your llms real quick we'll take a look at the code you can see here we have our prompt Fu configuration file and let's just go ahead and create another test quickly let's do a Speedster test so this is going to be a test between kind of the two fastest and most accurate models out right now gpg 3.5 and grock mixol let's go ahead and look at the tests that are going to be run against those items you can see here we're running nlq to SQL here's the prompt that you saw in the tool tip so just as you saw before you know given the natural language query bam and then the database dialect bam is going to be postgress we're giving some table definition information and then we're asking it to fill in the SQL statement that will satisfy the natural language query we then run a suite of tests so this is what the individual tests look like let's just go ahead and collapse here so we can just take a look at one of these so you can see here nlq to SQL we're passing in the nlq the database and then this is how we're asserting any llm block here that you saw that lost their battle let's refresh and go ahead and let these duel again so any llm that lost and got an invalid response was unable to assert these values or something went wrong with their API call right I think in one of these tests anthropic is timing out in a lot of these cases right just cuz their API is getting absolutely destroyed right now and they also have strict rate limits on their API so that's why we're seeing some of these clawed errors is actually performing a little bit better than we're seeing here in the test but it's still really cool to see it visually how it comes out so the interesting thing about these tests you're seeing is that these are deterministic tests so it's running on a prompt Fu configuration output file that has already completed let's go ahead and run a Speedster test for these 10 items inside my package here I have nlq tosql 10 and it's going to run on top of that promp Fu configuration file so let's just go ahead and do that right now so we'll say yarn nlq 10 and let's make sure that we don't have anything else in enabled otherwise they'll show up as a llm trying to join the battle we just want turbo and Gro so let's go ahead and let them battle here let's fire that off and so right now in prompt Fu with a delay timer both gbg3 and Gro mixl are running right now and Bam so we just saw them complete we have one failure out of 19 that's great but if we come into the output file you can see here in the package.json we are actually outputting these to a output 10. Json file so we open that up you can see here we have you know all of the results so it's results we have stats and then prompt food developers give us a nice set of information here to work with from our test output we focus on the results so so what we can do here is just take this we can open this go back over to our application refresh and then here we're going to drag and drop a brand new llm Last Man Standing battle we're just going to drag this over drop it in and you can see here we have a battle between Gro Mixel and gbg 3.5 turbo so let's go ahead and see how these two compare to each other let's see who can win first right this is really a speed test between guo and 3.5 let's let them battle all right so yeah really impressive but interestingly right so this is why this is so cool and important to see how this really looks and feels right just by seeing this visually we now know that you know grock mixol and you know I realize that this is spelled wrong forgive me but we can see here that Gro Mixel is really fast but you know it did get a prompt wrong right did bomb on this test here for whatever reason the assertion statement for this test did not go through at the same time we saw that complete very very quickly you know we can refresh go ahead and drag and drop again and we'll run it again and really just like it's so cool to visually see this right Gro is almost twice as fast as 3.5 but as you can see gbg 3.5 is more accurate in this test case scenario so really cool to see that we got the fastest LM alive over here on the left but we have the bullseye on the right at the same time though you know grock least expensive really cool to kind of just visually see this comment down below and hit the like hit the sub if you guys are interested in this getting hosted so that you can you know run your prompt food test and then drag and drop it in here into what will be a you know just a simple hosted website and then you can kind of watch your llms duel this is probably something that I'll be adding to the indev tools at some point just because it's something that I really enjoy using and it definitely fits the purpose of the indev tool tool set so it's all about you know building a gentic software it's all about understanding s it's all about you know building llm software and Building Technology that will help you you know create your reusable building blocks and really dig into these you know agentic principles like prompts are the new fundamental unit of programming one of the key key parts of building important software is making sure that you have tests and making sure that you really truly understand the technology that you're building with and the technology that you're building on top of so I'll probably build this into the indev tools but let me know if you guys are interested in a tool like this this doesn't have to be a public thing I can keep this private I just thought this was really cool and wanted to share it with you here so let's go ahead and look at that final battle here this is a 20 set prompt so there are 20 prompts here it's all nlq tosql tests we have a lot of llm providers here the cool part about this test is it's all built on top of promp Fu so after you run your test and you know you specify an output file right using you know uh their D- output flag here and you go ahead and get your results into a concrete J file then we can drag and drop and then just run a test right so it's really as flexible as your prompt food tests are so whatever your Outlets are here basically is just built right on top of promp Fu and it's a more intuitive way to really understand what your prompts are doing enough talk let's go ahead and let this last big massive llm Battle Run let's see who the last llm standing is here so right away you can see grock is Off to the Races that lpu is is insane we got some dead claw dead claw models and we have is this gbg3 very nice gbg3 with two errors only okay we have grock dead we have uh Gemini Pro dead and gbg4 nice gbg4 lives with one Health gbg 3.5 turbo lives here with one Health um everyone else is dead here we do have some awards so Mixel ran so fast that was really kind of difficult to see let's go ahead let's go ahead and run that again it's it's so cool to see how fast mixt is they'll just kick this off again and and these down here these are just inmemory files so if we go ahead and look at the code here I just threw a couple of these battles just right in memory right so I fired off my prom food test using the D- output flag and then just copied the results directly into these battles and all the information is right here that's what these three starter battles are let's just go ahead and watch how fast grock is here I think it really shows you know being able to see this visually really shows how much faster grock really is bam so fast but also we have to shout out 3.5 turbo probably one of the least appreciated models is open AI gpg 3.5 turbo it's fast it's very accurate and we can see here uh you know to be fair I'm pretty sure all of these claws are just hitting a rate limit that's on me if you guys are interested I'll set up better tests to share on the channel drop the like drop the sub we can have more llm Last Man Standing battles I'm pretty sure anthropic is just timing out here both with rate limits and multiple times I've had issues with their API they're getting hammered a lot right now but we had gbg4 come in Finish strong and you know vertex Pro not too bad it made a decent way there but it did ultimately you know bomb a couple of times not satisfying The Prompt assertion statements really interesting way to kind of feel the prompt sure you can relate to this like it's so great seeing these like hardcore benchmarks with a bunch of really intense problems and really domain specific things that you and I care about and you know the llm industry as a whole cares about because you know they really dial into how powerful is this at solving really intense at the edge of field problems these benchmarks are really important but they're really hard to feel they're hard to understand which is why I think something like Last Man Standing something visual that you can just see like you know let's just take four models and give it my personal prompt right I think that's a big Advantage here when you're prompt testing these are all prompts that I want to see how my personal prompts I'm building for my product products compare when used with different llms with different providers and that's really the Big Value here of not just the last man standing but just prompt testing in general I highly highly recommend you guys check out my previous videos and more importantly just check out prompt food this is a really really important tool in the age of LMS in the age of AI I've said it a million times on the channel and I'll keep saying it the prompt is the new fundamental unit of knowledge work so it's super important to truly know and understand how powerful are your prompts how good is this llm right really know what's happening you really want to be truly comparing and really internalizing when should I use this llm when should I use this prompt how good is this prompt so you know there are a lot of really interesting things that you can test and tweak with your own llm tests I just want to shout out promp Fu and all the engineers working on it it's such a fantastic tool as a concrete example I plugged this product on the channel a couple times but I'm building this product called talk to your database it basically takes your natural language queries and converts them into SQL and automatically runs them not just text tosql it's text tosql to results copy Json you can export you can do whatever you want let's just go ahead and open up the app right now and so when you're you know building out your application it's so important to know that your prompts are going to work right even before I hit enter here I can tell you with 99% certainty if my nlq is going to work because I have tested it thoroughly let me just search for a good query to make so you know something like products Price Less Than 5050 and quantity between you know 2 and 5 before I even run this I know this is going to run because I have prompt tests that have already validated it right not only did it run it ran insanely fast as you can see here we got no results there's nothing in the database that fits this criteria let's go ahead and play with that a little bit I'll tap that and let's do a quantity between you know we'll do 5 and 15 and we'll do price uh equal to greater than and we'll fudge some of the spelling let the llm fix it for us right bam already complete and nothing there either let's go ahead and get some cont rete result out of this so let's go ahead and drop this down to 20 I just actually want to see some concrete results we'll do 3 10 let that run bam and we can see that we got one product that fits this exact criteria the point here is that you want to be absolutely certain when you're building your products that your prompts are doing exactly what you think they're going to do I don't want to over plug this if you write SQL on a daily basis I highly recommend you check out this application I built it for myself and other Engineers you know that spend a decent amount of time in databases operating on data there's a free forever version and if you want to upgrade you absolutely can we just rolled out the agent configuration for pro members and of course it's got Gro in here the hypers speed SQL agent this thing is fast it's helping really Propel the application forward but again I just want to pull this all back and say I couldn't have built this into this product into that application without first really truly testing to make sure that I'm confident that these SQL statements will be able to properly run in a production database I just want to pull that curtain back again and you know just say that it's really important to test and have proof of value and fully understand and validate that just like you know your code is going to run the right way you really want to know that your llms are going to run the right way right and give you the right results you looking for it's so incredible how well gbg 3.5 turbo performs it's fast and it's accurate and you know inside of the talk to database application that's how I've kind of graded it in this simple performance measurement and this is all thanks to again prompt testing there's no way I could know this and build this into an application and there's no way you can know this and build this into your applications and your tools without you know first really setting up some really clean tests and running it there's not going to be any code for this yet so again let me know if you guys are interested in this prompt Fu Last Man Standing prompt testing visualizer and definitely definitely spend some time looking at prompt Fu this is again the best way to test your llms right now in the age of AI in the age of prompts where the prompt is the new fundamental unit of knowledge work thanks so much for watching hit the like hit the sub and I'll see you in the next one",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "llm-benchmarks, last-man-standing, prompt-testing, nlq-to-sql, prompt-fu",
      "generated_at": "2025-11-17T22:29:30.250961",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}