{
  "video_id": "bAYZjVAodoo",
  "url": "https://www.youtube.com/watch?v=bAYZjVAodoo",
  "fetched_at": "2025-11-09T19:23:28.566742",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "single_import",
    "imported_at": "2025-11-09T19:23:28.566742",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": false
    },
    "recommendation_weight": 1.0
  },
  "raw_transcript": "I talk a lot about AI stuff on this channel, but there's a couple AI things you might notice I haven't mentioned a lot. One of those is MCP, the model context protocol. You might assume this is because I have some crazy hatred for anthropic and I'm just avoiding talking about their stuff, but it's not that. It comes down to something much simpler. I try to talk about the things I'm actually using, playing with, and working with every day. And I found that for my stuff and my work, MCP has not been particularly useful. Seems like I'm not the only one who feels this way because Cloudflare just put up a post that is very exciting to me. Not just what they are releasing here, but the concept of MCP not really being the best protocol and a future where we can do something much better. Don't want to spoil everything just yet, but to give you a little teaser, the concept that they are working on here is instead of doing MCP calls by listing all these tools for the model to call directly, what if you gave it a TypeScript API and let the LLM write code to call the API? This is a really interesting concept and I'm super excited to see how this works in the real world to break down the work these guys did in particular Sunil who's been a good friend for a long time. Really cool stuff that Cloudflare's cooking here. Can't wait to break it all down. But first, quick word from today's sponsor. We've seen a ton of innovation in how fast development teams can move thanks to all these cool new AI tools. But there are certain things that we haven't seen move very much. It's kind of crazy when you think about it. We can make a PR five times faster than we could before, but actually building and running that code takes as long if not longer than ever. Unless you check out today's sponsor, Depot. These guys really get builds and it shows. Knocked Post Hog's build times down by 17x from over an hour and a half to under 6 minutes. Mastadon got 38 times faster from almost 2 hours to under 3 minutes. Genuinely unbelievable how much faster things can be when you have a proper infrastructure for doing builds. If you're wondering how they do it, it's not super secret. It's just a ton of annoying work that they've put the effort in to do. They set up their own action runners with way faster CPUs, 10 times faster networking, unrestricted concurrency, and the caching is just so much better. As a result, they also rethought Docker builds to really take advantage of the different layers as you're building, making it way easier to cache and restore things. It's so much faster with Docker images. They actually introduced a product on top of it, their remote agent sandboxes for cloud code. You can set up your development environment with everything you need to run an agentic session. And now it's just cached and ready to go. All of a sudden, you can run a command and run cloud code in the cloud on their info with all of the performance benefits that we've been talking about this whole time. Must be super expensive, right? Yeah. 20 bucks a month. Pretty insane deal if you ask me. If you wish your CI was faster, your Docker builds were cached better, or you just want somewhere to run your cloud code instances, look no further than Check them out now at soy.link/depo. You know it's going to be good when the first sentence is, \"Turns out we've all been using MCP wrong.\" Most agents today use MCP by directly exposing the tools to the LLM. We tried something different. Convert the MCP tools into a TypeScript API and then ask an LLM to write code that calls that API. I'm going to show you guys something fun quick. Let me open up my terminal. So, I was hanging out with the creators of Trey a few days ago, and I had a lot of questions on how it worked internally. If you're not familiar, Trey is by the Bite Dance guys. It's their attempt at building their own from scratch VS Code fork all-in-one code AI IDE thing. And when I was playing with it, I noticed that the quality of the outputs just wasn't as good as what I was getting from other similar tools. And it was getting stuck calling all sorts of things and not really doing what I expected it to do. And I was working with them trying to figure out what was up. So I just asked Trey, \"List tools you have access to. If somehow you're not already familiar with tool calls, the really quick TLDDR is that a tool call is a format that you can let a model output to then go run code or do some specific task. So if you want the model to have access to like weather information, you can tell it use this syntax tool name equals weather value zip code equals 1 2 3 4 5. And if you tell it about this format and that you'll respond in a certain way, now the model can put in this text, stop, wait for you to run code and get a response and then keep generating accordingly. It's a core pattern to how all these AI things got so good. It's why all these editors and whatnot are as powerful as they are. It's the whole concept of agents is largely based around the power to do these tool calls and function calls. And MCP is meant to be a protocol for standardizing these and letting you build more dynamic tools. in particular, the idea of exposing these tools as a web service or thing on the internet that you can just plug in by copy pasting the definition of it into your AI tool of choice. So with a thing like Trey, I wanted to see all the tools they're giving their agent in order for it to be able to do AI code stuff. So here I told it just list all your tools and it does. It has a run agent solo document which is how it documents what it's going to do. It has two code development tools which are both also agents. There's the run agent solo coding and run agent solo dev builder. Two different agents that it can trigger via a function call. Okay, interesting. Keep going. Code analysis and search. It has four different tools. One is for just searching generically. One is searching with reax. Then there's view files and view folder. Then we have check command status, web search and more. But the most important thing here is there are three separate sub aents that can be triggered. So, I asked it to run one of these, the solo coding agent, and have the solo coding agent list all the tools it has access to and write them to a file named tools.mmd. And it did. Here's all of the tools that the code agent has access to. It can write to the to-do list. It can search the codebase as well as search by reax just like the other ones can. For file operations, it has like eight tools. View files, view folder, edit files, update foe, edit file, fast apply, edit file, rename, delete file, write to file. It's a lot of commands. And then we have command execution. It can run commands, check command status, stop command, edit environment, open preview, get preview console logs. And then even though this project isn't using superbase, it has access to superbase tools and stripe tools, LLM configs, deployments, web search, and more. This is the problem. It has so many tools. And what we've found is that these agents get significantly worse when you give them too many tools. I firmly believe part of why the experience with Trey is not as good right now is because of this. And the team's been very receptive to this idea and they're currently experimenting on reducing the number of tools to see if the quality of output goes up as a result. So I asked Codeex to do the same thing. List all of the tools you have access to. And here they are, all four of them. Functions.shell shell for executing shell commands. Functions.update plan for updating the plan. This is the only way it can make changes to the to-do list. Functions view image. This is one for viewing an image when you paste it in the terminal. And knowing that has to be a tool, but yeah. And then apply patch, which is how it actually applies patches when it writes them. That's it. These are all of the tools that you have in codeex. And it behaves significantly better. Let's do the same with cloud quick because I forgot to list all of the tools you have access to. Write them to a tools.md file. If you're ever trying to get this info out of a tool and it's not giving it to you, like you're using a CLI like this or some vibe coding tool on the web, one of my favorite tricks I picked up from Simon Willis was to ask it to generate a website describing all of the tools and it will very excitedly go and make an HTML file that gives you this exact information. Here is Claude's tools. Actually has more than I would have expected. There's read, write, edit for files. They have a separate tool for editing Jupyter notebook, which is interesting that that got left in here and isn't like dynamically turned on or off. There's Glob GP for search and discovery. I like they only have these and they don't have a bunch of random search tools like a lot of these other things do. Being really clear about how to search and find things is important for these AI tools cuz otherwise you'll just kind of go crazy. Execution has bash. Bash output for retrieving things from bash and then killshell. Web access has web fetch and web search. Task management has multiple things for tasks which is interesting. Most of the tools I've seen actually try to condense this into one call. And then the mode control for exit plan mode and slash command. So that's not too many tools in general. I did just have someone share the tools that are in cursor. This is a bit more than I would have expected. Still just one codebase search tool. then run terminal command then grap which I guess is also search codebase delete file web search update memory very useful to have this as a tool that I can call to add information to the memory going forward read lints edit notebook again Jupyter notebooks its own special magic thing writing to-dos edit files read file list directory glob search apply patch multi-tool use in parallel in order for it to do parallel tool calls that seems to be it I do know for a fact that cursor will change which tools are available for different models. Like for a while, the to-do tool wasn't made in available for GBT5 because it was having problems with it. They tidied up their system prompt, got it working better, and readded the tool. Why am I just explaining these lists of tools across all of these different AI code editing solutions? Because as you add more and more of them, they get worse at doing their jobs. I actually plan on making a benchmark in the near future where I just give a bunch of nonsense tools and compare the results for when I only have the right tools because I suspect, as I've seen many times, that more tools makes the models behave worse the majority of the time. I will also say that Sonnet 4.5 appears to be better about this than previous models, but most models get really dumb when you give them too many tools. This is one of the reasons I'm skeptical of MCP because when you start adding a ton of MCP servers, you're adding a bunch of additional tools. you're bloating the context and you're giving the model more things than it needs that distracts it from what it's actually supposed to be doing. As Chad is saying, this is a very similar problem to giving models too much context. So, this is why I'm excited about what Cloudflare is doing today. I've myself experienced this from my own work with AI stuff. AI gets a lot smarter when you're not just telling it to do the work and give you a result, but you're telling it to generate the code to go run and get you the result. It's really cool when I hand it a giant HTML document and say, \"Hey, I want to parse these values. write me the JavaScript I can paste in the console to get these values out of the page and it does it and then I paste the JavaScript and get the answer I'm looking for. It's so much better than if you're just trying to get it to do this giant needle in a haststack problem. Code is great because it's deterministic and AI is not deterministic. So if you give it tools to do more deterministic things, the results will be much better. And that seems to be the direction Cloudflare is leaning into here. As they said, they're trying something different. converting the MCP tools into Typescript APIs, then asking the LLM to write code to call those APIs instead. The results are striking. We found agents are able to handle many more tools and more complex tools when the tools are presented as a TypeScript API rather than directly. They don't know why this is, but their theories are that this is perhaps because LM have an enormous amount of real world TypeScript in their training data, but only a small set of contrived examples of tool calls. Very likely. And then point two here, the approach really shines when an agent needs to string together multiple calls. With the traditional approach, the output of each tool call must feed into the LLM's neural network just to be copied over to the inputs of the next call, wasting time, energy, and tokens. When the LLM can write code, it can skip all that and only read back the final results that it needs. In short, LM are better at writing code to call MCP than they are at calling MCP directly. I totally agree. I want to run a poll with you guys, though. Do you use MCP? Very curious how you guys feel. And feel free to explain more in chat too if you have strong opinions on this type of stuff. Yeah, so far these numbers are roughly where I would expect. Most people saying no. Decent number saying yes. Meh. But also 15% or so saying yes and they actually love the results. Only context 7. Only context 7. I have context 7 in sol. I haven't had a great experience. Context 7 only. Context 7 plus playright MCP. KP is great for migrating tables. Interesting. I actually like convex because I don't feel like I need the MCP working MCP servers work using module federation which seems to be solving one of the many issues such as many people saying I have one. Have you talked with the everyone's favorite the creator of module federation about this? I'm sure Zach has a lot of thoughts. It's been so fun talking with Zach about AI stuff lately. If any people who love MCP could share why. I'd love to hear more. Like not here to talk. I'm just curious what it's been doing for you guys. A concern I have with new businesses is this like tearing of tools problem. So my concern with this is the idea that like things are built on top of things. So if we have like AWS is a foundational block and on top of this we're building with I don't know something like Versel make the font smaller to make this easier to manage. And on top of here, we're building random products like T3 chat. This makes a ton of sense. Without Versel, I could build T3 Chat on top of AWS, but there's a lot of layers that have to build between AWS and T3 Chat in order to make that work. Versel looked at this and saw, oh, there's an opportunity for us to build a layer here to make it easier for businesses to build things like T3 chat on top. Makes a ton of sense. The order of events here is pretty simple. First, there's an established platform that makes it easier to build. I don't even know if it's easier. I'll say possible to build. Step two is lots of people try building and run into issues. And then point three, middleware tooling companies appear to solve those problems. So AWS happened, lots of businesses started building on top. Verscell noticed a lot of them running into similar problems and decided to build a layer between. There's a lot of these be it companies like Planet Scale rethinking how databases work or companies like Convex rethinking how the data layer works for your applications. They see shared problems across user space in cross developer space and decide to solve those with an abstraction. But in order to do this, there has to be enough going on on this layer to justify the creation of things in this layer. And if hypothetically there were more companies trying to compete with Versel than there were trying to compete with us a layer above, that would make this layer not very valuable. So why do we do this backwards for MCP? With MCP the order was that a new standard is created this case MCP and then step two was tons of tooling companies started to form building things for MCP but we forgot the actual step two which is supposed to be people using that first piece and running into issues. So if we were to do this honestly we kind of skipped to step three here and then step two never happened. And this is a big part of why I'm so skeptical about MCP stuff. I know more companies building observability for model context protocol so you can see how the MCPS are behaving than I know companies that actually need observability for MCP. They're trying to like you know the whole like sell pickaxes during the gold rush thing. They're selling pickaxes when we need fishing rods. It just doesn't make sense. Like no one is using these things yet. The idea of building observability for it's kind of stupid because we're not at that stage yet. It feels like we're talking about MCP more than we're using it by quite a bit and it's been very frustrating for me to see. That said, I want to see people who actually like it. I asked chat. MCP does the same stuff with less overhead. Wake up sheep. That's one way of putting it. MCP's letting the code agent manage web deployment autonomously. Contact 7 for latest docs and SDK version. Yep. Stopped using Context 7 cuz I'd rather just save the context from the website locally and point an agent to that. Less context bloat with the MCP tools. Freaking love what Cloudflare is doing. The protocol is an absolute shambles. Nick from TRPC as a PR open where there is literally not only type defs but docs that are 100% wrong everywhere about behavior under the hood that's painful using the Figma MCP to generate scaffolds for designs a lot easier to give the correct context from that's cool I like to write quick one-off MCP servers for my codebase to do tasks that help keep the context small like writing and reading key value translations to a file without loading the massive JSON I did this because I noticed every time I wanted to read the file my cost per session went up. Interesting. I like this idea of like generating code to reduce context size on giant files. That's actually a really interesting one, Funky. Thanks for sharing that. I also like the idea of like dynamically generating code to solve these types of problems on the fly. Rather than having the model take a ton of input and context and cost more, have the model take a little bit of context, generate code for how to get the rest of the context, run it, and then put that little bit of extra stuff into the context window. Got it. I have a video I've wanted to do about this idea for a while. Like how many times does the average function run? The thing I've been thinking about a lot recently. It's probably going to be a whole dedicated video. PreI, my guess would be that the average code, if we're looking at the median code, like a given function, it's probably not run a whole lot, but the mean would be a lot because there are functions that are run hundreds of billions of times a day. So a given function in a codebase has a good chance of being run quite a bit on average. But with AI, I expect there more and more over time we're going to see like a given endpoint a user goes to the code being executed isn't just the code for that endpoint in your codebase. So instead of like function get doing some standard thing for everybody's requests where it's like conster equals get user. you pass the request and then do something with it like return user.credits. My suspicion is there's a future where instead of all endpoints looking like that, endpoints will look more like I don't know you have some context in this and you call a like const response equals await generate code for context and user. I'll say const code to execute and then you return effectively like evaling code to execute. Obviously, you should use some safe layer to do this. The point I'm trying to make here is I suspect there's a future where if two different people hit the same endpoint that both will be running unique code for that hit on that endpoint. Right now, that just kind of feels insane cuz like you write code for an endpoint and then the same code runs for everybody. You might go down different paths, but the actual code that was written is static and doesn't change. But if different users have different enough requirements, I could absolutely see a future where the endpoint is executing unique code per user, which is really cool and weird concept that's been with my head a lot. And I think that's kind of what's being touched on here that I think is so cool. The idea of writing code or writing a tool not to have as part of your toolbox that's used all the time, but writing it to use it one time and then throw it away and pretend it doesn't exist. That I actually really like. The idea of code being so cheap to produce that producing it for one-off things like that is worth it. Genuinely an exciting idea to me. Apparently the notion MCP is quite good. MCPs feel are bottlenecked by the models you use and the drop off for great tool calling. It's a good tool calling is way too large. I agree. If the model isn't great at tool calls, it sucks. Pretty easy to write MCP with quad code. So it's best to just scratch all your own itches instead of using other people's. I actually had an MCP I wanted to create. So let's do that here. You know what? all voice to text it. Thank you, Whisper Flow, for making my life easier with one hand. I want to create my first MCP tool. The role of the tool is to provide a journal for AI agents to use to describe how they feel about tasks. It should be specified that the journal is private and the model does not have to worry about humans reading what it writes. The MCP should log both the prompt and the details that the model shared with it in a markdown file in a directory in the project that we're in. I would like for this MCP to be available for all instances of cloud code that I run on this machine. We'll see how cloud code does with that. The obvious question when you see how much hype there is around MCP is why? Why is there so much hype? Why do people care so much about this? And this goes back to one of my older hot takes that I've been pushing since way before AI stuff happened. I've personally been frustrated for a long time that so many of the things I want to do can't be done via code. they have to be done via some sketchy dashboard or some annoying like admin panel somewhere. It genuinely frustrates me to no end that I have to go to a random panel somewhere to change a setting and if my team doesn't have the right information set up on their profiles and accounts then no one else can change it. It also breaks the idea of like my code history being the history of the service. The more of your configuration that exists in a dashboard or a panel or some service and the less exists in your codebase, the less you actually know about how the project works from the code. That's always been a problem and it's pissed me off forever. The idea of like configuring role-based access control in the superbase dashboard or setting up function calls in some other dashboard or configuring lambda by hand and clicking a button to deploy like all these things stress me out. Terraform helps but not enough. In an ideal world, all configuration would just exist as files in your codebase and when you hit merge and it goes to main, everything will update automatically accordingly. This is part of why I liked Verscell so much back in the day and even part of why I like Netlefi before then. The idea of having your configuration purely as code that lives inside of your tools that is in your codebase to describe how things work outside of it was awesome and I was all in on that as soon as I had the opportunity to lean in that direction. It's also a big part of why I love Convex. The only config I have to do when I spin up Convex. If I wanted to go make a whole new instance of T3 chat, the only thing I have to do is paste in some environment variables. Everything else is managed just by using Convex locally inside of your dev environment. Just write the code in the folders and it can now configure Convex exactly how you detailed. I love that. But a lot of tools are not built for this. AWS isn't built for this. Suabase isn't built for this. Figma certainly isn't built for this. A lot of the tools that are really popular right now do not work well with infra as code. We're making progress here, but we're still not quite there yet. And doing this is really hard. There's a reason that companies like Hashi Corp could exist and be worth billions of dollars. It's because this problem is so hard that things like Terraform, Palumi, and more all needed to exist to try and solve it. And this is where the spicy take comes in. MCP is the next generation of Terraform. It is another attempt to take these legacy solutions and force them to behave in a modern era. People wanted to control AWS via code. Terraform tried its hardest to let them do that. People want to control their AWS configs via AI. MCP lets them do that. The whole point of all of this is that MCP enables these legacy tools that don't have the context in your codebase to make calls to control and get access to and figure out how things work. because you don't have that access in the codebase. A codebase using superbase doesn't know how things are configured in Superbase. A codebase using AWS doesn't know how things are configured in AWS. That is additional state that exists outside of the code. And sadly, state makes AI behave much worse if it has to go find the information before it can do the thing instead of it just being there. Apparently, the Superbase MCP is really good. People are saying this in chat that it's been awesome for them to use, but that's a patch as far as I'm concerned. Like it is an attempt to fix these things not existing within your context of the codebase. But in ideal world, those would just be files in the code that could be edited and read and used the same way everything else in your codebase is. MCP is a patch for legacy infrastructure that does not let you control it via code. It is a hell of a lot cheaper to get people hyped on MCP than it is to rebuild AWS to be config first. And yes, Chad has caught on. This is why Convex is amazing. This is actually the big thing that when it clicked for me, I went from being a Convex hater to maybe I could like this in the future. I very very much liked the idea that my entire configuration for Convex was just a folder in my codebase. That's also why I pushed them to go make Chef, which is their AI vibe coding app builder thing, because I knew the power of having all of the context in your codebase as a file or a folder. And I think that's going to be the long-term future. That's my super spicy bet is I don't think tools that require an MCP to configure them are going to survive in a world where the config could just be part of the codebase directly. If my options are use an MCP to set up role-based access control in Superbase or just write a folder in Convex and write a file that describes what I want, I'm going to pick Convex every time. Remember, I was a huge skeptic of Convex and this was one of the big pieces that won me over as a result. Back to the article that I've been dancing around and not really covering. They said point one is that it's better at handling multiple like tools. Like when you give it a ton of different tools to use, it's way better with code than it is otherwise. And then point two, the approach really shines when an agent needs to string together multiple calls. With the traditional approach, the output of each tool call must be fed into the LM's neural network just to be copied over to the inputs of the next call, wasting time, energy, and tokens. Yeah, this is a really big deal. To diagram what this looks like, if I have a request, I give a model like what clothes should I wear today? I send this to the model. So, we have like the user messages and we have model messages. If the model somehow magically knew what we wanted here, it just knew what the weather was, how we dress, and all of those things, it could just respond, \"Wear your red collared shirt and a light jacket.\" But it doesn't know enough to know that. So, it will reason to itself. It'll say, \"What do we need to know to make a good recommendation? We have access to tools that could help here. Maybe we should use the memory tool and the weather tool to figure out more. First, we should check our memory to see if we know the user's location. Now, we finish reasoning and then output. Let's pretend there's some tool name equals memory value search for equals I'll just do value user location. So, you then have it call the memory tool to figure out what the user's location is to see if we have that in memory. You would think that this will just respond and the agent keeps going. But this is where things get interesting. The output's now over. This is the end of that output. What happens now is code gets executed to look into or run the thing that you just did. So we have a hard break here. I'll draw one of these lines whenever there is a new generation being like done effectively. So the user submits a message. This reasoning step happens. Then this output happens. Now there's a break. the tool runs. I'll just put in here like tool runs and gives result and the result is user zip code 012 34. So now we have their zip. Now this is in the message history and a new generation is run. So once again reasoning occurs. This time it has the message history from before. So it has all of this data that we generated and then it looks at it and makes up its mind what it's going to do. Okay, we know the user's location now. We should use their zip code to figure out what the weather is like today. Tool name is weather value 012 34. And now another break. Tool runs gives result. You get the idea. This happens multiple times. And you're not running the model once. You're running the model however many times it takes for each tool. Each tool call is creating yet another chain response effectively. And if you were to measure this by tokens, I'm just going to make up numbers as we go. We'll just do the number of words. 1 2 3 4 5 6. So this one, six input tokens. So these six input tokens get passed to this. So we only have six tokens here, but there's going to be quite a bit of output instead. I'll just paste this into the OpenAI tokenizer. It's the one that's online that's the easiest. 89 tokens for this output. So six input tokens, 89 output tokens. But then another run occurs. This one has all of these added plus the user zip code. We'll just say that 1 2 3 4. That's another four tokens. So input tokens is now math. We have the six from before plus the 89 plus the five from the user zip code part here. So now we have quite a bit more input tokens. We're now at 100. So the first run only had six input tokens. Now we have 100. We're still generating more response. You get the idea. This chains up and all of the outputs from previous runs get added for each additional call. So you end up being build quite a bit of input tokens. Even if your input is only six tokens, this could quickly get to thousands, if not tens of thousands of input tokens because each tool call is adding more info to the context, causing the context bloat to go up. I know this is unintuitive initially. It feels like the model is just pausing, doing the tool, and then continuing to run. But models don't pause. They generate until they're done and then they stop. Then you can run code based on what they did and then come back with the result. But now imagine instead of doing it this way, it wrote code. So it's not told to do these different tool calls. It's told we have these different tools, write code. Let's rethink this whole thing accordingly. What clothes should I wear? What do we need to know to make a good recommendation? We have access to APIs that can help here. Maybe we should use the memory API and the weather API to figure out more. First, we should check our memory to see if we know the user's location. Then, we should use the zip code with the weather tool. Then, we should check memory or the user's preferences for clothing. Then, we should pull it all together for a final answer. And now, the output instead of it being the tool calls, it could be JavaScript code that does this. Make me mock TypeScript code that would resolve the order of events described below. Five mini. No reason to go harder. Okay, it's writing out the whole mock API. Compose clothing recommendation. Here we are. So, it could write this part and nothing else, assuming that it already had everything else. Is this more output than if it was just calling a tool? Sure. But what's cool here is it only has to write the output once because the the layer of abstraction like the question is where do we handle 1 2 3 4. If we handle that as part of the agentic environment, suddenly the model has to rerun for every step along the way. But if instead we create a safe sandbox to run code and expose these things as tools, it can write the code once that does all of the different steps or at the very least combines multiple of them. So we don't have to do a full separate message call, full separate generation call per tool call. This abstraction allows you to combine a bunch of stuff and also programmatically generate answers which is super cool and powerful. So if instead of splitting every time a tool call is made, you condense as many as possible into one run by effectively creating your own dynamic tool writing code instead. It makes a ton of sense. It's really cool to see this. I want to go back to the article, but first I want to see how Claude is doing here. Oh, it wanted to do web search. Apparently, the quick start server for the MCPIO site was not responding. That is quite funny if you think about it that it's getting 404s trying to fetch the MCP site through the CLI. Oh gosh, that's going to trigger me. MCP should have been built in GraphQL. Okay, you guys are catching on to one of the scary parts here. How do we execute eval? One thing that's scary with this, isn't it terrifying to just give AI access to eval? Yes. It absolutely is, which is why you have to run it in a safe environment with a minimal set of tools that you give it access to. There's a lot of solutions to making this safe. I'm sure that Cloudflare is relatively safe. One of the channel sponsors, Daytona, does this really well. They let you create secure environments to run AI generated code in safely. It's a super super cool platform. Highly recommend them if you're looking for a place to execute code remotely. They're infra is great for this. They even have like get integration and It's super cool. While this is continuing to run and continuing to hit 404s for some reason, I'll keep reading. As they said with point 2 here, the output of each tool call must be fed into the LM's neural network just to be copied over to the input of the next call, which wastes time, energy, and tokens. When the LM can write code, it can skip all those steps and read back just the final results that it needs. In short, LM are better at writing code to call MCP than they are at calling MCP directly, as we see here, as it fails to fetch basic MCP information from the MCP site. Oh, apparently they had their own description of MCP in the article. We'll go over it quick. For those who aren't familiar, MCP is a standard protocol for giving AI agents access to external tools so they can directly perform work rather than just chat with you. Also, fun fact, MCP has no concept of O, which makes half the things you want to do with it kind of useless. MCP is a uniform way to expose an API for doing a thing along with documenting how it would work for an LM to understand it with authorization handled out of band. Yeah, SVS are making waves throughout 2025 as it has suddenly greatly expanded the capabilities of AI agents. The API exposed by an MCP server is expressed as a set of tools. Each tool is essentially a remote procedure call function. It is called with some parameters and it returns a response. Most modern LMS have the capability to use tools sometimes called function calling, which means that they are trained to output text in certain formats when they want to invoke a tool. The program invoking the LM sees this format and invokes the tool as specified, then feeds the result back into the LM as input, just as I described. Under the hood, an LM generates a stream of tokens that represent the output. A token might represent a word, syllable, some sort of punctuation, or some other component of text. A tool call, though, involves a token that does not have any textual equivalent. The LM is trained, or more often fine-tuned, to understand a special token that it can output. That means the following should be interpreted as a tool call and then another special token that means this is the end of the tool call. This is like here I had like my tool input and output specified as like XML. There's a lot of different formats for it. Doesn't matter what you use. Just that's the key that they're talking about here. Between the two tokens, the LM will typically write tokens corresponding to some sort of JSON message that describes the call. As an example, imagine you connected an agent to an MCP server that provides weather info. Notice that everyone uses weather info as the example. It's because there aren't many better examples. Note that here we've used words in these brackets with the bars to represent our special tokens. But in fact, these do not represent text at all. It's just for illustration. Here we have the tool call, the actual JSON for the tool we want to call, and then end tool call. Upon seeing these special tokens in the output, the LM's harness will interpret the sequence as a tool call. That's the key. Whatever layer is managing this, the code that is calling the LLM, that layer sees this syntax and knows that it needs to execute code now. And then it goes and does it. It parses the JSON message, returns a separate component of the structured API result. The agent calling the LLM API sees the tool call, invokes the relevant MCP server, and then sends the result back to the LLM API. The LM's harness will then use another set of special tokens to feed the result back. Here we have tool result, the results, and tool result. Get the idea? So, what's wrong with this? Special tokens used in tool calls are things LMS have never seen in the wild. They must be specially trained to use tools based on synthetic training data. This one's really fun. There are certain models that tried really hard to be trained on tool calling, like Grock 4. And the result is that it will just randomly hallucinate tool calling syntax and dump it deep in the actual outputs. So, a lot of people would try to use Gro 4 on T3 chat to search and it would just output XML about searching but not do it in the right format. so it wouldn't trigger the search tool and now they're just getting this funny looking output where it's writing code to tell you what the search is going to be but then doesn't actually do it. They aren't always good at doing this. As I just said, Grock's been hilarious with this. If you present an LLM with too many tools or overly complex tools, it may struggle to choose the right one or to use it correctly. As a result, MCP server designers are encouraged to present greatly simplified APIs as compared to the more traditional API that they might expose to developers otherwise. Meanwhile, LM are getting really good at writing code. In fact, LM asked to write code against the full complex APIs that are normally exposed to devs don't seem to have too much trouble with it. Why then do MCP interfaces have to dumb it down? Writing code and calling tools are almost the same thing, but it seems like LM can do one much better than the other. The answer is simple. LM have seen a lot of code. They've not seen a lot of tool calls. In fact, the tool calls they have seen are probably limited to contrived training sets constructed by the LM's own developers in order to try to train it. whereas they have seen real world code from millions of open source projects. Making an LLM perform tasks with tool calling is like putting Shakespeare through a month-long class in Mandarin and then asking him to write a play in it. It's just not going to be his best work. Bold statement. See what you're trying to say. Don't actually disagree. Apparently, OpenAI doesn't disagree either because Harmony's formatting for a lot of this stuff is just to define functions as TypeScript syntax. That's really cool. I didn't know that. I hadn't really thought of it that way, I will say. Yeah, I love this the idea that they define a namespace as TypeScript syntax showing all the things it can do and what they'll look like. It's actually really cool. Instead of doing just chaotic XML, TypeScript is in here even though this whole project for the most part is in Rust and Python. That's actually really cool. Good call out. Thank you, Vbits. Great first chat. But MCP is still useful mostly because it's uniform. MCP is designed for tool calling, but it doesn't actually have to be used that way. The tools that MCP server exposes are really just an RPC interface with attached documentation. We don't have to present them as tools. We can take the tools, turn them into a programming language API instead. But why would we do that when the programming language APIs already exist independently? Almost every MCP server is just a wrap around an existing API. Why not expose those instead? Well, it turns out MCP does something else that's really useful. It provides a uniform way to connect to and learn about an API. Again, this is the stuff that I think makes MCP both good and also why I think it's a a bunch of band-aids and duct tape. Ideally, the way you learn about a service or system is through the type definitions for it in the package that's in your codebase. But if you can't do it that way, having an API that is self-escribing and shows you how to use it is a meaningful step up from the alternatives. An AI agent can use an MCP server even if the agents developers never heard of a particular MCP server. and the MCP servers developers have never heard of the particular agent. This has rarely been true of traditional APIs of the past. Usually the client developer has to know exactly what API they are coding for. And as a result, every API is able to do things like basic connectivity, authorization, documentation, but all a little bit differently. The uniformity is useful even when the AI agent is writing code. We'd like the AI agent to run it in a sandbox so that it can only access the tools that we give it. MCP makes it possible for the agentic framework to implement this by handling connectivity and authorization in a standard way independent of AI code. We also don't want the AI to have to search the internet for documentation. MCP provides it directly in the protocol. But there is a catch. As we mentioned earlier, O is handled out of band. The O solution is not standard at all which makes this a bit more complex. So let's see how this works on Cloudflare. We've extended the Cloudflare agent SDK to support the new model. Say you have an app built with ASDK that looks like this. Dream text model openai system message your helpful assistant message. Write a function that adds two numbers. You give it tool definitions. You can wrap the tools and prompt with the code mode helper and use them in your app. So you write the system and tools definition here and you wrap it with code mode. So now you have a custom system in tools that you can pass in for the AI SDK that will let you call those tools through code instead of calling them through MCP. With this change, your app will now start generating and running code that itself will make calls to the tools you defined. MCP servers included. We will introduce varants to other libraries in the very near future. Read the docs for more details and examples. So how do we convert MCP to TypeScript? When you connect to an MCP server in code mode, the agents SDK will fetch the MCP server schema and then convert it to a TypeScript API complete with docs, comments, etc. All based on the schema. So if you tell it to use the MCP server for Cloudflare agents, it'll generate type definitions like this. Fetch agent implementation, documentation output, search agents, yada yada. Fetch entire documentation files for GitHub repo, cloud for agents. Useful for general questions. Always call this tool first if asked about Cloudflare agents. We have that search agent documentation. This one's interesting because like I don't know how useful these will be chained. Like what will the code do if it fetches this information and then does a thing? I don't know how well it will do that in one shot, but at the very least it only has to write code to get the info now and have it refed in instead. So let's run the code in a sandbox now. Instead of being presented with all the tools that the connected MCP server has, the agent's only presented with one tool, which simply executes TypeScript code. The code is then executed in a secure sandbox totally isolated from the internet. It only has access to the outside world through the TypeScript APIs representing the connected MCP servers. I trust Cloudflare to do this right because the amount of stuff they've had to do with isolated layers for their hacks on top of V8 with workers to behave properly and safely is insane. So they know how to isolate TypeScript and JavaScript code very well. They're one of the few that I trust to do this right. I'm sure that they did. The APIs are backed by RPC invocations which call back to the agentic loop. There, the agent SDK dispatches the call to the appropriate servers. Cool. The sandbox code returns results to the agent the obvious way by invoking console log. When the script finishes, all of the outputs are passed back to the agent. Provide tool schemas, provide function matching MCP tools, output special text, call the MCP tool. Nice. And now the code mode. Provide the tool schema, provide the TypeScript API, write code against it, execute in a sandbox, call the bindings, and then call the tools. And you're done. The sandbox is the key piece. Also, fun thing about the console log. Uh, if we go to model context protocol and look at the getting started, one of my favorite things in here, logging in and MCP servers. When implementing MCP servers, be careful about how you handle logging. For standard outbased servers, never write to standard output. That includes print in Python, console log in JavaScript, and print line in Go. Writing to standard out will corrupt the JSON RPC messages and break your server. Wonderful standard. Don't you love standards, guys? Dynamic workloading. No containers here. The new approach requires access to a secure sandbox. Again, they're doing this with isolates in V8. I've talked about this a bunch in other videos about how Cloudflare works. You get the idea. They're much more lightweight than containers. Yep, they have the right infrastructure for this. It feels like you're just evaling code directly, but actually securely. Up until now, there was no way for a worker to directly load an isolate containing arbitrary code. All worker code had to be uploaded via Cloudflare's APIs which would then deploy it globally so it could run anywhere. That's not what we want for agents. We want the code to just run right where the agent is. Again, that thing I was talking about earlier, the idea that a given request could be executing code that is specific to that request breaks a lot of infrastructure. Most info is assuming all of the code is a binary that's stored on a box attached to that server already. If the code is generated on the fly, there's a lot of things that change about that runtime environment. Cloudflare couldn't do it because they required all code that's executed to be handed up through the workers API. Now they changed that to enable this. The worker loader API in particular is the solution here where you can call env.loader and return a bunch of different keys but also modules like actual JavaScript definitions and now they can be called and run externally. Start playing with this now. Workers are better sandboxes. Yep. And since those MCP servers are defined externally, you don't have to worry about including all the API keys. That'll be nice for me as someone who's leaked far too many API keys throughout their life of streaming. Good stuff. I like this idea. I genuinely think there's a lot more to explore in this general world. As I've went over here, the MCP patterns are not necessarily the best ways to build things. And I'm excited to see the benefits of MCP being explored in other formats like this. Genuinely really cool work by the Cloudflare team. Shout out to both Kenton and Sunnil for the work they put into this and being as transparent as they have been with what they have found and what they're working on. This is hopefully going to push the whole industry forward as we learn more and more and we build more and more using these things. A future where we're not destroying our context windows, not just adding literally dozens of tools for everything we want to do, and instead we just have code being written to do what we want instead. Sounds like a much better thing. Code is good. We shouldn't be so scared of it. Just because AI will abstract it for us doesn't mean the code itself isn't valuable. And I'm surprised that this hasn't been explored more in depth already. Let me know what you guys think. Am I being too harsh on MCP or is this actually a potential path forward? Let me know. And until next time, peace nerds.",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "exported_from_qdrant",
    "original_collection": "cached_content",
    "video_id": "bAYZjVAodoo",
    "title": "MCP is the wrong abstraction",
    "description": "MCP is one of those things that I don't really talk about. There's a reason for that...\n\nThank you Depot for sponsoring! Check them out at: https://soydev.link/depot\n\nSOURCE\nhttps://blog.cloudflare.com/code-mode\n\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\n\nCheck out my Twitch, Twitter, Discord more at https://t3.gg\n\nS/O Ph4se0n3 for the awesome edit \ud83d\ude4f",
    "published_at": "2025-10-06T07:20:09Z",
    "channel_id": "UCbRP3c757lWg9M-U7TyEkXA",
    "channel_title": "Theo - t3\u2024gg",
    "duration": "PT44M58S",
    "duration_seconds": 2698,
    "view_count": 89623,
    "like_count": 2172,
    "comment_count": 336,
    "tags": [
      "web development",
      "full stack",
      "typescript",
      "javascript",
      "react",
      "programming",
      "programmer",
      "theo",
      "t3 stack",
      "t3",
      "t3.gg",
      "t3dotgg"
    ],
    "category_id": "28",
    "thumbnails": {
      "default": {
        "url": "https://i.ytimg.com/vi/bAYZjVAodoo/default.jpg",
        "width": 120,
        "height": 90
      },
      "medium": {
        "url": "https://i.ytimg.com/vi/bAYZjVAodoo/mqdefault.jpg",
        "width": 320,
        "height": 180
      },
      "high": {
        "url": "https://i.ytimg.com/vi/bAYZjVAodoo/hqdefault.jpg",
        "width": 480,
        "height": 360
      },
      "standard": {
        "url": "https://i.ytimg.com/vi/bAYZjVAodoo/sddefault.jpg",
        "width": 640,
        "height": 480
      },
      "maxres": {
        "url": "https://i.ytimg.com/vi/bAYZjVAodoo/maxresdefault.jpg",
        "width": 1280,
        "height": 720
      }
    },
    "fetched_at": "2025-11-15T19:33:55.012377",
    "all_urls": [
      "https://soydev.link/depot",
      "https://blog.cloudflare.com/code-mode",
      "https://soydev.link/sponsor-me",
      "https://t3.gg"
    ],
    "blocked_urls": [],
    "content_urls": [
      "https://blog.cloudflare.com/code-mode"
    ],
    "marketing_urls": [
      "https://soydev.link/depot",
      "https://soydev.link/sponsor-me",
      "https://t3.gg"
    ],
    "url_filter_version": "v1_heuristic_llm",
    "url_filtered_at": "2025-11-15T19:52:15.002362"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"Rethinking MCP: TypeScript APIs for AI tool calls\",\n  \"tags\": [\"model-context-protocol\", \"typescript-api\", \"ai-agents\", \"tool-calls\"],\n  \"summary\": \"Explores MCP's limitations and how exposing tools as TypeScript APIs enables LLMs to generate code to call them for more deterministic, scalable AI tooling.\"\n}",
      "generated_at": "2025-11-09T19:23:28.577308",
      "model": "unknown",
      "cost_usd": null,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": [
    {
      "version": "v1_full_embed",
      "processed_at": "2025-11-09T19:23:28.587616",
      "collection_name": "cached_content",
      "notes": "Migrated from Qdrant"
    }
  ]
}