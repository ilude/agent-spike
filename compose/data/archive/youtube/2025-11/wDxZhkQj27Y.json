{
  "video_id": "wDxZhkQj27Y",
  "url": "https://www.youtube.com/watch?v=wDxZhkQj27Y",
  "fetched_at": "2025-11-17T22:36:39.039100",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-17T22:36:39.039070",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "there's a lot of overhype and grifting in the AI YouTube space right now people are hyping up the latest and greatest releases of random GitHub projects like chat Dev dbgbt and meta GPT and tons more you've seen these this is cool some are useful but it's pretty click-baity and it's not Ultra useful and let's face it in five probably three or two years nearly all of these projects will be dead in this video I want to filter through some of that noise and focus on three concrete instant value practical prompt engineering techniques as soon as you finish this video you're gonna have three new techniques available to you instantly let's focus on the underlying technology that's giving us all this value in the first place llms so let's get into it the three techniques are Capper apps one word prompts and context chaining you've likely already used one or two of these techniques without even knowing it cap wraps when you're building out your reusable prompts that require several input variables it can be hard to organize these to get around this I use technique I like to call capitalized referencing or cap refs so here's a chat log I've had with chat gbt and let's just walk through this let's Workshop some titles for a new YouTube video we want high ranking SEO click-worthy titles create 10 highly engaging click worthy titles based on existing titles notice how this is uppercase that combine one or more video elements below again notice how this is capitalized when you generate the title Place elements involved next to the text like this you can see here I have a templated response format that's great and then here's the magic I place the capitalize references in separate sections and then I specify that variable with a list or single item below then I specify the next capitalize reference afterward and specify all of its details so as you can see here I'm building on a prompt that allows me to create high ranking SEO click-worthy YouTube titles if you run a blog or you have a YouTube channel yourself you know that this is a repetitious process and you want to get the ideas going as soon as possible this is how you can use capitalized referencing to reference specific information in your prompts okay so let's move on to another example let's talk about one word prompts there are many scenarios especially when using gpt4 where you just shouldn't overthink it if you have a problem you want fixed that's obviously an issue just toss one word at GPT in the single word can save you a lot of time a lot of typing let's walk through an example so let's type Pi test and I'm going to paste in a python function right I'm not going to format I'm not going to do anything I'm just going to type Pi test shift enter enter and then I'm dumping in Python content so all I'm saying is pi test here and as you can see gpg is just walking through this explaining the code we didn't give it a lot of details but at the beginning I did say Pi test and you can see here on bullet point number four you picked that up since you mentioned Pi test let's write a simple test for this function So eventually it got the whole idea that all I want to do is create a test and as you can see here it is building that test out just drop a single word in paste in your content typically if you're pacing Rich content like code or uh you know content from a blog or something from you know something that has detail Rich information you can just say one word if it's a verb it'll have a higher hit rate but you can basically just paste in a single word and gbt specifically gpt4 will know exactly what you want to do and it'll break it down for you so then we can do other simple things right we can sort of translate at GPT this is a you know simpler example you just say Translate and then you paste in some text and without specifying the languages you can see here it's translating this French and uh you know it translates to I like practical engineering tips from The Prompt which is close this is actually uh I like practical engineering tips for prompt engineering so this is great right and then let's go ahead and look at one more example we can do something like reword and then we can paste in you know purchase this YouTube title then we can paste in for instance like a title or a phrase and just throw that in there and you can see with just a single word gbt gives us back a solid response the great part about one word prompt is that typically gbt responds in a similar manner you can see for both the reword and the translate it just responded in a really like blunt forward way without producing a lot of text right we had a lot more going on here when we dumped in some code and I think that's pretty typical gpt4 especially it tries to walk you through everything and that's really great usually it's nice when you get some of these shorter prompts using the one word technique and it just kind of responds with the information that you're looking for if you guys are enjoying these concise practical tips that you can get value out of immediately like in sub for more on this channel we focus on real tangible in the field AI tools Tech and techniques speaking of techniques let's finish with the final technique I think this one's really cool start with a fresh chat window year so you can know that there's no tricks there's no memory happening here this last technique is called context chaining you know that moving fast is the key to engineering success and just success in general we all have 24 hours in a day but some of us get more out of those 24 hours than others how because they're moving faster they take one swing and they cut two trees context chaining saves you time and lets you communicate the contexts that are relevant to your prompts insanely quickly so for example let's type out a prompt in like a normal fashion right so I'm building a python CLI tool want to publish it to test pipod I'm using poetry management tool how can I publish package so let's see how many characters I have here right just gonna open up the console I'm gonna dump this into string type length on it this is 148 characters okay so we'll remember that 100 create characters for this prompt right here right I'm not going to run this I'm not going to waste your time this runs it works fine it tells you exactly how you can publish it using poetry with context shading you can pack all this information into a much smaller prompt check this out code python poetry publish to test iPod so I'm actually going to go ahead and run this one I'm going to copy it run it and we're gonna look at the length here this is 41 characters I just finished publishing a python package called diff bro check out the previous videos if you're interested in seeing the full process of that but you know looking through this I can confirm that this is exactly how to do this the magic here is that we did this in 41 characters versus 148. how are we able to do this with context changing context chaining allows us to concisely pack in our prompt in a simple compressed way we're in the realm of coding this is python we're operating in poetry and then at the very end we say in a really kind of Brute Force way exactly what we're trying to do publish to test Pi Pi after you give GPT all this context publish to test Pi Pi is not a stretch at all and so I just want to point out here that we cut our prompt size from about 150 to below 50. that is a three times improvement in our prompt link we dropped it down to 41. dropped it by a third actually you're writing 10 prompts a day right 100 prompts a day whatever it might be if you can cut your prompt time down by a third every prompt you're going to get more out of your 24 hours than the engineer sitting next to you so just to refresh we have capitalized references also known as cap refs we have one word prompts and we have context chaining one word prompts essentially will always operate on its own right there's not much more you can do with the one word prompt it either works or it doesn't work if you think a one word prompt will get the job done use a one word prompt just as a final Easter egg let's walk through one more trick you can combine context chaining with cap references to create even more compressed concise prompts so let's walk through one more example let's look at another python example right let's say something like this right we could do uh we could write something like code python combine Funk one and Funk 2 together and then we specify Funk one this function here we're just going to connect to an sqlite database and then the second thing is going to be funk 2. and we're going to give it another python function which is going to insert to some table and I'm just going to hit enter here so what we have done is combined context chaining with capitalized references and then we specify our references here with their respective content gbg just jumped right off and started combining these two functions so it is both doing the connection function and the insertion function and we have just combined two of these prompting techniques to speed up the efficiency of our prompting so just to recap one more time we have capitalize references also known as cap refs this lets you reference input variables in a seamless manner we have one word prompts if you don't need to overthink it don't drop in a single word and dump your text and just let it rip and then lastly we have context chaining you don't need to type as much as you think you need to on a similar mode of one word prompts context chaining is essential actually putting together a bunch of one word prompts to tell the llm the exact context in which you're working in use these three prompting techniques to save a crap ton of time over the long run whether it's gbg3 gbt4 whether using Falcon whether using any of these custom models or a different chat interface it doesn't matter at the end of the day we're all communicating with llms so these techniques are interchangeable and will be relevant now and in the future I think the direction things are going with llms is the better they become the less information we'll need to feed them and it's all going to be about specifying the right context so that they can load the right information they need to predict the very next token utilize capitalize references one word prompts and context chaining to excel your prompt engineering prompt examples in the description thanks for tuning in sub like and I'll see you in the next video",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "prompt-engineering, llms, cap-refs, one-word-prompts, context-chaining",
      "generated_at": "2025-11-17T22:36:49.404598",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}