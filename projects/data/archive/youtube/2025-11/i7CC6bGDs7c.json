{
  "video_id": "i7CC6bGDs7c",
  "url": "https://www.youtube.com/watch?v=i7CC6bGDs7c",
  "fetched_at": "2025-11-09T23:14:12.145719",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-09T23:14:12.145719",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "were not doomed. I want to focus in this video on some of the critiques I see of LLMs around the idea of P doom or probability that things will just go very terribly for humanity and we will all be overwhelmed. And I want to suggest some reasonable responses and things that help me sleep at night for those of you who are worried. I call it a letter to my friends who are worried about the end of the world. And there are many of my friends who are I get asked as someone who works in AI really frequently, Nate, what's the odds that the world are going to end? What are the odds that my kids won't grow up? That's a really dark question. That is not something that I expected to have to ask when chat GPT launched. So, let's talk about it. One of the top ones I see, maybe the biggest one, is what are the odds of humanity's full extinction? That's kind of implied, right? Like humanity will be done. I've seen lots of numbers. There are some people who claim that it's almost a certainty. There are people who claim it's like a double-digit odd, 30% odds. I've seen there are people who claim it's 1% odds, but the risk is unacceptable. And regardless, I think what the cha conversation is missing is an honest conversation about how we get from here to there. I have read the famous essay that really socialized this. It's the 2027 AI essay and it talks about this fast takeoff scenario where AI gets more and more intelligent and a global AI starts to plan and we end up in a world where the AI decides to make us extinct because it's just efficient to do that etc. that inspired a lot of fear. There was fear before, there was more fear after that essay. I appreciate the intent the authors had. This is not intended to discredit or critique the essay. Instead, I want to suggest that a more useful way to think about AI and risk is to think about the reasonable extrapolation of the present risks that we see materializing with AI now that we're two years into this Chad GPT moment. We are far enough along that we can see the trend lines of risk, not just the trend lines of technological progress. And critically, I don't think the trend lines of risk are materializing the way PDoom proponents are suggesting. One example, none of the AI behavioral experiments that I have seen suggests to me that LLMs are getting better at the kind of proactivity and long range planning that would be needed for any kind of meaningful action. And I mean meaningful work action, let alone meaningful action against the human species. We're just not seeing a lot of progress in that regard. Agent mode was released just this week from OpenAI. It does tasks for a few minutes. There are some models like clawed code that will go and do it for like an hour, two hour, maybe in a few cases three or four hours. But these are tasks that are tightly defined where it's trying to solve a specific problem. These are tasks initiated by humans where once the task is complete, the LLM wraps up. Open-ended LLMs are something model makers are contemplating, but there are several problems that no one really has a good answer for right now that stand in the way. And it's not clear that LLM architectures give you that answer. If you are using a transformer-based architecture and it just ingests tokens and it predicts the next token and yes, maybe you add inference on the top, maybe you add goaling, you add tooling on the top, it is not clear that that stack by itself is enough to get you long-term intent. It's not clear that bolting on a markdown file for memory is enough to give it meaningful memory and skin in the game that allows it to really go places. I get the idea of emergent intelligence. We have had emergent phenomena every time we've had an order of magnitude increase for LLMs. Translation is a great example. The LLMs are just good at translation now in a way that they just weren't. But in those cases where emergent phenomena have occurred, there have been clear seeds of that phenomena previously. We have been working on and seeing machines work on translation for a long time. It just suddenly was able to finally solve it. What we haven't been seeing for a long time is the seeds of goaling and planning and intent coming spontaneously from LLMs. And so I don't know that it's necessarily reasonable to suppose that they're going to become self-interested skin in the game, long-term goal planning, heavy memory using LLMs right out of the gate and just emergently do that when there's an order of magnitude increase in our intelligent systems. I I just don't see it. But that would be required if we are going to have a full doom scenario. You have to have the LLM act like that. Now there are other arguments I could use. I could argue that we are modeling this on primate behaviors. We are primates. We have dominant seeking behaviors. It's not clear why a machine that is not a primate would have a dominance seeking behavior even if it was smarter than us. I could also argue that any generally intelligent system is going to be able to goal multiply where it can go across multiple goals at once and blend them and that the paperclip scenario that assumes that you just optimize for particular resource mindlessly by definition presumes we don't have general intelligence. I could argue that human and machine intelligence is by definition complimentary. We find machine intelligence complimentary. That's why we're building them. Why would machines not find us complimentary by the same token even if they reach general intelligence? I think those are all valid arguments. I don't necessarily think they're my favorites, but I think they're valid ones. And I think that when I hear arguments for doom, I critically don't hear this level of detail. I tend to hear a statement of existential risk that cannot be challenged. I don't think that's fair arguments. Like I I don't think that's that's valid to do. If you want to have a conversation, you should be willing to get into the details. And if the detail that you are getting into is any percentage of risk is unacceptable because the longtail risk is so high. That is true of a lot of technologies. We have nuclear power is an example. We have nonzero longtail risk because of nuclear power. Uh and we live with it and we find a lot of value. In fact, we're reviving nuclear power. Another example, we have nonzero longtail risk from DNA research, but we see a lot of benefits, so we do it anyway. We have nonzero longtail risk from airplane usage, but we find airplanes worthwhile. And you might say, well, airplane usage is not necessarily something that would uh, you know, create problems for the species. But we see examples in our history where airplanes created a 20-year war and that was just, you know, back in 2001. So, yeah, even a technology as simple as that can have longtail risk for the species. We consistently as a species create technologies that generate risk for ourselves and we figure out how to mitigate the risk and we find the technology is worth it. I do not see why LLMs are different. Now, there's other categories of PDOM that we can talk about. There's energy usage. I've talked about that. I think the incentives there are heavily in favor of energy usage becoming a zeroedout problem because everyone is incentivized to build more energy to meet the needs of the LLM data centers that are growing. And everyone is incentivized to pay as little for that energy as possible. So, they're going to make their chips and their data centers as efficient as they possibly can. That's true for water, too. The incentives argue for continued growth and efficiency. And that's what we're seeing. Uh major cloud makers are on track for uh water positive data centers in the next three or four years. Every chip generation that Nvidia produces is exponentially more efficient. For that reason, uh Google's tranium chips are giving them an advantage because they are extremely efficient at inference. The list goes on. We find ways to make things more efficient and we should not presume that the current cost today is the same as the cost tomorrow because there's so much investment in this area and because investment historically brings down the cost of technology. Another example of doom is economic disruption. I've talked about this a fair bit. There's an assumption that LLMs that are generally intelligent will just suddenly uh emergently drive labor markets off the cliff. Look, I believe that LLMs are generalpurpose technology. Generalpurpose technologies do have a history of disrupting and changing economies. I'm not going to dispute that because I think it's just there. Steam disrupted and changed the economy. LLMs are moving quickly and so we'll see economic disruption or economic change compress. But that doesn't mean the same thing as saying it's all going to be over for all of us as workers. that presumes a degree of ability to deliver economic work that I haven't seen. I'm going to pick on agent mode again. Agent mode is supposed to be able to do economic work around spreadsheets, which is just one tiny piece of a bundle of skills that is just one tiny piece of many people's jobs. It can't. It can't reliably do it. I tested it over and over and over again. It's not reliably delivering insights that even an intern would be expected to deliver. It is really hard to do good economic work. And the fact that LLMs are even at 1 or 2% of good economic work right now is incredible. It's incredible. It's changing and disrupting industries rightly. LLMs as assistants are an amazing piece of technology. But I see much less evidence for that power reversal where LLMs will be managers. famously when Anthropic argued that LLMs are going to be managers in their uh writeup on Claudius managing the vending machine. I chuckled. I laughed because Claudius did such a bad job as a vending machine manager. To conclude from that that LLMs are going to soon be managers seems like magical thinking on the part of model makers. I get that they're close to the technology. Maybe they're right. But everything I see suggests that jobs are bundles of skills plus. They are not irreducibly just bundles of skills. They are more than that. There's glue work. There's human context that is difficult to tokenize. It's notable to me that X-ray technicians are increasing as a job family despite LLMs being able to do each part of their job. We will still see disruption. There will be customer service reps that are fired because of AI. There will be sales guys that build decks that are fired because of AI. I'm not saying that we won't see those moments. We will. We are. We we it has happened. But from an economic disruption perspective, what we are seeing so far does not line up with the thesis that AI is disrupting the job market yet as a whole. These are isolated instances that are typical of a technology adoption cycle. They are not at all supportive of the idea that AI is fundamentally disrupting the job market. And I think that's really important to call out honestly because I think that people who presume that it will are depending on future inference that frankly the pace of change, the pace of development even in agents isn't necessarily supportive of right now. So I've summarized a few of the things that most concern the people who believe in doom in my life and how I tend to respond to them. I'm not saying I have the perfect answer for everything. Nor am I saying that we won't face new challenges in the future. Nor am I saying that AI is not disruptive. I think it is. But I think it's more productive to have an honest conversation about the real risks involved than to have theoretical conversations about future risks that we are not on track to hit at this moment. For example, I don't think we talk enough about the idea that our learning methods need to change because of AI. Education needs to change because of AI. We face real risk for our young people if we don't figure out how we need to learn differently. But the PDOM advocates don't seem to be too interested in talking about that because I think that would be a great conversation to have. I think we should talk about how we can productively engage with learning risk. I think we can talk about how we can productively engage with helping people who are senior citizens not get fooled by AI fakes of their families. And we talk about that as one of a list of many risks. But we don't spend a lot of time talking about how we can productively derisk that. How we can give families the tools they need to manage safe words to make sure that they can verify that their loved ones are the ones that they're talking to. to make sure that, you know, their senior citizen grandpa isn't getting fooled by an AI deep fake into wiring money to the Cayman Islands. These are risks that have been real for a while in the age of telephone fraud and are becoming more real and I would like to see more work done to diffuse real risks like that because I think we're underinvested in the risks we're actually facing. And so when I talk with Pum folks, sometimes I want to say, well, talk about the risks we have today. Let's work on fixing those because I think that's a more productive use of our time.",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api",
    "video_id": "i7CC6bGDs7c",
    "title": "Don't Panic: AI Won't End Humanity",
    "description": "My site: https://natebjones.com\nMy substack: https://natesnewsletter.substack.com/\nThe story: https://natesnewsletter.substack.com/p/will-ai-really-doom-us-3-hard-facts?r=1z4sm5\n\nTakeaways:\n 1. Doom Narratives Lack a Mechanism: We have zero evidence that LLMs can form self-generated, long-horizon goals\u2014making human-extinction scenarios speculative at best.\n 2. Current Agents Are Clumsy: OpenAI\u2019s Agent Mode needed 58 minutes and multiple hand-offs to order cupcakes, while Claude bungled a vending-machine side hustle\u2014hardly signs of unstoppable super-intelligence.\n 3. Efficiency Outpaces Resource Fears: Cloud providers aim for water-positive data centers and each new GPU generation squeezes markedly more compute from the same watts, undercutting energy-doom arguments.\n 4. Jobs Are More Than Tasks: Roles like radiologic technologists are still projected to grow because real-world work bundles skills, context, and \u201cglue tasks\u201d that AI can\u2019t yet replicate.\n 5. Non-Zero Risk Is Normal: Society lives with nuclear power and aviation despite tail risks; AI should be treated the same\u2014mitigate and regulate, but keep moving.\n 6. Emergence Needs Seeds: Translation flourished after years of incremental progress; there\u2019s no comparable \u201cseed\u201d for dominance-seeking behavior in today\u2019s LLMs.\n 7. Prioritize Present, Solvable Risks: Deepfake scams, AI-driven fraud, and outdated learning models hurt people now\u2014addressing these is more productive than debating hypothetical extinction.\n\nQuotes:\n\u201cI would not hire this intern\u2014it takes 58 minutes to get cupcakes.\u201d\n\u201cWe haven\u2019t seen LLMs develop the self-directed planning a real doom scenario demands.\u201d\n\u201cIt\u2019s more useful to fix the risks we face today than to argue over hypothetical extinction odds.\u201d\n\nSummary:\nI push back on \u201cP-Doom\u201d fears by grounding the conversation in observable evidence. Today\u2019s LLMs struggle with sustained planning\u2014Agent Mode can\u2019t even order cupcakes efficiently\u2014so extinction scenarios lack a plausible mechanism. Resource and job-market doom myths also break down: data-center efficiency is accelerating, and roles like x-ray techs are growing. History shows we accept non-zero tail risks from technologies like nuclear power; AI should be no different. Instead of obsessing over speculative end-times, we should focus on immediate risks\u2014deepfake scams, education reform, and practical safeguards that help people right now.\n\nKeywords:\nAI doom, P-Doom, extinction risk, large language models, Agent Mode, Claude vending machine, long-term planning, energy efficiency, water-positive data centers, job disruption, emergent behavior, deepfake scams, education reform, non-zero risk, transformer architecture",
    "published_at": "2025-07-19T13:01:26Z",
    "channel_id": "UC0C-17n9iuUQPylguM1d-lQ",
    "channel_title": "AI News & Strategy Daily | Nate B Jones",
    "duration": "PT14M30S",
    "duration_seconds": 870,
    "view_count": 8692,
    "like_count": 444,
    "comment_count": 157,
    "tags": [],
    "category_id": "22",
    "thumbnails": {
      "default": {
        "url": "https://i.ytimg.com/vi/i7CC6bGDs7c/default.jpg",
        "width": 120,
        "height": 90
      },
      "medium": {
        "url": "https://i.ytimg.com/vi/i7CC6bGDs7c/mqdefault.jpg",
        "width": 320,
        "height": 180
      },
      "high": {
        "url": "https://i.ytimg.com/vi/i7CC6bGDs7c/hqdefault.jpg",
        "width": 480,
        "height": 360
      },
      "standard": {
        "url": "https://i.ytimg.com/vi/i7CC6bGDs7c/sddefault.jpg",
        "width": 640,
        "height": 480
      },
      "maxres": {
        "url": "https://i.ytimg.com/vi/i7CC6bGDs7c/maxresdefault.jpg",
        "width": 1280,
        "height": 720
      }
    },
    "fetched_at": "2025-11-15T19:25:43.805037",
    "all_urls": [
      "https://natebjones.com",
      "https://natesnewsletter.substack.com/",
      "https://natesnewsletter.substack.com/p/will-ai-really-doom-us-3-hard-facts?r=1z4sm5"
    ],
    "blocked_urls": [],
    "content_urls": [
      "https://natesnewsletter.substack.com/",
      "https://natesnewsletter.substack.com/p/will-ai-really-doom-us-3-hard-facts?r=1z4sm5"
    ],
    "marketing_urls": [
      "https://natebjones.com"
    ],
    "url_filter_version": "v1_heuristic_llm",
    "url_filtered_at": "2025-11-15T19:52:18.806804"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"Letter to my friends who are worried about the end of the world (ai-doom risk)\",\n  \"tags\": [\"ai-safety\", \"large-language-models\", \"existential-risk\", \"economic-disruption-ai\", \"education-and-ai\"],\n  \"summary\": \"An evidence-based exploration of AI doom scenarios, analyzing current LLM capabilities, plausible risk factors, and practical strategies to derisk AI's impact on society, education, and the economy.\"\n}",
      "generated_at": "2025-11-09T23:14:24.507322",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}