{
  "video_id": "k-eZabjdOD4",
  "url": "https://www.youtube.com/watch?v=k-eZabjdOD4",
  "fetched_at": "2025-11-09T22:27:10.298331",
  "source": "youtube-transcript-api",
  "raw_transcript": "[Music] Heat. Heat. [Music] Heat. Heat. [Music] Are you [Music] feel. [Music] [Music] [Music] Hey everybody, welcome to Tech Talk. AI with Nate and Mike. I'm Michael Cricggsman and my friend Nate. >> Yep. I'm Nate Jones. Uh this is the second one of these that we have done. You guys had so many questions that Mike and I decided to do a whole extra 30 minutes on this live stream. And you know, this is an experiment for us in public. Uh both of us have done our own shows for a bit and we have so much fun talking together that we thought why not do something together semi-regularly and see how it goes. So our first one we both had a ton of fun. We got too many questions for the hour. So we're going 90 minutes this time. What are we starting with Mike? >> Well, we need to talk about Nano Banana. >> Yeah. >> Which is such a I mean who who comes up with that? >> I don't know. I think honestly it was the throwaway uh the throwaway code name that they used in the arena and it was so much better than what they actually had planned on using they stuck with it and now there's banana emojis in Google Gemini. >> You know, I have to say I've been working with software companies for too long >> and they come up with interesting code names but then it becomes all corporate. You know, inside they can have fun but then we release it in public >> and That interesting, right? >> Yeah. You know, we need to have our suit and tie on. So, I like I like Nano Banana. >> Yeah. Yeah. No, it's a fun name. And it clearly has struck a chord. >> It's It's been wild how many people have figured out what image models can do simply because, and what's interesting is it's not because the it's a particularly good model at creating images. It's just a phenomenally good editor. And that seems to have been a real click point. It's it's a it's you know, but it's also really good at I was going to say creating images, but there are so many models that do that pretty pretty darn well >> right now. So, I think you're right. It's the taking of an image and changing the context. >> That's right. That's right. In fact, I think you have an example that we were playing with before. Do you want to show like some of the progression with nano banana? I think sometimes like especially with image models a picture is much more useful than just talking about it. >> Okay. I I'm going to show some some pictures. Uh but before I do that, I just want to tell people >> ask your questions because I'm I'm on LinkedIn and I'm seeing the questions starting to come in the comments. >> Amazing. And last time we didn't have enough time to get to everybody's questions and we're going to really focus on the questions. So ask your questions and but but let's take a look at some of these nano bananoids we can call. >> Okay. So here's a picture of me. Oops. Hold on. This is this is hardly scripted. You can see it's advancing. Okay. So there's me just you know very corporate. Sometimes you need a uh you need a beard, right, Nate? And you need >> you need a beard >> and you need long hair >> and and you know, but we did it. Okay. But sometimes also you need to be in a European pub in a green jacket with red glasses smoking a cigar and a hat. And I think this is pretty good. And by the way, you probably can't see this easily, but if you look around the edge of the top of the hat, I told it to backlight the hat. So, it did it. And of course, the obligatory picture of the guy in the green jacket smoking a cigar in a hat, uh, plowing the fields, getting back to his roots. And so, we have that. >> But sometimes you need a red suit. And >> true, >> plowing the fields in a red suit. But then again, what about sustainability? And sometimes you need to take your horses and your plow and your red suit and do it under the sea. So, we've got that. And what else do we have? Oh, of course. Plow the fields on the moon. Looks pretty good. And but in a space suit, no less. There's only two more here in the red suit. And finally, this is really the most interesting one. I've got three arms, but I still have my beard and my hat and my long flowing now red hair. So, it's got three arms. So, Nate, yeah, context. But what about the three arms thing? You know, one of the things that I have observed, if you're not careful with prompting and you're just sort of prompting successively, I I'm going to guess that you prompted successively. And these are image sort of modifications. >> Yes. >> It starts to lose the original context of the image by the time you get to a certain point in the modification loop. And you're not necessarily telling it, I need you to keep all of these things the same and only change this. I'm gonna guess it was a more casual prompt and you were just like, I don't know, put me on the moon, right? Um, and that starts to break down over too many iterations, even in a image editor as good as Nano Banana. And part of why is because image generation models, just like language models, are trained on single turn conversations. So all the reinforcement learning they get is ask for a thing, get a thing back, done, end of conversation. And so this whole idea of an ongoing conversation is not really something any of the models are trained for. >> Yeah. Well, this is what it's this ongoing conversation with these images that is what really makes this valuable because if we think about text, our text interactions, it's the iteration, right? You ask it a question and then you iterate. You're not just generating a static report. You can do that with using deep research. But it's the it's the iteration and it's the iteration with the images. Oh, change the lighting here that will make it so useful for say marketers. >> Right. Right. Exactly. Um on LinkedIn, uh Gustavo Salami says that his theory is that they regarding I think the Nano Banana name is they first released it as Gemini Flash preview image and actually um I remember that as well and then eventually after a few days they changed the name on Gemini and AI Studio to to the banana symbolism. >> Yep. That's true. >> Yeah. I because I remember going there initially and it was uh Flash Studio. I don't know what they were doing >> or flash. >> I think that would be fascinating to hear why they decided to make that change because it was clearly a very impactful choice. Um and I don't think we really have the answer there. Hey, I have a request for the people who are watching. Uh especially if you're if you're asking question on questions on LinkedIn, send a uh LinkedIn connection request to me and to Nate so that we can invite you to the next one because if you're not a direct connection, then when we do this again, we we don't have a way to invite you and we'd love to do that directly. So, Yeah, go ahead. >> No, I was going to say Greg Walters on LinkedIn is asking us to dive into that MIT report versus reality. He says his belief is that AI cannot be implemented in the old ways, centralized top down and software as a service. He he thinks but he's not sure that the best AI LLMs the approach is uh ground up which is problematic. So implementation should we should we talk about what about that report for a moment? >> I think we should probably set up what the report is because I want to make sure we all level set on the context and I think it's yeah it's worth getting into that and then getting into the question after that. Okay, so this report came out and you can easily search search on if you want to find it uh MIT AI failures report and it will come up and it said that 95% of Genai projects are not delivering value and it gave two fundamental reasons for this. Number one, I'm paraphrasing here and this is a long research study and it's I'm giving you a very high level synopsis here. Uh so don't don't don't kill me for simplifying this. uh AI number one is not sufficiently embedded in processes >> which means you haven't figured out how to integrate AI into what people actually do. So it's like a graft on or a toy. Oh, let's do this over here to to the side. Mhm. >> And then an interesting point number two is that the conventional wrong wisdom is that we need to build our you know customuilt custommade tools and customuilt tools failed more than off-the-shelf tools. The other thing we are probably remiss if we don't mention is that the sample size in this uh particular report was not very large. Uh so we're talking about you know a couple of dozen there there there was not like this is I think people sometimes think thousands of companies were surveyed and this is what they said. It was actually different. It was a sit down conversation with a much smaller number of companies to dive into what was going on and why at those organizations. And I think that's very interesting, but I think people sometimes interpret it as if it were a gigantic survey. >> Yeah. I mean, I I think uh Oh, welcome Brian Mosley. I'm just reading through the comments. Uh it's one of it's one of the few popular studies on AI failure. And failure is easier to talk about and kind of more fun in a lot of ways. It's more salacious. So right so we so we grasp onto it but but still the points are are I think very accurate and there's nothing new about this. This is this is human nature. If you don't take AI and embed it in what you're actually doing then how is it going to make you more productive? It's actually going to slow you down because now you're all in learning mode and changing. >> That's right. That's right. Um and I think that like to go back to sort of the original question uh that we got I would agree that one of the nature one of the things we're learning about the nature of this general purpose technology in chat in chat GPT and LLMs in general is that they are bringing with them their own expectations of workflow changes that are so powerful they extend into organizational dynamics. And so one of the things I've written about a little bit on the Substack is this idea that we have expectations of organizational reporting structures, expectations of leadership, expectations of how change works that that are frankly, if you dig into it, hundreds of years old inside the cor sort of history of the corporation. Those are now being somewhat challenged because of the flattening effect of having a machine that speaks language. And we don't really understand all of the implications of that yet. Yeah. Well, we have a a history of how we do things and that history of process which is that's what it is. It's it's a history. >> It's a history of process. That's exactly right, Mike. >> Right. Of processes and data. They that's what has enabled companies to create products on a on a repeatable basis and to scale and but this this is not an AI issue. This is a core issue of innovation and disrupting myself and disrupting oneself or one's business. I know we want to talk about the AI aspects, but you can't divorce it from the rest of daily reality. >> No, you can't. And in many in many ways, when I am talking to a company about AI these days, we have the initial conversation or the presenting issue is always we're not doing enough AI or we want to do AI in this way or that way. But when you start to peel the onion, when you have a couple of conversations, what we find like nine times out of 10 is that it's a culture issue. It's a people issue first and then the technology flows from getting the culture and people issues solved. >> Yeah. Well, that's always the case. Uh, you know, but again, historically, that's that's that's basically the issue here is how do you drive change? How do you disrupt yourself? But let's let's jump to some questions on LinkedIn here. And hey folks who who are listening, if you have better suggestions than LinkedIn for this, please let us just >> we don't want to send everybody to LinkedIn for questions if we can avoid it. But we haven't found a better solution. >> Yeah. So if you have a better solution, tell us. So and I'm hoping I'm not missing anybody here. Christian Perez. Oh, actually I'm sorry. Uh, Christian Dondo says asks, \"Nate, are you seeing the questions on Substack?\" And I responded that you're not. And can Christian ask people to come here? >> Yeah, I'm actually looking in my chat. I'm not necessarily seeing questions other places. Um, I'm wondering I'm wondering if there's another place I can look. So, I'm digging to look and see if I can find that. But for now, come over to LinkedIn where we know that we can see it. Um, and we'll go from there. Yeah. Okay. So, let's see here. Christian Perez says he thoroughly enjoys making fun images with a AI as well as writing with AI. Consumers, however, seem to have a filter for it. They'll say, \"Oh, that's AI.\" And it becomes a product of lower caliber, >> for lack of a better word. Is this because we haven't crossed the uncanny valley or do we innately value humanmade creative works or are we just seeing the transition phase and future generations will just accept AI art by default? >> So really like three or four questions in one there. Um I do think we're in the middle of the uncanny valley. I don't I don't think it's that I I I don't think it's the case that we anyone who suggests we're early in the uncanny valley I think hasn't seen what AI can do. Um I I tend to be of the persuasion that art is what people make of it. Um so art is what the artist is going to choose to create with. And I think that future generations will probably look at digital tools as tools for art in some cases. And I also think that we're a tangible enough species that physical tools for art aren't going away. Um it's this is actually a very real story I have. Um there's a local mural painter who's very famous uh in my town. Uh and he uh is going to come by and put a mural on my house at some point in the next few days. And that is a that is a physical thing, right? He's going to get out the paint. He's actually going to put something on the house. And that's uh he's so famous that like it's something that like everyone in town loves. Um and it's sort of it's considered public art. That kind of experience knits a community together. It's not something that you can sort of easily replicate. But I don't think that undercuts the fact that digital art is going to get bigger and bigger and bigger and bigger with AI tooling. >> Yeah. You know, part of it is we we are in this transition getting used to the new stuff phase. and bigger and bigger. >> Yeah. >> Right. I mean that's that's this stuff for most of us is only a few years old. >> It is. It is. >> And the capabilities change all the time. Um should we jump to some more questions? >> Yes. And I wanted to call out I think I can I think I found the spot on Substack where I can see questions. There's some sort of live chat that I've dug into. Um, and so, uh, I am now seeing questions on Substack where my Substack folks want to put them, uh, in the live chat on the live stream. So, have fun, guys. I I'll continue to report in those questions. >> Yeah, sounds great. And I'm looking at LinkedIn and I and you know, audience, we have I have another question for you. Uh, the tradeoff of Nate and I riffing on things versus focusing on questions. You can see this is a big experiment for us right now is if Nate and I riff on something for a while, it's focused on that topic. If we take the questions, then we're going to be jumping around. For example, I'm seeing questions relating both to nano banana and art and I'm seeing other questions relating to that MIT study and it's >> and I have a third set of questions coming in here. There's ones about Microsoft, Azure, and OpenAI. And the Substackers are saying they hate LinkedIn. We've got some good old audience here. I love this. >> So, so let me say to your, if I may say to your Substackers, Hi folks, I'm not on Substack. Um, I could I entice you to come over to LinkedIn. Not that I like LinkedIn particularly, um, but but hi Substackers. We'll get him over to Substack eventually, guys. >> Well, then there's there's that, too. Okay. Um, let's So, so regarding this point about questions versus jumping around, folks who are watching, share your feedback. Okay. And also, as I said before, if you're on LinkedIn, send a LinkedIn connection request to Nate and to me so that we can invite you to the next one. LinkedIn has all these weird idiosyncrasies. Uh, so just do that. Anyway, so let's jump back to that MIT failures report. And Ian JSON makes a really good point. He says the report is largely talking about pilot projects. If the goal of the project is to learn, then no, it won't have a measurable direct ROI. So that's a good point. >> It is a good point, but I think it's a little bit circular. Um because one of the things that I have noticed about pilot projects that don't have uh business goals associated with them is that they tend to have a failure rate even higher than the rate of projects that deliberately attempt to hit business goals because they run inevitably into the corporate politics that keep anything that isn't on the revenue line from getting done. And so I've seen cases where they're like, \"We want to start out really simple and easy with AI. So we're just going to take our HR policy manual and we're going to put it into a rag and then people can query the rag and they can find out what the vacation policy is.\" Well, there's nothing wrong with that, but it doesn't hit the business goals in any way and it tends to get depprioritized. It tends to not get pushed for use and then when you talk to someone about it, they're like, \"Well, we didn't generate any value from it.\" Well, part of that is the goal. >> Yeah. And I'm sorry, I've been looking through the questions. I'm like getting overwhelmed with the uh >> Do you want me to propose a question because I have one that I think is really interesting here. >> Yeah. Pose a pose a question to me or to the audience. >> Uh to Well, no, this is this is from the audience to us. >> Oh, perfect. Great. Great. >> Um so, uh this is from Kim uh on Substack. Microsoft just announced that they'll switch between OpenAI and anthropic models in Office 365. Uh what are your thoughts? Uh is there some kind of impact here on Microsoft's investment or relationship with OpenAI? I'm paraphrasing very slightly, but but that's the idea. I think that's a really interesting one because it gets at the ongoing um somewhat public separation between Microsoft and Open AAI where they were very very close a year and a half or so ago and now it's not so much the case. Now you can see that their incentives are diverging. Uh they are still disagreeing about how they want to structure their long-term relationship together. what had started as a partnership changed when OpenAI began to take a lot of outside investment and began to grow its valuation very very quickly. And so I wasn't surprised to see that Microsoft is starting to bring frankly capability they've had in Azure for a long time into the rest of their product offering and product suite. Their incentive is to keep you using Microsoft products. That's it. And so they're going to act in accordance with that incentive. I was just going to say for both companies, self-interest is the guiding rule here. >> Well, and they have different businesses, right? Because for Microsoft, cloud is so huge. >> They're different companies. Yeah. >> Entirely different companies. Yeah. >> They're different companies. You know, look, they had a love fest. Okay. We were on a summer vacation and we met each other. And >> that's exactly what it was, too. >> And we had a good time. >> We had a press conference. like >> and we fell in love and we had a press conference and then we got home and I don't like that you do that. Well, I don't like that you do that. >> Yeah, >> but we're married now. >> Here we are. >> Well, maybe we're not married, but we're definitely living together. And >> yeah, we're we're definitely cohabitating. There's definitely a relationship of some form, right? Like that's that's the the dilemma for Microsoft and Open AI >> and they you know now what's interesting is will who needs who needs the other more? Does Microsoft need open AI more or does AI does open AAI need Microsoft more? >> And that dynamic is shifting as open AAI takes on more outside funding and starts to scale up. When OpenAI started this relationship, it was not a billion user company and now it is or close to. >> Yeah. It's like, you know, well, part of this Yeah, we met in the summer, but I wasn't rich then. >> Yep. >> And I'm really rich now. >> That's right. >> And uh Oh, by the way, did you see Oracle's earning announcements last night? Oracle this morning was up like, you know, 30% today. their stock all on the back of Oracle cloud infrastructure growth including uh open AI, >> right? Like they they are trying to push this narrative because for for so long when I was in tech, Oracle was the dinosaur, right? Oracle was the old way of doing things and everyone was trying to move you off Oracle and and Larry and his executive team are trying to really really push the narrative that Oracle has now fully pivoted and Oracle is going to be the backbone for the future of AI inference. And I think one of the biggest questions that really unfolds is how big is AI inference going to get? I I was looking at a breakdown of the return on investment dynamics for data centers and it's very difficult right now to get the math to math. It is hard to show that a given data center investment is going to generate conventional ROI multiples because chips depreciate fairly rapidly. And so it's not like the railroads where you put this tremendous investment in and the iron is there and like you're going to drive on that iron for a long time. You have to sort of make that money quickly. And so I think one of the biggest unanswered questions in the business right now is you have this tremendous demand for AI but how are you going to get the data centers themselves paid for? >> Yeah. there. It requires this enormous capital investment and in addition there's a set of other issues also relating for example to electricity and the electricity grid. >> So there's a whole there's a whole host of issues. >> I'm going to jump again to a question and again this is from Gustavo Salami for a very simple reason. um he put at the front of his comment question so I can identify it easily as a question. So I'm going to ask again we're we're all figuring this out together. So I'm going to ask you folks in the audience to do two things. Okay, please. One is can you ask respond to each other's questions because you guys who are listening are really really smart and experienced. So, if you see a question and you know the answer, just respond to it. And if there's a question you like uh that you think is good, just like it and we'll see who which questions get the the most likes. So, that like a voting system. And if you have a question uh if you can put question, I don't know if maybe that's >> I'm not having trouble filtering for questions over here. Like I'm seeing people respond to comments in the substack and respond to questions. One that hasn't been answered is uh this is a really interesting one that I think gets at the future of like we we wanted to talk about jobs a little bit and how jobs are shifting and all of that. This sort of opens the door into that conversation. So I think it'd be worth grabbing. Um it's from Pavle uh on Substack. With developers increasingly using AI to write code and code reviewers using AI to review the code, how much of a concern is it to an org? And do you see change in how deployment processes work? I think that's worth answering on its own, but I think it also sort of begs the question, how is engineering changing as a job family, which opens the door into the whole jobs conversation. So I think we should kind of like go there in stages. >> Well, there was this other report uh that came out from two Harvard economics uh researchers, two Ph Harvard economics PhD students >> titled generative AI as senioritybased technological change. >> And if you want to find it, you can easily search for it. And they made a really interesting point. They said it is junior level employment that suffers uh from AI and that this damages the senior level talent pipeline. So in other words, as AI replaces junior level jobs, aside from those job displacements, you have a very significant set of implications for senior people. So how do we train senior people if we're getting rid of our junior folks? And I don't think we have the answer to that as a business right now uh as an industry right now. And I think it's actually more even more uh fraught, even more complicated than that because if you look at the charts and so on, it's not that junior employees have gone to zero. It's that sort of the the headcount in those roles has declined in correlation with the roll out of AI just as the headcount in senior roles has risen. If you look at it that way, the juniors that are getting in are learning in a world where as Pavle notes, AI is being used to write code, AI is being used to review code. Uh, and they don't necessarily get the experience of engineering principles that they would have gotten in a preAI world. And so one of the worries that some people have that I've talked to who are senior in engineering is that the juniors coming through, even the ones that are getting hired are not getting the grounding and engineering principles they need because they're incursor from day one. And you don't necessarily get them to a point where they understand engineering systems so that they can actually architect those systems and be in the room and be seniors when the time comes even if they are juniors now. Does that make sense? >> Yeah. Uh I mean that whole apprentice system is being decimated. I mean it doesn't really exist now. Uh well in some in some fields it does. There are certain there are like in in the trades right you still have an explicit apprentice system. I guess you could say uh with doctors you have uh interns, residents, fellows, which is a >> view tech kind of was like that for a while, Mike. Like in the 2010s, it sort of worked that way. >> How so? Can you uh be be more specific? >> Yeah. like uh I I recall very vividly hiring conversations in the 2010s where you would look at an intern coming in or you would look at someone who was entry level coming in and they were in engineering or product or marketing um and you would say I see enough in this person that I think they can grow. I think they can perform at a junior level or an intern level today and I see potential for a high ceiling. And then you would deliberately pair them with a senior person to be a hands-on guide. And that person was responsible for getting them through those initial rocky first few months and getting them to productivity and starting to mentor them up into the next level. >> Don't we call that mentorship? We do, but I think it functioned at scale kind of like a guild because if you think about it, you had like acknowledged experts across all of these disciplines who would then be tapped on the shoulder and said, \"Okay, now it's your turn to sort of bring this person along.\" >> Yeah. That you know that has also really been affected by uh the pandemic >> and people not going into the office. It's really really hard to do that. It's much harder to do that remotely than it is in person. >> It is. Yeah, it it is much harder. Um I have found that with onboarding, it's really really hard to onboard remotely. It's hard to onboard other people remotely. >> Uh Leo Gome says he'd like to help me figure out the live video with QA challenge. Hey Leo, just uh send me a connection request and a note. I love that. >> Okay. Um I'll take all the help I can get. I really appreciate it. Uh so this is this mentoring system doesn't change I think the more powerful reality that AI will eliminate certain kinds of jobs. Right? if you're a translator. I mean, remember that last time we spoke about this report from Microsoft where where they analyzed hundreds of thousands of uh Microsoft co-pilot chats >> in order to determine which jobs would be most affected and by AI >> and uh lock controllers, you know, canals. >> Oh, yeah. The ones that were at the bottom of the list. Yeah. Yeah. dredge operators were at the bottom, right? I mean, if you're running a dredge in the ocean or in a river, >> your job is safe. >> But on the other hand, if you're a translator, the then you know, if I had kids, I wouldn't want to send my kids to translation school right now. >> You you know what's ironic about that? the the current data that we have suggests that the number of translators hasn't yet been affected. Um we would need to look it up, you know, every few months to see how that shifts because that can be a trailing edge indicator. But I wonder if one of the crosscutting currents in this whole conversation is the idea of accountability. And so if you're a translator for a corporate uh for for a corporate entity, they want a ringable neck, right? They want someone they can call up and say, \"You screwed up this translation.\" And now, you know, 500 people in Japan think that our washing machine looks like this, right? Or works like this, and it's your fault. You got it wrong. And you can't do that with the AI. Where this really comes into play is in medicine >> because this issue is so fundamental because AI is taking over. Taking over is too strong a term. AI is making inroads into medical diagnosis and accountability in medicine is everything. It's one thing to get a trans a translation wrong to miss a few words. It's another to make a mistake. This is part of the cultural change of how do we deal with this? >> Well, and I it gets at something that like comes back you to medicine, to engineering, to any field where you have specialization and you're trying to figure out what to do. I think that one of the things that humans are really built for is understanding that this person has this job and they're accountable for that job and they go and do that job. But now we have a world where maybe that person is still accountable for the world. The doctor is accountable or the engineer is accountable for pushing the code but they have an assistant. They have someone else in the mix and I use someone loosely. They have another machine intelligence in the mix that is working to help them. and they are responsible for figuring out how that works. And I think that we would do better as a society if we started to establish some norms around where we think uh there's a level of assistance that's appropriate for those situations. So in what situations does the doctor have an obligation to consult chat GPT because the diagnosis is statistically better? In what situations does the doctor have an obligation not to potentially in what situations does the engineer have an obligation to doublech checkck because sometimes the LLM will go through a particular code and produce a different result because it's a nondeterministic system functionally and so we we need to have those conversations and we're just we're so used to a much simpler way of organizing ourselves. >> Yeah. It's because the historically we did not look to computer systems for creation so to speak because if you're if you're diagnosing if you're looking a radiologist looking at an X-ray for example to me that falls into the category of creation because you are making a comparison between this two-dimensional representation that you see >> against your own internal body of knowledge aided by your intellect and your intuition in order to make a decision. >> And to me that's a creative act. And historically we didn't think about computers as they were mechanistic. you know, it's like big adding machines. >> Mhm. >> So, there's a difference in how we we anthropomorphsize uh computers and with good reason. You know, it's like, hey, you know, you can't put a good computer down. [Laughter] Well, I it gets at this idea. So, I published a video today on reading, right? Like, and I and a little sort of article on reading and how we read in the age of AI. And I think that um you might look at that at the surface and say, \"Well, Nate has a lot of books and so this is about reading books.\" But it's more than that. It's about this idea that you're talking about, Mike, where you say, \"How do we read?\" Where we bring our experience to bear. We look at something new and we start to metabolize it and sort of pull it together. And it it doesn't have to be a book, right? It can be reading an X-ray. It can be uh reading a section of code. It it's this act of sort of bringing in new information against our existing experience and it's the act of figuring out where do we do that and read deeply versus where does the machine read for us and how much do we trust the machine to read for us in particular situations the trust part I think is easier because the uh hallucinations will decline over time we will that will get better get better but it goes to the heart of the kind of skills skills we're developing. For example, what is the value? I love music. Let me preface this. What is the value of somebody who is a great musician or a composer when you can program the AI to create music that's really really good? There's some obvious answers to that, but this also uh raises questions. And I think I think one of the things that we we have not fully sort of processed through is like we live in sort of a pop era where like music and audience and celebrity are all sort of in this overlapping ven diagram. And so, you know, the the story of Sabrina Carpenter and her new album that dropped is as much about the controversy around the album and the album cover as it is about her headline in Coachella as it is about the music itself. And so, we we're used to this all being sort of overlapping. And one of the things that AI music does is it sort of uh pushes that apart because you don't have a celebrity there. You don't necessarily have a personality there. You just have the music. And it may be good and it may not be good, but either way, it's different from the way we usually experience music these days. >> Well, there's a lot that's going to be different. Hey, just for kicks, I'm going to show people what I'm actually looking at and why this is so hard sometimes. You guys, can you guys see this? That is my This is basically the Nate and Mike control center. And that's why >> Look at that. >> Yeah. And I'm looking at this and trying to get the questions at the same time, but I I want to really thank those folks on LinkedIn who are responding to each other's questions. And if you're on Substack also, you should do do the same because >> definitely doing that over on Substack. We're getting answers back and forth. >> Oh, that's great. And folks on LinkedIn, help your fellow LinkedInites uh out by ask answering their questions. But on this subject of LLMs on LinkedIn, uh Jonathan Cousins has a really good one. He says, \"Can LLMs be leveraged to perform the role of a mentor? Would you expect interns in 2025 and beyond coming into an interview having invested in pre-mentoring?\" How's that? >> That's a great question. Um I think it's already happening in practice. uh like people who are coming into interviews are using AI to prepare extensively now. Uh whether you call it mentorship, whether you call it interview prep, uh there's a lot invested in basically taking advantage of the fact that you have this intelligence in your pocket that has probably read up on the company you're interviewing for, has probably read a lot more than you have on the job family you're in, and you can work with them. I think where it falls down is that classical mentorship uh is very contextrooted and it assumes you're in a particular problem space. You have a particular situation you're trying to solve and it helps you take that general body of knowledge and apply it. That is the gap that I see that is really hard for AI knowledge bases to cross right now. Well, we have uh kind of a related question uh from Ed O'Connell who says, \"There are so many emerging approaches to long-term memory in AI.\" Uh he says, \"Me zero, Langraph, and others. From your perspective, what's the best way for someone to evaluate which approach actually fits their needs?\" He's especially interested in two contexts. one as an individual developer trying to manage complex software projects and at the institutional level where real organizational memory is at stake and with so many solutions. How can we tell what's signal and what's noise but it gets right to the heart of institutional memory and what matters. >> Oh, it's such a good question. Um I feel like there's like two big categories, right? Like the organizations I talk to are first trying to figure out how they take data that they view as proprietary and valuable and get it into some format that they can use with AI. So there's that whole piece of like moving our data out of a traditional database into a rag system or something like that. Um and then there's the whole question of what is the memory that matters that isn't written down or that we sometimes call institutional memory, right? the memory where like you know Beth in accounting knows how this particular invoice system works or this product manager knows how this particular uh workaround works and can explain it to customers in a way nobody else can and that is highly valuable information. It's almost never written down well and it's difficult to get anybody else in the organization to understand it because everyone has their other jobs let alone to get AI to understand it. And I think it's actually one of the biggest questions in business right now is what do businesses do with that kind of information? >> Yeah. Well, when we were talking about mentoring, so much of mentoring is about that retained institutional knowledge. But it also raises another related question which is the we now have the massive encyclopedia in the sky that knows everything. Yeah, we do. Uh, and it's but what is it being called now? Like the people who are sort of trying to transition out of SEO or just calling it AEO, answer engine optimization. >> Yeah. So, we've got, you know, the whole SEO thing is like is another category. I mean, people people are always trying to game the system. And I I shouldn't cast dispersion on SEO. I realize it's, you know, look, Google is your silent partner if you're creating content. Doesn't matter who you are. Uh, Google is your silent partner. And if you think about Tik Tok and Tik Tok videos, and Nate, you're the expert in this, but what's the structure of a Tik Tok video? Focused, to the point. start with your initial hook to grab people and that's a nod to okay in this case not Google but it's a nod to basically the same thing to your Tik Tok overlord am I not correct is that wrong >> no and I think one of the things that you learn if you're sort of on different channels is that the algorithms work really differently um and you sort of start to understand what the algorithm is going to reward what the algorithm expects you to do and you start to adjust to that. And so content is not just content. Content is shaped by the algorithmic environment. >> Yeah. So anyway, so SEO, that's what that's what that's about. Hey, you want to take a question from Subsac? I've been pulling them from uh from LinkedIn. >> Yeah. Oh, there's so many good ones here. Um, I think that one of the ones that that I kind of want to dig into a little bit is we've talked a fair bit about this idea of um, hallucinations and and Benjamin on Substack has a spicy take which I think is worth bringing up and talking through. Um, he says, \"I doubt hallucinations will go away as AI systems get more advanced. I would argue that the more general the AI becomes, the more vulnerable it is to social engineering attacks. And I think that gets at one of the really interesting questions right now, which is how how do we understand what in advanced intelligence or better AI is in a world where we have so many capabilities we expect of those systems. And I think that question is tied together with the idea of hallucinations because the more an AI can do, the more opportunity it has to do something unwanted, which is I think my best way of explaining what a hallucination is. It's sort of like a weed in the garden. It's an unwanted plant. Well, a hallucination is an unwanted answer. That's the Yeah. I mean, that's the definition of nondeterministic AI tools, right? That's what we're talking about is you give in an an you put in a question and Am I allowed to curse here? >> Uh, I don't know. We haven't figured that out. >> Yeah. So, you give you you put in a question and it gives your out the other end. >> Yeah. Yeah. But you don't know how good that is until you try it out. And that's the nature of it. And all of these techniques like Rag and all of these companies are working trying to put boundaries so that it only gives you questions relating to your data and not making stuff up. Well, and and the challenge is that the deeper you go into the question, the more multi-dimensional it gets. Like I think that uh Benjamin is right that there's a degree to which reasoning and inference can sometimes make the AI uh backtrack and confuse itself when it's trying to answer a question that is in a particular domain that it feels like it needs to train to reinforcement learning data versus the question you gave it. uh which is a lot of the reason why sort of writing styles are sticky um on some of these systems. And then there's also the dimension that you're talking about where you're in a rag and you're telling it to retrieve and what if it does retrieve from the rag but it retrieves the wrong thing from the rag or it puts the things together in a way that doesn't make sense. So it retrieves the quarterly number from Q1 and it associates it with Q2 because of the way you use chunking strategies. Um and that's that's not something that's easy to solve. And I think that I I I am beginning to wonder if instead of talking about hallucinations as this sort of um sloped line that goes to zero eventually or whatever it is, we think about it as the opposite. How do we start to recognize that AI systems are going to be a part of our workflows in a wide variety of domains? And our goal in each domain is to define positively what good work from the AI system looks like in a fairly high degree of detail. What does good engineering work from AI look like? What does good product work look like? What does good CS work look like? And then how do we start to measure positive performance and then we can start to measure defects from there. But we've spent so much time talking about this general category of defect called hallucinations. I think we miss some of that opportunity to get more specific. Uh, well, there's creativity, right? I mean, the hallucinations are the definition of creativity, but but I'm not sure that's really what you're asking. >> I I don't think that's really what I'm asking. I think what I'm suggesting maybe goes back to this idea of mentorship and roles and like how we read AI and bring AI in. It's this idea that there is um a missing category of information or discourse or conversation that we should be having now that we have these systems and everyone has them in their pocket. Uh where we don't know and we haven't talked about and we don't have an agreement among the best in the business as to what good work looks like. So as an example um in the product realm what does good uh chat assisted product requirement documentation look like? What in the engineering realm does a good code structure that an LLM can read and contribute to but that isn't uh croft that isn't slop after a year look like? Um and you can go into other fields and kind of extend that but the idea of like what is good work is underd discussed I think. >> Yeah. I just read a study that said that the there are relatively few AI productivity gains because by the time you or that those gains are very often an illusion because by the time you figure out your prompt, by the time you wrangle back and forth with the AI and then clean it up and if you're a lawyer, for example, make sure that it hasn't invented case law for you and filter all that out that you could have just done it yourself. >> So I take I'm going to get spicy. I take some issue with the way that is framed because I read the same thing. I saw the same results but the anecdotes don't match the data. And one of the things I learned when I was at Amazon is if the anecdotes don't match the data, you probably the anecdotes are right and you're not measuring it right. And so I want to suggest that part of the way we traditionally frame productivity is we take a standard job and we say is it better right do we go farther do we get more done in a day >> but the stories I'm hearing don't that category very well so um a good example uh is this is a public story I think I can share this um there uh was a dinner in Stockholm for uh creators on lovable.dev dev, which is headquartered there. And they brought out some like people who were like creating really interesting software. And one of the stories that popped out uh was a woman who uh taught Polisheing uh people in, you know, Warsaw how to speak English. She was an English teacher and she had a proprietary system. There's a particular way that she wanted to teach Polish speakers English and she had like 20,000 flashcards and a special system for like associating images and words and it was this whole thing she had evolved over years. She never programmed. She never did anything with code. And she used Lovable to take her entire 20,000 card system and turn it into a usable app. I can't think of any argument that would suggest that's not a productivity gain for her. It's a gamecher for her, but it doesn't show up in traditional measurement. Like is she like, you know what I mean? Like that's not inside her traditional job family. >> Yeah. Actually, uh, on LinkedIn, Ryan Kyle says, \"Productivity gains are often in productivity, not in ROI.\" But that doesn't solve the problem because that's essentially what Nate's point is that we know that there's a productivity gain because it's obvious, but but it doesn't show up in ROI. And for her, it's not going to show up in ROI unless she has an ROI metric. So, is she selling these things? Is she creating new ones and she can measure her output? >> Or maybe >> I think there's an idea of monetization for this, but but like it it sort of goes beyond the one story, right? Like when you have the ability to put this into software in a way that you didn't before, you open the door to like the oneperson business, which we see a lot of examples of in the last year or so that we just didn't see before. That is see that's the that's the unknown part right now. That's the part of the equation that says how will AI change what we do in many different and that's why AI stocks are so high in the market. That's why billions and billions and billions of dollars are being invested in data centers by the hyperscalers and by others. It's because we all have this sense that AI is going to do something great, but we don't know how it's going to fundamentally change our lives. And I think the reason for this is because to date, for the most part, AI has has been, shall we say, point solutions. So, for example, they solve a particular problem. So for example, Adobe uh is doing a lot with AI talking about uh in Photoshop the ability to remove a background to automate to to take this to change this to change that. >> So that is a gamecher and enables you to do something different that you couldn't easily do before and to do it better. >> But does that change your life? Does that fundamentally change the way you relate to creating art in the world? >> I and and and let me just I'm sorry. >> Yeah, go ahead. You finish and then I'll take my >> to does it fundamentally change how you create art in the world? And does that fundamentally change the nature of your career? >> I think it does. Um, uh, someone someone on the substack, Christopher on the substack actually had a question back that I think is is a great way of putting that. What was the ROI for finance teams when they learned Excel? Uh, I don't know if we actually measured that. Um, but Excel is ubiquitous across finance now. Uh, another article that I saw pop up on my feed today that I think sort of under underscores that is this idea that uh, these tools are about democratizing access to previously complex pieces of work. Um and in that world we in that world we find that we have new peers. We find that we have new uh ways of working and creativity unleashed because there's just more people in the system. Um and the article that I was reading basically suggested that AI is the greatest accessibility lever that we have ever seen. And it's the greatest leveler for people of all abilities to be able to do a complex task because it makes things so much easier. It makes writing easier. It makes uh as we're seeing image editing easier. It makes uh so many things are at basically your ability to speak and communicate intense like that becomes the barrier now is your ability to show this is what I want. And so the question then becomes is the future people who are able to communicate intent with high fidelity and with genuine insight into what needs to be done like they have like highgrade intent that other people are going to get excited about them sort of building from intent. So that's the So is that the core issue here? What we're trying to accomplish our intention >> is that the new grade for quality work that's something I'm sort of wrestling with because the people who use AI and make the most of it are people who have intent that maps deeply on to the ultimate outcome of the task they're going for. And I sometimes see this with high level uh individual contributors in companies where their intent um aligns really well. They're able to communicate it to the AI. They know exactly where they're going. They know where the outcome is. They know what they need to do to get there. For founders, it's sometimes about aligning intent to the marketplace, aligning intent to the customer. And so when I think about sort of prompting, people often ask me for prompting tips and tricks. And you know, Lord knows I publish lots about that. But it's about intent at heart. It's how do you communicate intent in a way that is clear and deeply congruent with the outcome you're looking for? >> I don't I don't know the Michelle Clark, let me just jump I'm just jumping at random here uh onto LinkedIn because Michelle Clark asks this question that's right directly relating to this. She says it's not about intent per se. That's not enough. Setting and managing expectations is the big failure with AI's roll out so far. Where are the communications pros? We we know how to do change management, but we're not doing it. >> That's an interesting question because I think the answer that I would give having sort of looked at organizations rolling out AI is that that is both an absolutely astounding yes uh and also no. Um and so maybe we can open up both and see sort of what we think. uh in a sense I think yes it's true that organizations are investing less in their people than in the tech for this and so I would say like 10% of the overall AI budget right now is going to people change and that doesn't seem correct to me and I think that good change management would suggest that we do something different >> so yeah go ahead >> no I didn't mean to interrupt please >> no the second piece the sort of the contrasting thing is that I see that people are treating change management when they do it as if this is just another technology that needs to go through traditional change management practices and then we get out the other side uh we'll have all adjusted to the change and that is also breaking down because this is a general purpose technology and it doesn't respond to traditional change management that's sort of predicated on the idea of like here's a new software here's a new process or here's a new sort of organizational uh structure that we all need to adjust to and so I think it's kind of yes and Yeah, I mean I'm I'm hesitant on this one because for for decades now the problem with enterprise technology as we discussed earlier was the inte integrating the technology into processes and do you adapt your processes to the new technology and to do things that are better? Do you fit the technology into what you used to do before but was inefficient? I mean I don't have to rehash all of this but I do have a question for you Nate which is you said that there is a very small amount of investment in people as opposed to in tools. So, are you calling all of these enterprise execs who are talking about how much they're investing in their people and how much they care liars? >> I think that we have always had a public narrative uh from companies about the importance of people and investing in people. Um, I think that in this situation, it's not so much that like it's about the drama of saying they're liars as it's about the implicit assumption that investing 10% of the budget in people is enough because it's a relatively easy technology to learn because you can sit down with it and see the chatbot and use it. And so the executive basically says, why would I invest more than 10%. I am investing in my people. This is enough. And so the the mental model needs to shift that this is actually a much more complex tool than they might realize at first glance. >> Well, I will say that the consulting companies, the large consulting companies have invested literally billions over the last number of years in an in an ongoing series of investments in retraining, reskilling their people and tools. But that also makes sense because their stock and trade is people expertise. >> That's right. like they're they're heavily incentivized. Uh we have a question from Gustavo in the substack that ties right into this. Um and the the question is what do we think about OpenAI's announced plans for certification for potentially an AIdriven job market uh facilitated by OpenAI? This ties into the jobs question. It ties into the productivity question. It ties into certification. All all kinds of things we've been talking about this this hour. You know, I think this is not much more than standard enterprise software enterprise technology maturity. >> You have uh you have a company, you have a product, it becomes popular, >> you are trying to grow your customer base and they want training and you build a certification program. So I don't I don't make too much out of it. Do you think I'm missing something? To me, this is just standard straight vanilla enterprise software growth playbook. >> I think it's the same playbook, but I think you have to filter it through the ambition that this company has demonstrated for everything else it's gone after. You know, this is the company that hired away uh Johnny IVive and basically is trying to create uh the next massive iPhone, right? whatever that's going to be, we haven't seen what it is yet, but they have the ambition. My suspicion is the larger ambition is to disrupt LinkedIn and the LinkedIn job market and to answer in public the charge that's being leveled at them and other major model makers that the technology that they're pedalling is going to be net net a drag because it's going to harm the job market. And so they want to basically come back and say, \"No, we're good actors. uh we know that the current sort of job pipeline is really broken because AI powered applications are getting reviewed by recruiters who may be using AI and so nobody's getting anything done. Uh and instead of being like Cloy and saying well you can cheat your way through it they're going to say we're going to be the good actor we're going to facilitate this and that's partly a PR move. It's partly what you're talking about where it's a very standard move and then it's filtered through their ambition and I think they're trying to make it bigger. Yeah, I I I think that there's it's it's a good point. AI that one fundamental difference between uh open AI and everybody else has been that meteoric growth. And >> yeah, >> somebody also on uh LinkedIn made that comment earlier about this potential interest in being the new LinkedIn. Some interesting arguments to be made in favor of that. By the way, >> I mean I think you're seeing it like it's funny for me to read the Substack chat because the Substack audience does not want to go over to LinkedIn. My Tik Tok audience does not want to go to LinkedIn. My YouTube audience does not want to go to LinkedIn. like there's there's a strong aversion factor to the to that that makes me suspect they're ripe for disruption. Um but no one has really quite put the pieces together to figure out what disruption looks like. >> Yeah, but I mean come on. How many users How can you compare the number of users of Substack? Uh >> you can't. That's why there Well, that's why disruption is hard. You're exactly right. Is that you have like uh you know multiple billions of users on LinkedIn, I think at this point. Um, and it's not it's the gorilla in the room. >> And uh, Randall G said on LinkedIn says LinkedIn is broken. So, listen folks, you know, once again, if you're asking questions on LinkedIn, send a connection request to Nate and to me so that we can invite you next time to these chats. And by the way, we've had a number of questions asking uh about the technical setup. And so let me just briefly tell folks what it is. This is especially on Substack. So we've had we have folks who are sort of cross-pollinating between LinkedIn and Substack or I should say Substack and LinkedIn and sending transmitting questions from Substack over to LinkedIn. So basically the setup is this that Nate and I are talking over Zoom. Nate connects to a standard Zoom call that goes into Zoom rooms. And we use Zoom rooms because for a couple of reasons. One is it takes processing away from the local computer from my computer and segregates it onto a separate machine which happens to be a Mac and then and it also does a very high quality 1080p video. And then from the MAC that you guys can't see off to the side here, we use a protocol called NDI, which is vaguely like Bluetooth, but for video. It lets you transmit video over IP. That then gets fed into this switcher here. Uh, I don't know if you can see the mouse moving, but the thing that I'm looking at and I can give you a closer look. You guys are really >> We had a request to switch off the technical mic that to get back to the AI question. So, let's make sure that we do that. >> Yeah. Okay. Anyway, so this is a it's >> and people are offering to like send me advice on how to make this better. So, I think what we should do is give the audience homework to like get in touch with you or me and give us advice on how to make it better and we'll just see what works. >> Yeah. Yeah, and especially the chat. >> Yeah, >> we can figure out the chat. Anything else interesting that you're seeing on uh Substack right at the moment? >> Uh well, Claude is down. That's that's one of the things that we should probably say is literally during this live stream, Claude has gone down. Um and I think it gets at this idea of reliability um and the challenges that come with these tools that we keep talking about. Anna, they're everywhere, right? We're going to use them all the time. Well, what that we can't use them if they go down. Um, and both OpenAI and Claude have had very public issues with reliability. And what's funny is I don't see the same reports of things going down as often with Gemini, but it doesn't get talked about as much. >> Yeah. You know, this is it failures. I used to write a lot about and study it failures um, a long time ago. And again, you know, I I separate all of this stuff into really two buckets. AI specific and general tech and change >> issues and you know look historically uh there have been times Salesforce has gone down there have been times Gmail has gone down it's a the broader problem is how deeply entrenched uh these cloud-based tools are and we run our lives and our businesses on these these tools and when they go down, which eventually they will, sometimes it can have nothing to do with the provider itself. I mean, there could be a DNS problem, for example. >> Uh, and then you're dead in the water, you know? It's like, hey, if the electricity goes out, you're screwed. >> Yep. Yep. And people are pointing out that Google has like this 139s of reliability culture that is helping them with this situation. And I think that engineering culture is absolutely at play in a positive way. Um I do have a couple of questions coming in over here. Um Kim asks, uh given bold predictions that AI agents will be common workplace tools by 2025, how realistic is that actually turning out to be, especially around deployment at scale, safety, and ROI? And then I'm going to add one specific prediction that got a lot of press uh in the last week or two, which is Dario Amade, the CEO of uh Enthropic that makes Claude, saying 90% of code will be written by AI uh in 2025. And people are taking turns arguing over how true that is. And I think that that it's a nice doorway into that question set. Well, look, code is one of the primary areas that large software companies are using um to replace people right now. >> So, >> yeah, it's it's so Did you see Goldman rolled out Devon uh to their entire organization yesterday? >> Oh, no, I didn't see that. >> Um, so it's it's absolutely happening. I think that one of the things that probably doesn't get talked about enough is that um organizations like Anthropic live in Silicon Valley and the people they talk with are startup founders and startup founders are coming out and saying yeah 90 95% of the code we write is written by AI. Um and then public companies that want to talk about it are saying that they're not at that goal. And I think one of the more interesting questions because the obvious one is like well the big company has a harder time changing. Well, fine. But is it even the right metric for a larger company? Because if you set for an organization of 10,000 engineers, your goal is to have 95% of your code written by AI. You're not putting any quality metrics in place there. You're not talking about functionality. You're not talking about product market fit. You're not talking about anything that sort of uh goes with clean code, clean architecture, deleting lines of code, not writing too much. And so you may get into a situation where a goal by a CEO who wants to get you to use his product is getting used by other CEOs to push for changes in their engineering org that may not be helpful long term if it's a large business. >> Yeah. you know, uh, but, uh, Mark Beni off from Salesforce came out not too long ago and he said, \"We laid off all of these people because AI is writing code.\" Uh, I'm so glad you brought that up because if you look at the hiring pattern for Salesforce for the last two years, they laid off much less than they hired over the last two years during which AI was coming on the scene. And one of the comments that I've seen circulating uh among leadership not at Salesforce is that Mark wants to cover up the fact that he overhired. Um and he wants to call it AI and it isn't. >> Well, that is very interesting. Uh had not thought about that one, but it's certainly not out of the realm of possibility. I can tell you historically when I was studying IT failures, it would definitely happen that there was some tech some business issue. For example, a company can't ship its products and it had to do not so much with the technology but with some other stuff going on at the company. But it was really easy to blame it. Oh, you know, our systems went down because it screwed up, but in fact, our systems went down because there was some finance problem, you know, whatever whatever it might be. And it is very convenient and Mark is the genius master of presentation, communications, and PR. and he knows how to take anything >> and turn it around so that it's positive for >> Salesforce. >> Exactly. >> We don't know the truth behind the scenes >> as you point out. >> No. And that's that's one of the things that I think doesn't create trust, right? Like if you're looking in this world and it like it feels like it's uncertain and for junior talent it feels like there's even more uncertainty because of what we've talked about in this in this time together. It doesn't build trust when you're looking at the statements from leadership and you can't tell what is substantive and what is AI washing. >> Yeah. But again, I mean, maybe I'm jaded just because I've been just around the tech industry for so long, but I remember going to enterprise software keynotes and especially when I started my career, which is going back a while now. A while now. Yeah. >> Sitting in the audience and not being able to tell what's hype and what's real. >> Yeah. >> Because >> that's not a new problem you're saying. >> No, that is that is not a new it's not a new problem. I mean, it's just come to the four right now, >> right? >> Because we all have this intuition that AI is going to have such profound implications across so many different domains, which I believe it will. >> Yeah. Yeah, >> but all of the, you know, we we we sometimes forget that all of these companies that are making these huge investments and making all of this PR noise, these are forprofit entities. These are killers. You know, the people running multi-billion dollar valuation companies, these are this is not like happening in a garage out back. >> No, no. are big big big companies with big claims and big resources etc. >> And a lot at stake and these guys are killers. They want to win. >> Yeah, they do. They do. And those incentives really shape how we're having this conversation. >> The incentives are all You spoke about intention earlier. The incentives The intentions are >> all important. >> That's right. That's right. We have another question coming in. I think it's a really good one um because it sort of brings us back to how people use AI and sort of the day-to-day experience. One of the big shifts in the sort of conversation in 2025 has been moving from this idea of prompting and prompt engineering, and I'm using the scare quotes, but uh over to context engineering, to this idea that you're not just doing a prompt, but you're actually talking about how the whole context of the of the prompt works. And I think um it is widely misunderstood. Like people people sometimes say, well, it's just the same as prompting. People sometimes uh will say that it's only for production pipelines that production pipelines care about context engineering and ordinary people shouldn't. I'd be curious for your take. I have a take as well. What do you think, Mike? Context matters a lot. The the another interesting aspect of LLMs is we think that the LLM is tailored to us. We ask a question, it knows what we need, >> but it knows what we need because we've told it either explicitly through providing sufficient context in the prompt or in the case of o uh open AAI for example and I think uh anthropic is now doing this through some type of memory function where it is simultaneously reading your previous chats. So it knows about you. So I think that context, call it what you want, giving it enough information is the key to prompting actually. >> Yeah, they're they're tightly related. I think the reason or maybe the differentiator between context and prompting for me is prompting is very intuitively like what you actually write. You might write it in a development environment and give it to a production pipeline and this is your system instructions. you might write it in the chatbot. Uh but either way, you're directly instructing the model at something. Context is broader. It includes that piece and then it includes a bunch of other stuff. And as models get more tools and more capabilities, that other stuff gets bigger. And so if I tell perplexity, here's some direct instructions and then go search the web. I don't even have to tell it to search the web. It just does it. the context for the response is like 99% tokens that it scoured across the web and 1% what I gave it. And I think that's one of the key things that makes people sort of say aha when they realize what context engineering is is that you're essentially using this tiny little prompt that you can directly control like a rudder on a boat to shape where this gigantic sort of context uh slice that the model's going to grab for its response is going to come from. And that might be from your rag internally if you are using a corporate tool. It might be from calling an MCP server. It might be from surfing the internet. But all of this gigantic piece of context is shaped by that rudder of a prompt that you use. And so I think that's sort of how I think about that relationship. Yeah, it's a very it's a it's an interesting and a very hard question because as the LLM, excuse me, is searching across the web to find relevant background and context to inform its response to you. In order for that response to be useful to you, it needs to have a way to filter out the stuff that's not relevant and not useful. But the only way, actually, I'm going to correct. I was going to say the only way it knows what you want is what you've just prompted it. But a really smart LLM knows that as a human being, you have intellectual consistency. And so therefore, your world view as reflected in the questions you ask will shape how it relates to you, which is the essence really of that memory function. But the discrimination between what's relevant and what's not, I I think you start to get into what they call super intelligence. It's not artificial general intelligence. It's more than that. It's not just the >> I don't know that I agree on that one. >> That's good. We disagree. It's going to be a spicy take. Um I think >> How can you disagree with that? Well, I don't I don't disagree that like memory matters, right? Like they absolutely you're talking about how memory works. Um on OpenAI it works as sort of encoding memories and like referring to these sort of snippets that you can see on on Claude. You're searching past chats. So yes, it looks at sort of the past experiences you've had. I'm not sure that I agree that it goes so far as forming a consistent mental model of your responses. As an example, and so I I don't think the super intelligence claim is fair at this time. as an example. Um I have uh a lot of arguments with Claude right now about Claude's ability to not only remember past interaction uh models that I've had, but to keep that same uh preference for how I like it to answer in the current conversation. It will sometimes go and do what you just described. It will search the past chat, but then it will come back and it will not do what I've told it to do with the memories. it will not be able to actually take that aboard because the system prompt from the model maker is so strong that it overrides the memory feature. Yeah, I didn't mean to imply we have super intelligence now. Uh what I'm saying is to do this really well. Maybe it is all a matter of a mechanistic approach where the system prompt has to be balanced and eventually they'll get that right and the machine will simply give the illusion of not just consciousness but of understanding me >> who I am. But that illusion how is that different from super intelligence? That's an interesting question. I think that's probably going to have to be for next time because we're running up against time here. >> I know we could talk about super intelligence next time. Um, yeah, I think that's a fair call out. I think I understand better what you're trying to say here. And it's there's a lot to unpack there. Like what does super intelligence mean? Do we have a common definition? Do we understand what we expect? And it kind of gets at the fudge fizzy fudgy wudginess fizzy wizziness around what general intelligence is and why we argue about it so much is because I don't think we share a common definition. >> You know we we are almost out of time. Can we just talk for a moment about us >> and about the audience? So, as everybody I think you know because we've said it a few times, people who are listening on Substack and YouTube and maybe Facebook and Twitter and LinkedIn, this is has been an experiment and a really great and fun experiment. This is the second time. We want to do more of these. But how so I I have two questions for you guys in in the audience and please think about this. Give us feedback. How do we manage the fact that we that there are audiences in different places? We have the Substack crowd and the LinkedIn crowd and these are more or less walled gardens. They don't intermingle. I'm not on Substack. Um so how do we manage the chat? A related question to that is well see it's the LinkedIn is horrible for chat. I have to every it doesn't refresh automatically. So every time I want to see the the chats I have to refresh the screen and then manually select show the most recent ones. So it's really bad and then I lose track of what mate is saying. So how do we manage that? Please you guys are smarter than us. So can you figure it out and tell us please? or if you have ideas. That's number one. And number two, what should N how can Nate and I do this better? Give us critical feedback. What would you like us to talk about? How would you like us to do this? How often should we do this? Tell us, please. >> Well, and I think another related question you and I were talking about is does it make sense for us to bring um a guest on and like have a conversation with a guest for part of the show? Um, is that something that would be fun for folks? And like that helps us to sort of figure out like is there a theme to the show? Do we want to talk about a particular person? Um, yeah. So, I think that's another question that would be interesting to sort of dig into. >> Yeah. So, you guys, uh, and and and again, if you're on LinkedIn, please send me and Nate connection requests so that we can connect to you. Um, I put out a LinkedIn I have a newsletter for CXO Talk, but I do a new LinkedIn newsletter as well, and I announce these there when we create these events. So, if you subscribe to that, then if you're on LinkedIn, you'll >> get the announcement. Any final thoughts, folks in the audience? >> Well, people are saying we should vibe code an app for chat, which I don't entirely disagree with. I barely have enough time to breathe, let alone vibe, code, an app or anything. >> Maybe I'll have a look at it at some point. Um, >> that would be that would be fun. I think that one of the things that uh that I want to call out is also from that people are saying that they really appreciate that we're picking out questions over on Substack and so that like the the responsiveness of the audience is something that that they're appreciating. Um, I think one that I think would be interesting is, and this is a little bit philosophical, but maybe it's a good like final two or three minutes. Um, when I ask the same question of Claude versus OpenAI versus Gemini versus whatever else, I get different answers. What have I how has what I've done in the tool in the past perhaps changed that? Or maybe if it's someone who's new, why do those answers tend to differ? Um, I think this gets at like part of the challenge in teaching people to use these tools. And maybe we can kick it around a little bit for the last couple minutes. >> I'm sorry. I was looking at the See, this is ask ask that one. Oh, the differences. See, this is again, >> yeah, like between different models, you ask the same question, you get different answers like what what's you know why is that the case like why if I ask OpenAI? Why is it different than Claude? >> Why if you ask Open AI the same question uh five minutes later, you get a different answer. I think that's so true. >> I think different models have different strengths. Uh rather than ask why that is the case, personally, I ask the question of what model is most useful for a particular reason. >> So for example, if I want to do a research report, I am going to use I I've been subscribing still on the fence to uh chat GBT Pro. So it's 200 bucks a month. sprung for it. And if I want something in depth, I'm going to do deep research. On the other hand, if I want better quality writing, I'm going to go to Claude and I I use the Claude uh console, you know, so it's not filtered by, you know, perplexity or what have you. >> I think that's a really good call out um that you look at what the strengths of the model are. Um, and I think the other one is recognizing that iteriveness is just part of the way we create things now. And so if you don't like a particular response, iterate with it and see where you go. Uh because you are going to get different answers. And getting used to the idea that it's not one response, it can be multiple is I I find it's a big aha moment for people. You know, I was helping somebody uh just just yesterday who wanted to use AI and LLM to help write some social media posts. And this person writes the posts themselves. And I and I was saying, \"Hey, you should see if it can help you in some way.\" >> And we got stuck in this prompting nightmare. Mhm. >> And if you don't know how to prompt in the age of AI, you're basically illiterate. >> How's that? >> I I it's it it's it sounds like it's a spicy take, but it doesn't sound spicy to me, Mike. It feels like it's one of the things that if you're a kid coming up, like our schooling systems need to teach prompting. It it needs to be in the ABCs. >> Uh yeah. I mean, just like calculators, there there was a time when kids in school didn't use calculators. >> Yeah. >> I'm old enough that I remember in uh junior high school, maybe maybe it was elementary school, uh the introduction of calculators. >> I I I remember having to take tests and being told like, you may not use a calculator for this test. Use your pencil, right? Like work through it. Um, >> yeah. >> Yeah, >> maybe it was in high school. Whenever whenever it was. Um, so that'll happen with AI as well. >> Yep. >> So, we're we're out of time. I'm really hoping. Greg Walter says he used an abacus and a slide rule. >> And Ryan Kyle says he was probably from middle school onward. Uh so so again you folks in the audience can you send send us a note send Nate a note send me notes tell us what you want to hear give us advice on how to handle this chat situation connect on LinkedIn and tell us what will make this useful for you and fun and what we should do and by the way you can solve all of our other problems too. Well, and I'm gonna I'm gonna say one thing for the Substack audience. This is your chance to take a topic that we have talked about on this live stream and come back to me in the Substack chat and say, \"Nate, I want you to write something about this or do a video on this.\" Um, let me know. I would love to hear your suggestions. >> Sounds good. So, are we done? This is As you can see, folks, as you can see, this is this this ain't scripted. >> I think we're done for today. I have to run. So, let's call it good. >> Okay. So, uh till next time and we'll put the word out. >> We'll we'll we'll figure out a date. >> It'll get smoother every time. How about that? We're going to get a little bit better every time. >> That works for me. Nate, it's always a pleasure. I can't wait to do this again. >> Yep. Me, too. All right. Talk soon, sir. >> Bye, everybody. Thank you so much for listening. We'll talk with you soon. Have a good one. Bye-bye.",
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"Tech Talk: AI with Nate and Mike \u2014 Nano Banana, MIT Report, and AI Adoption\",\n  \"tags\": [\"generative-ai\", \"ai-image-editing\", \"prompt-engineering\", \"ai-adoption\", \"ai-failures\"],\n  \"summary\": \"Discussion on Nano Banana's image editing capabilities, iterative prompting, and embedding AI into business processes, with insights from the MIT AI failures report.\"\n}",
      "generated_at": "2025-11-09T22:27:28.091769",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "processing_history": []
}