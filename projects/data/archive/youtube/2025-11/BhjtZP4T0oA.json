{
  "video_id": "BhjtZP4T0oA",
  "url": "https://www.youtube.com/watch?v=BhjtZP4T0oA",
  "fetched_at": "2025-11-10T00:35:49.841136",
  "source": "youtube-transcript-api",
  "raw_transcript": "today I want to talk very briefly about the memory problem for large language models I believe this is going to be one of the dominant issues we need to discuss in 2025 at the end of the day large language models are becoming very intelligent but they still have atrociously bad memory 100,000 200,000 token memory it's like having a PhD in your pocket that forgets a conversation from 10 minutes ago it's not working well and the problem is if you do the math on the cost of memory there is no easy solve given our current solution architectures I I did the math I wrote a substack on this it would take over half a trillion dollars to solve this problem just at the current daily active user count for chat GPT which is roughly 125 million and that's growing all the time the problem is getting worse all the time and that's assuming you don't want human level memory which lasts years you would be happy with long-term memory that's several months and I don't even know if we would be happy with that to be honest with you but that would still be vastly better than just a few minutes of memory which is really what we have today if the chat is going well I burn through chats with Claude in about 10 minutes claud's great well Claude lasts and then we're done and I think that one of the things that we need to ask ourselves is if you have this much intelligence but it has this much short-term memory what kinds of problems are useful to solve with that kind of intelligence and what kinds of problems are we inherently limited from solving because memory is an issue even 01 Pro 200,000 tokens you have very limited memory now people are doing incredible things with it so I'm not saying that you can't find really cool problems to solve I absolutely you can but it is making me wonder is memory the next breakthrough that we need to be looking for and if it is I don't see anything on the horizon that help helps us to solve that right now and I think that's probably worth talking about a lot more than we currently do especially if we're using llms all the time and they have very short-term memories is that going to affect the way we remember things does that change the and shape the way we remember things there are anecdotal stories coming out now of the way people are changing their vocabulary changing their thinking because of the way they interact with llms especially early on at formative stages in education if that continues and we are used to working with these thinking partners that are very smart but have very very limited short-term memory like it has read everything in the world but it cannot remember your conversation from 20 minutes ago does that shape the way we remember things too maybe for good maybe for ill maybe we're the ones that have to get better at remembering because our thinking partner can't I don't know but to me it's one of the most interesting Dynamics in large language models right now and I think it deserves to be talked about about more than it is so I wrote a substack on that if you're interested um otherwise enjoy the YouTube",
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"memory problem for large language models\",\n  \"tags\": [\"memory-constraints\", \"large-language-models\", \"short-term-memory\", \"long-term-memory\"],\n  \"summary\": \"Analysis of the memory limitations of large language models, the high costs of scaling memory, and potential impacts on education and cognition.\"\n}",
      "generated_at": "2025-11-10T00:36:03.752440",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "processing_history": [],
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-10T00:35:49.841136",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  }
}