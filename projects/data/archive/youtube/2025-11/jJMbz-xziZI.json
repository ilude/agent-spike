{
  "video_id": "jJMbz-xziZI",
  "url": "https://www.youtube.com/watch?v=jJMbz-xziZI&pp=ygU9aHR0cHM6Ly93d3cuYW50aHJvcGljLmNvbS9lbmdpbmVlcmluZy9jb2RlLWV4ZWN1dGlvbi13aXRoLW1jcA%3D%3D",
  "fetched_at": "2025-11-09T19:23:28.114205",
  "source": "youtube-transcript-api",
  "raw_transcript": "Enthropic just published something about MCP that every single person building AI agents needs to hear. If you have been using model context protocol in production, you might have noticed your agents hallucinating more than they should. Your token costs spiraling out of control and workflows randomly breaking when they hit context limits. And here is what nobody is telling you. It's not because MCP is broken. It's because the way that everyone has been using MCP is fundamentally inefficient. Now, I'm talking about burning through 98% more tokens than you need to, and your agents are getting confused because their context is cluttered with hundreds of tool definitions they're never going to use. So, if you're building AI systems for clients or running these systems in production for your own business, this changes everything about what is actually reliable and profitable. Now, I have been building AI automations for businesses for 2 years now, and I've hit these exact problems on previous projects. context windows are maxing out, costs exploding to where the economics just do not work, and agents making just stupid mistakes because there's just too much noise. Now, what Enthropic just laid out, it isn't some new tool that you have to learn. It's a completely different way to think about your agents and how they interact with MCP servers and it solves all of these problems. So, let me just break down what this actually means for you. So, most of you by now know what MCP is. Model context protocol. This became the industry standard for connecting AI agents to any external tools and data sources. Now the genius of MCP is that you just build a server once and any agent can connect to it. So we have seen thousands of these servers get built over the last years. The whole ecosystem has exploded because finally we had a universal way to connect agents to anything like Gmail or Slack, databases, CRM, whatever it may be. So you build the MCP server and you're done. But here's the problem that hits you the moment that you start building anything complex for real clients and try to run it in production. Everything dumps into your agents context window and it becomes a complete mess. So let me give you an example from some of our own work. So we recently built a system for a legal client. Now, they needed their agent to just search case law and pull some of their documents from their document management system, check their internal management software and update some of their calendars, send client emails, and pretty much just log everything into their CRM. So, they had about six different systems, right? So, sounds pretty reasonable. Now, each MCP server, it had maybe 15 to 20 different tools that you can use. So, now we're looking at over a hundred different functions. And here was what was so detrimental is that even though the agent only uses maybe three or four of those tools for any specific task, all 100 tool definitions are logged into the context window from the very start. So every single function has its description. It has its required parameters, optional parameters, return types, examples. So what we're talking about is tens of thousands of tokens just sitting there before the agent even reads like what the user wants it to do. So immediately your costs are higher than they need to be. Your response times, they're going to be slower because the agent has to process all of this noise. And here's what I had noticed that really matters for production systems is that the agent, it makes more mistakes when there's just too much clutter in the context. So it gets confused about which tool to actually use. It hallucinates parameters that just do not exist. It tries to call tools in ways that do not make sense. And when you have a hundred tool definitions competing for attention, the agents accuracy, it just drops. And that's a massive problem when you're running this for any real clients who expect it to work reliably. But that's just the first problem. The second problem is even more brutal for your economics. So let's say your agent needs to grab a deposition transcript from the document system. That transcript, it might be 40,000 tokens. And with the traditional way of using MCP, that entire transcript, it loads into the agent's context. And then the agent needs to summarize key points and update the case file in the CRM. So now that same 40,000 token transcript, it's getting processed again as the agent just writes it into the next system. So you're literally paying for that same data to flow through your context multiple different times. And if you're chaining together several operations across different systems, well, you're just going to hit context window limits or just completely blow through your API budget before you even finish the workflow. Now, I've had projects where the token cost made the whole thing economically questionable. And that is before we even talk about the reliability issues from context limits before hitting any midworkflow. So, here is where code execution changes the entire game. And this is really just about understanding how AI models actually work best. So instead of presenting your MCP tools as function calls that the agent makes directly, you just present them as a file system that the agent can explore. So each MCP server it just becomes a folder and each tool within that server is a TypeScript file and the agent it can just search through the structure, find exactly what it needs and then write code to use those specific tools. And here's why this approach is so much more powerful and reliable. is that AI models, they're fundamentally trained on massive amounts of code during their pre-training phase. So, we're talking about millions and millions of lines of code. So, tool calling, that is something they learn during post-training with way less compute behind it. So, when you let the agent write code to actually interact with your MCP servers, you're just leaning into what the model is actually exceptional at instead of just forcing it into a more rigid structure that it is less naturally good at. So let me just walk you through how the workflow actually changes between these two approaches so that you can see the entire difference between these. So with the traditional approach, all of your tool definitions, they just load into the context window right from the start. And then the user, they ask for something. So that agent has to sort through all of that noise just to figure out which tools are even relevant to its goal. And then it calls tool A and it gets back maybe 40,000 tokens of data. That all goes into the context and then it needs to call tool B using some of that data. So another 30,000 tokens just flows through and your context window it's filling up fast and the agent is struggling to keep track of everything and compute the tasks without making any sort of mistakes. Now if you compare that to the code execution approach on the other hand the agent it has access to this organized file structure of your MCP servers and all of its respective tools. So a user they ask for something and then the agent it searches for the right tool folder and it finds what it actually needs and then it loads only that specific tool definition not every single tool from every server. So already it's way less noise and way less confusion and then it just writes codes to call that tool. So here's the big piece about this though is the results they stay in a sandbox variable just outside of the agent's context. So the agent can then write more code to filter that data, transform it and then extract, you know, just what actually matters and only the final processed result, maybe 500 tokens instead of 40,000 goes back into the agent context window. Now the agent, it never gets overwhelmed with massive amounts of data it does not need. So the difference it's absolutely massive for both reliability and also cost, which obviously is very important. So, the legal transcript example that I mentioned, instead of 40,000 tokens flowing through the contacts twice, you're talking about maybe 2,000 tokens total for the whole operation. And the agent, it does all the heavy data processing work in the sandbox environment where it can actually filter and transform and extract only what is relevant. And then it is going to bring back just the key information that it needs. And because the context stays clean, the agent makes way fewer mistakes. So think about it like this. The traditional MCP approach, it is like being forced to carry every single tool in your toolbox with you everywhere you go and having to read all the instruction manuals out loud before you can even use just one tool. Now, of course, you're going to get confused and you're going to pick the wrong tool sometimes. That's just going to happen. So code execution, it's just like having a workshop where you can walk over to the right section, grab exactly what you need, work on your project there, and only show, you know, people the finished results, so you're not cluttering up your workspace with everything all at once, so you can actually focus and do the work correctly. Now, really quick, I just wanted to mention that if you want to learn more about implementing advanced systems just like this and actually building a real profitable business around AI automation, join my school community. We have got over 15,000 members and they're actively sharing what is working right now in the real world and we give out all of our free resources with monthly competitions and so much more. Link is down below in the description. Again, it's completely free. All right, so beyond just the token savings and reliability improvements, let me tell you why this actually matters for your actual business or your client's businesses. So, first off, the economics of what you can build, it completely changes. So, I have been doing discovery calls and audits with potential clients constantly and when I sit down and actually calculate what it would cost to run the automation using traditional MCP, sometimes the numbers just do not make sense. So, a customer support agent that's handling 200 tickets per day and each ticket it requires pulling data from multiple different systems. Well, you could be looking at $400 to $600 per day in API cost alone with the traditional approach. But with code execution, I can get that down to maybe $40 to $60 per day. Now suddenly the return on investment, it actually makes sense and the client can afford to run this at scale without the cost, you know, spiraling out of control. And more importantly, the system it actually works reliably because the agent it is not getting confused by any of the cluttered context associated with it. Second is that you can finally build things that were literally impossible before because of cost constraints and reliability issues. So, here is a use case that I have wanted to build with MCP alone for months, but I just couldn't make the economics work or just trust it to at least run reliably. Now, this was just an agent for an e-commerce brand that monitors inventory levels across Shopify, Amazon, their third party logistics warehouse system, and also some of their accounting software. And then what it would do is identify the discrepancies between these systems and then flag any potential stockouts before they happen and just suggest specific reorder quantities for each SKU. So with traditional MCP each data pull from these systems, it was very large. So comparing data across multiple different sources, this obviously means multiple passes through your context window. Now the token cost, this would just be completely insane and unsustainable. And with all of that data flowing through context, the chances of the agent making a mistake or hallucinating something, it increases dramatically. But with code execution, the agent, it's just going to pull everything into the sandbox and then it's going to write a comparison script and then run all of the calculations there. And it only returns something like SKU123. It's 47 units short in Amazon versus your 3PL system. Here is the recommended reorder action. So maybe a,000 tokens instead of 150,000. And because the context it stays clean, it's way more reliable. So that completely changes what is feasible to build and actually deploy in production. Third is privacy. It becomes a massive selling point instead of a deal breakaker. Now I have personally lost deals because enterprise clients absolutely will not allow their customer data to touch enthropic or open AAI servers. So healthcare companies, financial services, law firms, they all have strict compliance requirements like HIPPA for example. So with code execution sensitive data, it never actually goes to the model. It just stays in the sandbox environment. So you can even set up automatic tokenization where the model sees something like customer emil1 instead of the actual email address. Now, the real data, it just flows from system A to system B, but the AI never actually reads the sensitive parts. So, for regulated industries, this just unlocks deals that you literally could not touch before because of compliance issues. And fourth, this one's honestly kind of wild, is the agent can actually learn and improve over time. So, because the agent is working in a file system, it can save useful code that it writes. So, let's say it just figures out a really clever way to parse a specific document format. So it can save that as a reusable function and use it just again later. So over time your agent it builds up its own library of solutions. So it's not starting from scratch every single time. And this is very similar to how claude skills works. So the agent literally evolves its own capabilities. Now, before we get into the practical implications, I just wanted to mention that if you are a business owner looking for help implementing this or just to transform your business with AI to ultimately increase your bottom line and save your team hours every single week and actually grow and get an edge over your competitors, then you can click on the link in the description to schedule in a call with our team just to learn more. Completely free. We've worked with more than 30 different companies now, either driving significant amounts of leverage or just increasing their bottom line and allowing them to scale efficiently. So, if you're at all interested in growing your company, then again, the link is down below in the description. But in any case, let's jump back into things. So, let's get really practical here and talk about what this means for different types of businesses and use cases. So, if you are running an agency or just doing consulting work, your entire pricing model, it just got way more flexible. So, I have been hesitant to propose certain automation projects to clients just because, you know, when I would run the numbers, the token costs made the ROI really questionable. And honestly, I wasn't confident that the system would be reliable enough for any production use with any traditional MCP methods. So, if a client needs an agent that processes 500 documents per day. So, with traditional MCP, I mean, you're just staring down massive ongoing API bills that eat into everyone's margins, plus the risk of the agent making mistakes because of context overload. So, code execution, it fundamentally changes that calculation. Now I can confidently promise much more ambitious automation projects just because I know that the cost will scale in a reasonable way and the system will actually work reliably and that changes what you can actually sell to your clients and how you price your services. All right. Now let me be completely honest about the downsides here because they definitely exist and you need to know about them. So first it's way less reliable in certain ways. So traditional MCP tool calling it's rigid but it's very predictable. So the agent, it calls a specific function with specific parameters. So it either works or it just throws away an error. Pretty straightforward code execution. This just means that the agent has to write systematically correct code every single time that it needs to do something. So that opens the doors to either syntax errors or logic bugs, edge cases that the agent just didn't account for. So, I've personally seen agents write code that works perfectly 19 times in a row and then just completely fails the 20th attempt because the data came back in a slightly different format than expected. So, you do need much better testing, error handling, and also monitoring system. It's also not as bulletproof as simple tool calling can be. So, second, the infrastructure overhead. It's very real. So you absolutely cannot just deploy this to a simple serverless function and then call it done because you need a proper sandbox environment that is secure that is isolated from your other systems and has strict limits on what code can actually do and what resources that it can consume and that is real DevOps work. So for a simple chatbot that does one or two different things that level of infrastructure it's just completely overkill. But for production systems that are handling actual business processes with real consequences, it's pretty much necessary. So it's just not trivial to set up and maintain. So when should you actually use each approach? Well, let me give you my framework for thinking about this. So traditional MCP, it still makes total sense for any simple use cases that only needs one or two or maybe three tool calls. You know, where it's low volume operations where token costs do not really matter in the grand scheme of things. So quick prototypes and MVPs where you need to move fast and prove the concept in situations where absolute reliability it matters more than cost optimization. So that is where traditional is going to be the better bet. Now code execution. This makes way more sense for any complex workflows that involve any heavy data processing or transformation or high volume operations where costs can compound very quickly and really matter in enterprise clients who have strict privacy and compliance requirements or just workflows that keep hitting context window limits with the traditional approach. So these are just situations where you need the agent to handle any messy unpredictable data that does not always come in the same format and also production systems where you need the agent to actually be reliable and not hallucinate or make mistakes because of context overload. So here is my personal rule of thumb that I use when evaluating all my projects. If you can actually build the entire thing with under 10 tool calls and the data being passed around is relatively small, just stick with traditional MCP. Keep it simple. But if you're chaining together complex operations or just processing large amounts of data or if you need it to be bulletproof in production code execution it's absolutely worth the upfront investment in infrastructure and setup time. So that being said if you are building AI solutions for your clients or for your own business just understanding this right now it already gives you a massive head start over your competition. So those complex workflows that other agencies are turning down just because they can't make the economics work or don't believe in it or guarantee reliability, you can build those profitably now. Or those enterprise deals that just keep dying because of privacy and compliance concerns. Well, you can close those deals. But here's what I really want you to focus on is just stop obsessing over which tool or platform is the best. Code execution, it's just one approach. Traditional MCP, it's another one. So the real question is never which one is better in abstract. The question is which one actually solves your specific client's problem most efficiently given their constraints and their requirements. So that's just the thinking that separates people who make real money from people who just collect tools. So with that being said, go ahead and investigate for yourself. Let me know what you guys think down below in the comments. Go ahead and read the article. I put it down below in the description. But again, if you're a business owner looking to implement things like this for your own business or looking to any other AI solutions to implement to ultimately save time and increase your bottom line, then check out the link down below in the description. You can book in a call with my team. And with that being said, thank you guys for watching. and I'll see you in the next video.",
  "youtube_metadata": {
    "source": "exported_from_qdrant",
    "original_collection": "cached_content"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"Rearchitecting AI Agents: Code Execution over MCP to Cut Costs and Improve Reliability\",\n  \"tags\": [\"ai-agents\", \"model-context-protocol\", \"code-execution\", \"token-economics\", \"sandbox-environment\"],\n  \"summary\": \"Explains replacing traditional MCP tool-calling with a sandboxed code-execution approach to reduce token costs, improve reliability, and enable scalable AI automation for business use cases.\"\n}",
      "generated_at": "2025-11-09T19:23:28.125037",
      "model": "unknown",
      "cost_usd": null,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "processing_history": [
    {
      "version": "v1_full_embed",
      "processed_at": "2025-11-09T19:23:28.135014",
      "collection_name": "cached_content",
      "notes": "Migrated from Qdrant"
    }
  ]
}