{
  "video_id": "1piFEKA9XL0",
  "url": "https://www.youtube.com/watch?v=1piFEKA9XL0",
  "fetched_at": "2025-11-14T21:03:07.989265",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "single_import",
    "imported_at": "2025-11-14T21:03:07.989176",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": false
    },
    "recommendation_weight": 1.0
  },
  "raw_transcript": "Oh boy, it's time for another MCP video. If you're not familiar with my takes on MCP, it's my favorite example of AI being a bubble. I know way more companies building observability tools for MCP stuff than I know companies actually making useful stuff with MCP. When you see everyone building the tool layer for a new thing and nobody building the product around the new thing, you know that new thing is probably crap. I still remember back in the day when web 3 was blowing up that I knew about six companies doing OOTH for web 3 and one single company that could potentially benefit from that existing. Yet here we are with MCP and thankfully people are waking up to the fact that it kind of sucks. Not that the spec sucks, which it does, or that the implementations suck, which they do, but the models suck at using it too. I covered this before with code mode from Cloudflare, which was them realizing that MCP is bad and solving it by letting agents write code to call these things instead of just bundling it all in as a giant pile of context that makes everything run like [\u00a0__\u00a0] And it seems like our friends over at Enthropic, you know, the people who made this spec and curse us all with it, are waking up to the same thing because they just wrote a new article, code execution with MCP, building more efficient agents. And depending on how you choose to read this article, you can see it as them admitting that MCP is not a good protocol because MCP requires you to add a bunch of [\u00a0__\u00a0] context that makes the models dumber and worse. Models do not get smarter when you give them more tools. They get smarter when you give them a small subset of really good tools. An MCP does not encourage that way of thinking. MCP encourages you to add 500 plus tools to a model and then nothing [\u00a0__\u00a0] works anymore. Do you know what does work really well though? Today's sponsor. Writing code has never been easier, but actually hosting and deploying it's never been more annoying, especially if you're hopping between clouds constantly, and you're on a stack that isn't really well supported on these new fancy, cool, modern tools. Well, there's somebody here to help you out. Savala, these guys get hosting. I could tell you what I think, but I'd rather just quote their own customers. With Savala, you no longer need to worry about uptime or managing infrastructure. It simply works. If I were to start a new project today, I would choose Savala from the very beginning. So, that's got to be really expensive, right? Uh, no. It's 78% cheaper for them to use Savala than the stack that they were on previously. And it makes sense once you start playing with it. They just made it so easy to spin up and take down servers. Here's a bunch of servers that I have deployed right now on Savala. It's so easy to spin up new things, link them to GitHub, create the pipelines for deployment, and even spinning up databases is super easy. I click here, I click create DB, peg whatever I want to use, and now it's deployed in a data center directly attached to all of my other stuff. When you look at the applications, things get even cooler. Here's my sub nerds production deployment that's linked to a GitHub repo. I have Cloudflare in front being used for DOS protection as almost everyone should. But if I want to take more advantage of Cloudflare, like using it as a CDN or for caching data on the edge, I hit settings and I turn it on. That is a single click toggle to enable a CDN for your static assets. I've never used any service that made it that easy to do this type of thing. If you're tired of stressing out about your deployments and you want $50 of credit for free, go get it now at soy.link. link/savala. I'm actually really excited to go into this because I secretly deep down do want something like MCP to work, but the current implementations just don't. I've yet to have one impress me with its capabilities. There are some that are really cool, but none that are actually useful for my experience. Let's see if they succeeded in making them useful here. Code execution with MCP. Building more efficient agents. Direct tool calls consume context for each definition and result. agents scale better by writing code to call tools instead. Here's how it works with MCP. There we go. There it is. Thank you, Enthropic, for admitting I was right the whole [\u00a0__\u00a0] time. It makes no sense to just clog up your system prompt with a bunch of [\u00a0__\u00a0] that probably isn't relevant for the majority of work you're doing. There just isn't enough data for the models to be trained to do that well. Do you know what they do well? Because there's a lot of examples. Write code. It's It's so funny to see this line in an official thing on the Anthropic blog. They're admitting that their spec doesn't work for the thing they build, which is AI models. Hilarious. Let's see how they actually implemented this cuz I am curious. The model context protocol is an open standard for connecting AI agents to external systems and making them dumber in the process. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. You know, if you were trying to solve this problem to make a generic solution for models connecting to things, you'd probably want to make sure it handles the things that you need with those connection layers. You know, like O. Did you know MCP has no concept of O at all? At all. Now, there's like 18 implementations of it because there's no way to do proper handshakes with MCP. Your best bets to go and hardcode a custom URL that has a signed like parameter in it that allows it to work. Actually, insane. I I I hate the standard. I really do. Ah, MCP provides a universal protocol that does a third of what you need. Developers implement MCP once in their agent and then five additional layers to make it work. And it unlocks an entire ecosystem of integrations. Since launching MCP in November of 2024, adoption has been rapid by people trying to sell you things, not people trying to make useful things. The community has built thousands of MCP servers. SDKs are available for all major programming languages and the industry has adopted MCP as the de facto standard for connecting agents to tools and data and also implemented a dozen standards for how to make the data safely accessible. Today, developers routinely build agents with access to hundreds or thousands of tools across dozens of MCP servers. However, as the number of connected tools grows, loading all tool definitions up front and passing intermediate results through the context window slows down agents and increases costs. It also makes them way dumber. Weird how you missed that one. In this blog, we'll explore how code execution can enable agents to interact with MCP servers more efficiently, handling more tools while using fewer tokens. Excessive token consumption from tools make agents less efficient and less effective. As MCB usage scales, there are two common patterns that can increase agent cost and latency. The first is tool definition overloading the context window and the second is intermediate tool results consuming additional tokens. So, first is the tool definition overload. Most MCP clients load all tool definitions upfront directly into context, exposing them to the model using a direct tool calling syntax. I'd say it's more than most, but this does also include all of anthropics builds. So yeah, these tool definitions might look like the following. Here's a tool definition. G Drive.get document. Description retrieves a document from Google Drive. parameters document ID which is a required string the ID of the document to retrieve as well as fields which is an optional string specific fields it should return this will return the document object with title body content metadata permissions etc or this Salesforce update record you have an object type and a record ID and data that you're adding and it returns the updated record object with confirmation cool tool descriptions occupy more or one thing I do want to call out here with the G drive document one this requires that you already I have a document ID. Where are you going to get that document ID? Is the user going to pass it? No. You probably have a G drive.find document tool that is an MCP definition that you use to search for the thing that you want to go find. And then once you find it, you have the ID that you then pass to G Drive.get document. And now you've done multiple back and forth runs for no [\u00a0__\u00a0] reason and made everything harder and slower and wasted a bunch of tokens because you have too many tools. Tool descriptions occupy more context window space, increasing response times and costs. In cases where agents are connected to thousands of tools, they'll need to process hundreds of thousands of tokens before reading a request. Yep. And then the second point, which is that intermediate tool results consume additional tokens. Again, like here we're getting the document then updating Salesforce. I want a funnier example for this. Instead of G drive.get document, how about we do G drive.find document content videos taxes. And this will return an array of documents that might have it. Document one, document two, document three, etc. So then you do a bunch more tool calls because you want to have this content. It's useful to you. So what you then do is you call G drive.get document for each of these and each of these is an additional message being sent to the model that is a whole separate request and each of these adds to the context. So if this one has a context window of let's say 20 tokens and this is an additional 20 then this request will be 40 because you have to include everything from before. This one will be 60. You have to include everything from before. This one will be 80 etc. To make this very clear every additional tool call is carrying all of the previous context. So every time a tool is being called the entire history is being re hit as input tokens. Insane. It's so much bloat. It uses so much context. It burns through so many tokens and so much money. And if you don't have caching set up properly for your inputs, you're just burning cash. It sucks. It's such a bad implementation. We need parallel tool calls. We need better tool design to prevent this. Or you can write code because if this was just writing code to go find the documents and then for each of them go do this thing and then return all of the results with a single tool call, that's a lot less [\u00a0__\u00a0] In Enthropic's own words, every intermediate result must pass through the model. In this example, the full call transcript flows through twice. For a 2-hour sales meeting, that could mean processing an additional 50,000 tokens. Even larger documents may exceed context window limits, which would break the flow entirely. With large documents or complex data structures, models may be more likely to make mistakes when copying data between tool calls. Here's their little diagram for it. So, the MC God, this diagram is kind of [\u00a0__\u00a0] So, the MCP client as a context window. We have the system prompt, the tool definitions, the user message. The user message goes to the model. The model responds with the assistant message and a tool call. It then goes and makes that tool call, gets a result, and now you have to send it back to the model to do the new message. But this is the part to look at here. This is how much data is being sent to the LLM. See, it starts with this data and now it has way more. Every additional tool call is stacking additional data that the LLM has to process, thereby slowing it down, making it dumber, and increasing your costs. The MCP client loads tool definitions into the model's context window and orchestrates a message loop where each tool call and result pass through the model between operations. Yep, you do a whole new message gen, a whole new run of the LM every time a tool call responds. It's great. Code execution with MCP improves context efficiency. With code execution environments becoming more common for agents, a solution is to present MCP servers as code APIs rather than direct tool calls. Wow. Turns out that writing code is more effective than making a [\u00a0__\u00a0] generic wrapping layer that doesn't have half the [\u00a0__\u00a0] you need. Who would have thought? The agent can then write code to interact with MCP servers. This approach addresses both challenges. Agents can load only the tools they need and process data in the execution environment before passing results back to the model. There are a number of ways to do this. One approach is to generate a file tree of all available tools from connected MCP servers. Here's an implementation that uses TypeScript. So here again, we have TypeScript files that have the different things we might want to do and have type definitions for all of these things. And the model can search through these the way it does with any code project to find the specific things that it needs to do the task it wants to do. Each tool call corresponds to a file. Like here we have callm MCP tool from client as the interface for getting document input, the interface for the response and then the function the Google drive to Salesforce example above becomes this code. So again to show that example because I [\u00a0__\u00a0] with it a bunch. The first tool call gets this document that has all of this content in it and then they update a record in Salesforce with the data that they got from this. Instead, it writes the code that imports the G drive client and the Salesforce client, defines the transcript as this thing that it awaited from the G drive.get document call, and then puts it in. This might be hard to read because for whatever reason, okay, I know the reasons. Anthropic's not very good at front-end code, so they don't have syntax highlighting in their blogs. Amusing. Regardless, you get the idea. This is just TypeScript code. I'd go as far as to say that like MCP as a spec is proof that letting Python people define this entire ecosystem is going to destroy the whole thing because it's so bad. MCP is such a Python spec. It's trying so hard to be simple and elegant that they forgot to put the meat in. TypeScript is a much much less compromised language in these senses. And that's why when Typescript devs tried adopting this, myself included, we wanted to put our head through the wall, and now that we've taken over and shown them that this would be better as code, they're listening. There's a reason why the examples are using TypeScript and not Python. The agent discovers tools by exploring the file system, listing the /servers directory to find all available servers like Google Drive and Salesforce, then reading the specific tool files that it needs like get document.ts and update record.ts PS to understand each tool's interface. This lets the agent load only the definitions it needs for the current task. This reduced the token usage from 150,000 tokens to 2,000 tokens. A time and cost savings of 98.7%. How the [\u00a0__\u00a0] can you pretend that MCP is the right standard when doing a shitty codegen solution instead saves you 99% of the wasted [\u00a0__\u00a0] That is so funny to me. The creators of MCP are sitting here and telling us that writing [\u00a0__\u00a0] TypeScript code is 99% more effective than using their spec as they wrote it. This is so amusing to me. Oh, it turns out that they even linked the Cloudflare thing. I guess that's what woke them up to this realization. Cloudflare published similar findings, referring to code execution of MCP as code mode. The core insights are the same. LMS are adept at writing code and developers should take advantage of this strength to build agents that interact with MCP servers more efficiently. So what are the benefits of code execution with MCP? Code execution with MCP enables agents to use context more efficiently by loading tools on demand, filtering data before it reaches the model and executing complex logic in single steps. There are also security and state management benefits to this approach. For example, you don't have to dump the entire document into the LLM and then send it over to our friends at Anthropic or Google or AWS, whoever is hosting your model because all of that's just happening inside of the sandbox that the code is executing in. That's so much better than loading the entire document. Remember here where we were grabbing this document and then sending it to the Salesforce thing. That means that the entire content of that Google Drive doc has to be loaded into context and you have to hope that the model doesn't typo anything as it forwards that over to Salesforce. Or you can just write code. You can just literally in like five lines grab the variable, have it in memory in your sandbox instead of in context in the model that could be hosted anywhere and then update whatever you want to update. Now the content of this document never becomes part of the context. It's never seen by the model because it's not touching any of that because the model doesn't need to know what's in the doc. It needs to know what to do with it. That's the whole [\u00a0__\u00a0] point. There are other benefits too like progressive disclosure. Models are great at navigating file systems. Presenting tools as code on a file system allows for models to read tool definitions on demand rather than reading them all up front because crazy MCP had no concept of progressive discovery. There was no way to give more context via MCP when it was necessary. I saw people doing crazy [\u00a0__\u00a0] like having a separate model that would pick which different agent to use and different subsets of tools depending on what task was being completed for those sub agents. entire orchestration layers of [\u00a0__\u00a0] in order to try and make the spec usable. Turns out writing code is easier. Crazy. You could add a search tools tool to the server to find relevant definitions. For example, when working with the hypothetical Salesforce server used above, the agent searches for Salesforce and loads only those tools that it needs for the current task. Including a detail level parameter in the search tools tool that allows the agent to select the level of detail required like name only name and description or the full definition of schemas also helps the agent conserve context and find tools efficiently. I'm going to pick on my friends at Trey really quick here. I think they might have fixed this since I don't know for sure and don't feel like checking. But when I was playing with it and I noticed the quality of outputs not being great, I decided to analyze what tools their agents have access to. They have a top level agent in solo that determines what's going to happen and it calls separate agents for coding in building. There's a solo coding agent and a solo dev builder agent and these have their own sets of tools and the tools they have access to are interesting. In here we can see some of them. Do you see how many of these there are going to look for all of these dashes. There are 23 tools available for the solo coding environment agent. This includes seven separate tools for doing file management stuff, three for running commands, and my personal favorite, three for Superbase. I don't use Superbase. I don't even have an account. I've never built anything with Superbase, but when I use Trey, every single request I send has this context included for things I don't even use. Ah, this is awful. How is this where we ended up and we assumed everything was okay? This is when I complain about AI bros not building software or understanding how the software world works. This is what I'm talking about. All of these things are obviously wrong and dumb. You just have to look at it to realize. And thankfully enough engineers are now using AI tools that they're complaining about these things like a Cloudflare. Like what do you think Cloudflare is better at, LLMs or software development? If you use their dashboard a lot, you might be confused as to which it is because the dashboard's rough. But if you've used Cloudflare's infrastructure, you know they're good at writing code. You know they're good at infrastructure and engineering. They had to make this a very popular thing and idea. And I had to make videos about those things cuz I have strong opinions to get Anthropic to start acknowledging these facts. Now, let's talk about context efficiency with tools. When working with large data sets, agents can filter and transform results in code before returning them. Like if you fetch a 10,000 row spreadsheet, so give.get sheet returns 10,000 rows. If those were all in context, good luck. Have fun. or you can process them in code before they get to the model. And pending orders will be any row where the status is pending. And now we have all of these. Cool. Can you tell these guys are really good engineers? I can't remember the last time I saw anyone selecting from an object in JavaScript with array syntax. The Python has rotted their brains at anthropic. Anyways, the agent only has to see five rows with this example instead of the 10,000 it would before. Similar patterns work for aggregations, joins across multiple data sources or extracting specific fields all without bloating the context window. Like imagine you have data in two places in Google Drive and in Salesforce and you want to find all the people that exist in both. If you ask the model to do that with tools, it'll fetch all of the data from Google Drive. It'll fetch all of the data from Salesforce and then put that all in context and a needle in a hay stack its way into an incorrect answer. Or it could write code that does that. Match the IDs and return everything that matches. Duh, that's so much better. Let found equals false. While not found, const messages equals await. Slack.get channel history. Found equals messages summ.ext.includes deployment complete. If not found, await new promise set timeout R5000. Very good code. That doesn't scare me at all. This approach is more efficient than alternating between MCP tool calls and sleep commands throughout the agent loop. Additionally, being able to write out a conditional tree that gets executed also saves on time to first token latency. Rather than having to wait for a model to evaluate an if statement, the agent can let the code execution environment do this. Oh yeah, crazy. As slow as JavaScript is, it's actually faster than letting an LLM go through a 100,000 tokens and hopefully guess the right answer. and also crazy. I know that this doesn't mean anything to anyone. Code is deterministic. So when it writes the code, it's not going to 50% of the time hallucinate. It's just going to write the code and the code will just do the thing. The more tokens you generate, the more tokens you have in context, the more hallucinations you're going to be dealing with. As weird as JavaScript is, and as much hallucination has had to occur for it to be designed in the first place, is a relatively consistent language when it comes to executing things. As long as you're not doing weird string manip stuff. Yeah. And then we have the privacy side. I touched on this a little earlier. When agents use code execution with MCP, intermediate results stay in the execution environment by default. This way, the agent only sees what you explicitly log or return. Meaning data you don't wish to share with the model can flow through your workflows without ever entering the model's context. For even more sensitive workloads, the agent harness can tokenize sensitive data automatically. For example, imagine you need to import customer contact data from a spreadsheet into Salesforce. The agent writes code like this where we have the Salesforce update record call for all of these. This is very good code that totally isn't blocking and going to be really slow. The quality of the TypeScript in this article is almost as funny as the fact that it's not syntax highlighted. You know what? Uh quiz for the watcher. Why is this code slower than it needs to be? I'm not going to answer. You should know by now. The MCP client intercepts the data and tokenizes PII before it reaches the model. So here when the data is returned, we will offcate all of the data. We'll just change it to like email one, phone one, name one, email two, phone two, name two. That way we could still identify them and pass things between the two, but none of the data ever has to make it to the company that's hosting the model. Duh. So much better. When you think about it, if you're using a tool like in Trey here, the get tables tool, that one's not giving you PII because it's data about what tables exist in their definitions. But if there was a get rows tool here that you just gave the model access to all of the data in your database and anything in there could hypothetically be included in context and therefore sent to Anthropic or OpenAI or whoever else you're hosting from. This is the easiest way to not have to do that. When the data is shared in another MCP tool call, it can be untokenized via a lookup in the MCP client. The real email addresses, phone numbers, and names flow from Google Sheets to Salesforce, but never through the model. That will prevent the agent from accidentally logging or processing sensitive data. You can use this to define deterministic security rules, choosing where data can flow to and from. Great. Also, state persistence. This is another one of those really annoying things with MCP state. LMS have no state. Databases are just state. Balancing out the gap there requires a lot of [\u00a0__\u00a0] With code, the state stays within the code. As long as it's in memory, it hasn't changed. And if you want to stay there for the future, you can write a file and now it's there and you don't have to worry about it being in context. So here we write this CSV data from leads that we got from Salesforce. We write this to a file and now later on we can grab that. Great. Agents can also persist their own code as reusable functions. Once an agent develops working code for a task, it can save the implementation for future use. So if we want to be able to save a sheet as a CSV, we can write this function, store it, and now have it as a skill we can call whenever. This ties in closely to the concept of skills, folders of reusable instructions, scripts, and resources for models to improve performance on specialized tasks. Adding a skill.mmd file to these save functions creates a structured skill that models can reference and use over time. And this allows your agent to build a toolbox of higher level capabilities evolving the scaffolding that it needs to work most effectively. Is this not reinventing MCP again? Let me get this straight. So let's say we have this API. We could tell the model about this API, but then we have to do that for every single thing. So we want to standardize this. We need a standard. So we create an MCP tool that includes all of the definitions and all of the other things that are needed to call this endpoint. But then we realize, oh no, we have too many tools. So we do the very obvious thing of changing this into an SDK interface sales course. This is what we're talking about here, the code stuff. And then now that we've done this, this code is useful. We should save it. And now we have this skill get Salesforce user data. And then these should be documented. And we get our skill.md file. How to use get salesforce user data. And then we end up roughly where we started. And I'm sure this loop won't continue indefinitely as we reinvent the same 15 things over and over and over again in the AI world. That's definitely not going to happen. This is the real agentic loop everybody's talking about. I'm going to go insane. Yeah, I was actually liking this until this point. Note that code execution introduces own complexity. Running agent generated code requires a secure environment for execution with appropriate sandboxing, resource limits, and monitoring. I don't know if Daytona is a sponsor for this video or not. We determine that later, but Daytona is the only sane way to do this that I know of. These guys have made deploying these things so much easier. You want a cheap way to safely run AI generated code, just use Daytona. They're not even paying me to say this. They might have paid me earlier. They're not for this. I like these guys. They're awesome to work with. They're genuinely really fun and get this side really well. I'd go as far as considering this a solved problem. At least it's a hell of a lot more solved than MCP is. These infrastructure requirements add operational overhead and security considerations that direct tool calls avoid because direct tool calls at MCP don't have security because there's no off. The benefits of code execution are reduced token cost, lower latency, and improved tool composition. Should be weighed against those implementation costs. No, this is [\u00a0__\u00a0] [\u00a0__\u00a0] This is absolute [\u00a0__\u00a0] [\u00a0__\u00a0] Every implementation of MCP have seen that can do anything is way more insecure than a basic [\u00a0__\u00a0] sandbox with some environment variables. This is delusion. God, I this article was really good until this line. From here down, it fell apart. MCP provides a foundational protocol. MCP provides a foundational protocol for agents to connect to many tools and systems. However, once too many servers are connected, tool definitions and results can consume excessive tokens, reducing agent efficiency. Although many of these problems here feel novel, context management, tool composition, and state persistence, they have known solutions from software engineering. Codexian applies these established patterns to agents, letting them use familiar programming constructs to interact with MCP servers more efficiently. If you implement this approach, we encourage you to share your findings with the MCP community. We need more engineers in high places. This is what happens when we let these LLM people make the things that we have to use as devs. Devs should be defining what devs use. And if you don't let them do that, then you'll end up realizing they were right all along. Whenever somebody tells you AI is going to replace developers, just like them this is all the proof I need that we are good. This is what happens when you let LLMs and more importantly you let LLM people designer APIs we get something so useless that we reinvent the whole wheel multiple times in the process and I'm going to continue to not really use MCP. I hope this helps you understand why. Let me know what you guys think. Let me know how you're executing your MCP tools. And until next time, peace nerds.",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "metadata",
      "output_value": "{\n  \"title\": \"MCP critique: why code execution with MCP improves efficiency and reduces token cost\",\n  \"summary\": \"A critical look at the Model Context Protocol (MCP), arguing that code execution with MCP is more efficient than loading all tool definitions upfront. The video compares MCP to Cloudflare's code mode, discusses token leakage, context size, security, and provides examples of on-demand tool usage and TypeScript code to interact with MCP servers.\",\n  \"subject_matter\": [\n    \"Model Context Protocol (MCP)\",\n    \"code execution with MCP\",\n    \"tool definitions vs on-demand tool loading\",\n    \"token efficiency and context management\",\n    \"context window and tooling overhead\",\n    \"Cloudflare code mode\",\n    \"security and data privacy in MCP workflows\"\n  ],\n  \"entities\": {\n    \"named_things\": [\n      \"MCP (Model Context Protocol)\",\n      \"Anthropic\",\n      \"Enthropic\",\n      \"Cloudflare\",\n      \"Savala\",\n      \"GitHub\",\n      \"Google Drive (G Drive)\",\n      \"Salesforce\",\n      \"Daytona\",\n      \"Trey\",\n      \"Supabase\",\n      \"Supabase (Superbase)\"\n    ],\n    \"people\": [],\n    \"companies\": [\n      \"Anthropic\",\n      \"Cloudflare\",\n      \"Savala\",\n      \"Daytona\",\n      \"Trey\",\n      \"Supabase\",\n      \"Google\",\n      \"Salesforce\"\n    ]\n  },\n  \"techniques_or_concepts\": [\n    \"token-efficiency\",\n    \"context-window management\",\n    \"tool-definition overload vs on-demand loading\",\n    \"code execution with MCP\",\n    \"progressive discovery of tools\",\n    \"security via in-sandbox data processing\",\n    \"state persistence with code vs model memory\"\n  ],\n  \"tools_or_materials\": [\n    \"Google Drive tool (G Drive) definitions\",\n    \"Salesforce tool (update record)\",\n    \"TypeScript code examples for MCP integration\",\n    \"MCP server tool discovery via file-system approach\",\n    \"Daytona sandbox for secure code execution\"\n  ],\n  \"key_points\": [\n    \"Direct tool calls consume context for each definition and result; code execution environments reduce token usage by loading tools on demand.\",\n    \"A common critique is that MCP burdens the model with hundreds of tool definitions; code-execution approaches can reduce token costs dramatically (example cited: 150,000 tokens down to 2,000 tokens, ~98.7% savings).\",\n    \"Loading all tool definitions upfront can slow down and bloat context; progressive discovery and on-demand tool loading improve efficiency.\",\n    \"Code execution preserves data in the execution sandbox, enabling progressive disclosure and reducing exposure of sensitive data in the model context.\"\n  ],\n  \"content_style\": \"critique\",\n  \"difficulty\": \"advanced\",\n  \"references\": [\n    {\n      \"type\": \"blog_post\",\n      \"name\": \"Code execution with MCP, building more efficient agents\",\n      \"url\": \"https://www.anthropic.com/engineering/code-execution-with-mcp\",\n      \"description\": \"Anthropic blog article discussing code execution with MCP and efficiency benefits.\"\n    },\n    {\n      \"type\": \"blog_post\",\n      \"name\": \"Cloudflare code mode\",\n      \"url\": null,\n      \"description\": \"Cloudflare discussion about code mode as a solution to MCP-related inefficiencies.\"\n    },\n    {\n      \"type\": \"product\",\n      \"name\": \"Savala\",\n      \"url\": \"https://soy.link/savala\",\n      \"description\": \"Sponsorship mention; hosting platform cited as an example of streamlined deployment.\"\n    },\n    {\n      \"type\": \"product\",\n      \"name\": \"Daytona\",\n      \"url\": null,\n      \"description\": \"Sponsor referenced as a sane way to run AI-generated code in a secure environment.\"\n    },\n    {\n      \"type\": \"documentation\",\n      \"name\": \"Google Drive get document\",\n      \"url\": null,\n      \"description\": \"MCP tool definition example illustrating a tool interface for Google Drive.\"\n    },\n    {\n      \"type\": \"documentation\",\n      \"name\": \"Salesforce update record\",\n      \"url\": null,\n      \"description\": \"MCP tool usage example illustrating a Salesforce integration.\"\n    },\n    {\n      \"type\": \"github_repo\",\n      \"name\": \"MCP tool definitions and code-execution examples\",\n      \"url\": null,\n      \"description\": \"References to repositories showing MCP tool definitions and code-based interactions.\"\n    }\n  ]\n}",
      "generated_at": "2025-11-14T21:04:11.963557",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": [],
  "manual_annotations": {
    "importance": "high",
    "annotation_type": "important_critique",
    "annotated_at": "2025-11-14T21:10:00",
    "critiques": [
      {
        "target_url": "https://www.anthropic.com/engineering/code-execution-with-mcp",
        "target_type": "blog_post",
        "relationship": "critical_analysis",
        "note": "Important critique of Anthropic's MCP blog post, highlighting fundamental issues with MCP protocol design and advocating for code-first approaches"
      }
    ],
    "tags": [
      "important",
      "mcp-critique",
      "engineering-perspective"
    ]
  }
}