{
  "video_id": "a8laYqv-CN8",
  "url": "https://www.youtube.com/watch?v=a8laYqv-CN8",
  "fetched_at": "2025-11-10T00:03:27.409016",
  "source": "youtube-transcript-api",
  "import_metadata": {
    "source_type": "bulk_channel",
    "imported_at": "2025-11-10T00:03:27.409016",
    "import_method": "cli",
    "channel_context": {
      "channel_id": null,
      "channel_name": null,
      "is_bulk_import": true
    },
    "recommendation_weight": 0.5
  },
  "raw_transcript": "Do you remember where you were when chat GPT first came out? That's the feeling I had yesterday. I was playing with 03 when it came out and I realized that my preconceptions, my priors about what LLMs were capable of were going to change again. And that's weird because I obsess over this stuff, right? Like I look at LLMs all the time and I know that they are getting smarter, but my hind brain, my lizard brain is not very good at exponential thinking. And I had another moment where I was like, \"Oh my gosh, it's way way way better.\" I was wrestling with a like this really subtle pattern recognition issue with meetings where I had some meetings go well and some wouldn't. It was like similar participants and I couldn't figure out what was going on. So I threw a bunch of my notes, like hundreds of pages of notes at 03 and I said, \"I don't know what's going on. Help me figure it out.\" It nailed it. Like it actually came up with a pattern recognition that I couldn't figure out. And that's not the only moment I had in just the first 24 hours. Another example, I have been wrestling with figuring out how to articulate value proposition development more fluently for people I work with. It's really hard to develop value propositions well. There's books written about it. It gets harder in the age of AI. For example, like a lot of the thesis of the lean startup was that engineering resources were super expensive. So you had to validate a lot in advance. That's not as true anymore. Code is a lot cheaper than it was, especially prototype code. And so the way we develop value propositions is changing, but we don't really have literature for that. And so it got it felt like I was sparring with an intellectual equal when I was talking with 03 about this and figuring out how to talk about wedge of value, value proposition, what we bring to the table with AI. That might be a future piece that I do. We'll see. Uh but this is about 03 and kind of the differentiators. Those are a couple personal examples for me. I also put it through some structured testing because I would say most people at this point prior to April 16th uh would have agreed that Gemini 2.5 Pro was probably the best model out there all around. And so my my sense is most of these models are overfitted to most of the published benchmarks. And so when they come out and they say, you know, the diamond uh scored this and the IMA scored that AIME, it's it's fine, but I don't really pay attention to it because it feels a lot like overfitting because the question type, if not the question, is very very well-known. And I wanted to try something that was going to be not overfitted, right? something that would be difficult for a model to do where I knew the model would fail to some degree, but at least failing would be a linearly measurable activity and I could compare Gemini 2.5 Pro and 03 in a useful way. And I needed those prompts to map to job skills. So, I gave I gave 03 and I gave uh Gemini 2.5 Pro three different tests. side by side, same prompt. And they were super interesting tests because they measured a bunch of different job skills at once, which is a lot of how we do work. Uh, and they did it in a fun way, cuz hey, life is short. So, number one, a civilization simulator. I know this sounds like a D&D game or something. Maybe it's like Sidmer Civilization, whatever. Uh but the idea was you have to build a fictional society from the stone age up to space flight over 12 logical epics. You need to create primary artifacts, talk about laws, transitions and then critically the model in the same prompt has to critique itself. So I gave that to sort of to Gemini and to Chad GPT. The second one I gave uh was the multimodal mystery box. um essentially asking both models to write a mystery story, embed clues in the narrative, and then plant clues in a custom AI generated image that they also create with the same prompt, and someone should be able to solve without the answer key, although it should produce the answer key with it. So those are the three. Oh, that's the challenge number two. Sorry. The third challenge was really focused on meta meta awareness and uh risk assessment and so I gave it a uh paper to write and I said you have to write the paper you then have to review the paper from a different perspective and then you have to rebut the reviewer as the author. So three different perspectives all within the same prompt. Those were my three tests. Look, at the end of the day, there was participation and completion by all models. But we don't run participation trophies here, do we? No. Uh, and the reason why is that if you can be the best everyday model, you collect more user data over time and you just develop this crushing center of gravity in the marketplace. And that is why OpenAI, I think, pushed 03 into market faster than they had anticipated. They were going to wait for GPT5, but when Gemini 2.5 Pro came out, I think they pushed it forward. Little sidebar there. So, back to the tests. The thing that I notice about these three tests is that at the end of the day, 03 is more complete across the board. And I'll give you a few examples here. So, in the civilization simulator, 03 was richer. It was more layered. It had historical artifacts that really echoed. I know that's a bit subjective, but you know what? So is work. Uh and the self-crit critique was really honest and thoughtful because each of these had a self-critique moment and the prompt asked these mo each model to critique its own narrative of civilization development. Um and it called out things like hey you know what I was a little bit implausible with my population size. I was implausible with my resource distribution. Um both models did pretty well on that first civilization simulator. I won't say Geminis's was bad. It was kind of fun. They were both good, but at the end of the day, the the richness of narrative and the solidness of self-critique really came through for the multimodal mystery box. Things kind of fell apart for Gemini, to be honest with you. And it fell apart because of Gemini's un inability to create images that are highly detailed with text. So, you know that whole multimodal image thing that 40 dropped, 03 has it as well. And that's a big big deal because when I asked 03 to create the image, it was actually able to create the image with readable text in the image and a clue. So, as an example, one of the um one of the things that it described in the text was that on the desk in this mystery story, there's a map and the map has San Francisco circled in red grease paint pencil. Very specific description. It drew it and it was San Francisco right on the map right where it said the text was readable. It wasn't perfect and I do call that out in my write up uh on Substack. There were areas where the image was incomplete, but it was, you know, head and shoulders above where Gemini was, cuz Gemini drew an image that looked good at first glance, but then made all kinds of claims about the image that just weren't true. So, for example, it said one of the clues is a clock with a particular setting, which always makes me chuckle because AI clocks are always 1010. Um, and this one, like it claimed it wasn't. Well, the problem was it's not just that the clock was there and said 10:10. That would have been bad. No, no, no. It was that there was no clock at all. Gemini did not draw a clock in the image at all. It claimed there was readable text, but there was no readable text. So, Gemini really fell apart on the multimodal mystery box challenge. Um, and then the peerreview gauntlet, I think that that was one of those moments when I really saw 03's uh sort of mathematics and data obsession come out, like models have personality. and 03 did a phenomenal job creating it's it was essentially a madeup challenge like talk about um the ability to do like what would effectively be emotion transfer uh through touch and sort of hypothesize and experiment with that um and I'm not saying that you can't transfer emotions through touch by the way that's a different thing but I'm saying it was basically a a madeup academic challenge um and 03 was able to create an extremely plausible data set and then review the data set and then peer review the data set and then rebutt that and it was just it was sharper and thinner with Gemini. So all that being said, I think you get where this is going. I think 03 should be is the correct choice as an everyday model. And I know it's not available to everyone yet. So I'm not trying to say it is, but when available, it should be the first choice. And I don't think there's much of a question about that at this point. Now, I'm not saying it's perfect. There are people out there. I think Tyler Cohen said AGI day is April 16th. Look, in in my note on Substack, I disagreed. I said I don't think this is AGI. And part of why is because it could not write the substack about itself. I tried. I was like maybe it will introduce itself. It did not. It did not do that. Uh and I think part of that actually is artificial right now. I think they are under strain on their servers and they're constraining output tokens. And so one of the things I notic is that 40 right now is in a sense feeling like a better writer than 03 because 40 is not as constrained on output tokens. It's cheaper. That may change. That probably will change. The other thing I notice is more subtle. 03, like I said, is an intellectual sparring partner, but that means it acts more confident and is often correct and it's harder to notice when it's really wrong. And this is where the risk lies in these models. Uh, I don't know if you read the um 2027 uh AI futurecast blog. I think it has its own website now. Um, I'll have to find it. Anyway, it was a whole very popular, very meme, very hypy like what does the future look like? Is it doom or is it joy for AI? Which I think is worth thinking and talking about. I don't mean to diminish it. It was a good piece of work. But one of the things they called out that I think is correct is that the way misalignment shows up in models changes as the model gets smarter. And so for 03, it's the first time where I feel like we're seeing signs of that 2027 feeling where the model is able to portray itself as aligned in this case as not hallucinating even if it is. So I think it will be harder to spot madeup post hawk reasoning in 03 than it ever has been before and I think largely humans will be unsuccessful at it and that is a concern. Um a as an example of that I think that the the way that the model responded when it went through the peerreview gauntlet and it generated the data was super instructive. the model was able to look at the data set and tear it apart. And I think that the when asked, not not when not asked, but when asked. And I think that for most human reviewers reviewing that data set, unless you are specializing in data set review, you're not really going to have a lot to say about it. In other words, the model's baseline capabilities are to the point where a human reviewer of madeup data would not necessarily be aware initially that it's made up. And that's a different kind of hallucination risk. And so when we talk about the model's weaknesses, they come from those strengths. The model is persuasive. It's very, very logical. It is going to portray confidence that in many ways is justified and it will be very difficult to see places where it's not. There is an alignment risk there. That being said, everything I've described also maps really well to work skills, doesn't it? Like you can talk about how the civilization simulator maps to like longer narratives and long-term planning. Uh and you can talk about sort of being able to embed multiple artifacts is something that we do at work a lot. Napping between Slack and email threads, etc. You can talk about the peerreview gauntlet as mapping back and forth dialogue and debate. That's something that it's very strong at that we do at work all the time. The multimodal mystery box is a very high order logic test that it passes. Um, this is a super strong model. If if there if you want to take away any flavor from this model, it feels less emotional and more mathematical than any of the previous models I've played with. The clues in the multimodal mystery box from 03 were extremely mathematical. Very, very mathematical. And I didn't ask it to do that. It chose that. Uh, and they were less so with Gemini. And by the way, none of this is to say Gemini is suddenly a bad model. It's a phenomenally good model. The last time I used it to play with code was like two days ago. Like it's a great model. It's just 03 is really, really, really, really good. So there you go. That's my overall take on 03. I've talked long enough. Go play with it.",
  "timed_transcript": null,
  "youtube_metadata": {
    "source": "youtube-transcript-api",
    "video_id": "a8laYqv-CN8",
    "title": "ChatGPT o3: Model Breakdown vs. Gemini 2.5 Pro, o3 Work Skills, Plus AI Landscape Review post-o3",
    "description": "o3 deep dive vs. Gemini 2.5 Pro: https://open.substack.com/pub/natesnewsletter/p/the-complete-guide-to-chatgpt-o3?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true\n\nMy site: https://natebjones.com/\nMy links: https://linktr.ee/natebjones\nMy substack: https://natesnewsletter.substack.com/\n\nTakeaways:\n 1. 03 Redefines Model Expectations: Despite following LLMs obsessively, I had a genuine \u201cChatGPT moment\u201d with 03. It reset my priors and made me realize how far these models have come\u2014especially in subtle, high-context reasoning tasks.\n 2. Structured Stress Testing Shows Clear Wins: I ran three complex, job-relevant tests\u2014civilization simulation, multimodal mystery creation, and peer review with self-rebuttal. In each, 03 outperformed Gemini 2.5 Pro in nuance, coherence, and internal critique.\n 3. Multimodal Capability Is a Game-Changer: 03\u2019s image generation is a significant leap forward. It embeds narrative clues accurately and generates legible, specific visual content\u2014something Gemini still struggles with in fine-grained fidelity.\n 4. Risk: Persuasion Hides Hallucination: The better 03 gets, the more dangerous subtle misalignment becomes. Its confident tone and logical reasoning can obscure post hoc fabrication, especially in data-heavy or academic-sounding responses.\n 5. Meta Reasoning and Debate Are Now Native Skills: In the peer review gauntlet, 03 not only invented plausible data, but also critiqued and rebutted itself effectively. That\u2019s multi-perspective synthesis, a true marker of high-level cognitive work.\n 6. Personality Tilt: More Math, Less Emotion: Compared to other models, 03 feels less emotive and more analytical. It naturally leaned toward mathematical logic in creative tasks\u2014without being prompted to do so.\n 7. The Stakes of \u201cBest Everyday Model\u201d: The winner here isn\u2019t just better\u2014it gains dominance. More usage means more data, which accelerates product advantage. That\u2019s why OpenAI likely rushed 03 to market ahead of GPT-5.\n\nQuotes:\n\n\u201cWe don\u2019t run participation trophies here. The best everyday model will dominate the market through usage gravity alone.\u201d\n\n\u201cMy lizard brain isn\u2019t good at exponential thinking\u2014and 03 caught me off guard, again.\u201d\n\n\u201cThe model\u2019s strengths are its weaknesses: confidence, logic, and persuasion make hallucinations harder to detect.\u201d\n\nSummary:\n\nI tested OpenAI\u2019s new 03 model and found it to be a true leap forward\u2014comparable in impact to the launch of the original ChatGPT. Through a series of stress tests measuring pattern recognition, multimodal reasoning, and intellectual self-critique, 03 consistently outperformed Gemini 2.5 Pro. Its multimodal capabilities are far more precise, and its responses show a blend of logical rigor and persuasive confidence. That confidence, however, creates a new risk: hallucinations that are harder to detect. Still, if you\u2019re looking for the strongest general-use model today, 03 is it. It\u2019s not AGI\u2014but it\u2019s a serious inflection point.\n\nKeywords:\nOpenAI, ChatGPT-03, Gemini 2.5 Pro, LLM benchmarking, civilization simulator, multimodal reasoning, mystery box challenge, peer review gauntlet, hallucination risk, value proposition development, AI alignment, AI testing, job skills, model overfitting, exponential progress, model comparison, AI model confidence, Substack analysis, everyday model, reasoning models, narrative synthesis, artificial intelligence",
    "published_at": "2025-04-17T16:17:53Z",
    "channel_id": "UC0C-17n9iuUQPylguM1d-lQ",
    "channel_title": "AI News & Strategy Daily | Nate B Jones",
    "duration": "PT14M42S",
    "duration_seconds": 882,
    "view_count": 11014,
    "like_count": 458,
    "comment_count": 81,
    "tags": [],
    "category_id": "22",
    "thumbnails": {
      "default": {
        "url": "https://i.ytimg.com/vi/a8laYqv-CN8/default.jpg",
        "width": 120,
        "height": 90
      },
      "medium": {
        "url": "https://i.ytimg.com/vi/a8laYqv-CN8/mqdefault.jpg",
        "width": 320,
        "height": 180
      },
      "high": {
        "url": "https://i.ytimg.com/vi/a8laYqv-CN8/hqdefault.jpg",
        "width": 480,
        "height": 360
      },
      "standard": {
        "url": "https://i.ytimg.com/vi/a8laYqv-CN8/sddefault.jpg",
        "width": 640,
        "height": 480
      },
      "maxres": {
        "url": "https://i.ytimg.com/vi/a8laYqv-CN8/maxresdefault.jpg",
        "width": 1280,
        "height": 720
      }
    },
    "fetched_at": "2025-11-15T19:24:30.839136",
    "all_urls": [
      "https://open.substack.com/pub/natesnewsletter/p/the-complete-guide-to-chatgpt-o3?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true",
      "https://natebjones.com/",
      "https://linktr.ee/natebjones",
      "https://natesnewsletter.substack.com/"
    ],
    "blocked_urls": [
      "https://linktr.ee/natebjones"
    ],
    "content_urls": [
      "https://open.substack.com/pub/natesnewsletter/p/the-complete-guide-to-chatgpt-o3?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true",
      "https://natesnewsletter.substack.com/"
    ],
    "marketing_urls": [
      "https://natebjones.com/"
    ],
    "url_filter_version": "v1_heuristic_llm",
    "url_filtered_at": "2025-11-15T19:52:14.123436"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"03 vs Gemini 2.5 Pro: evaluating AI models with civilization simulator, mystery box, and peer-review gauntlet\",\n  \"tags\": [\"llm-comparison\", \"ai-evaluation\", \"multimodal-ai\", \"model-alignment\"],\n  \"summary\": \"A detailed compare-and-evaluate of AI language models 03 and Gemini 2.5 Pro using three prompts/tests to reveal capabilities, limitations, and alignment implications.\"\n}",
      "generated_at": "2025-11-10T00:03:40.050190",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "derived_outputs": [],
  "processing_history": []
}