{
  "video_id": "6iEgJsqkdeM",
  "url": "https://www.youtube.com/watch?v=6iEgJsqkdeM",
  "fetched_at": "2025-11-10T00:30:23.711772",
  "source": "youtube-transcript-api",
  "raw_transcript": "one of the things I've been calling out is that we really need to get past this idea of short-term memory context windows in AI where you have a limited context window and the AI just forgets well Google has written a paper that helps us think about how to get past that uh it's called Titans and it's basically presenting a different architecture than traditional Transformer based architecture and large language models I'm going to try and explain it to very briefly we should probably do a longer video on this at some point but the paper just came out I'm still reading it so the takeaways that I have at the top right now Transformers use self attention to compute relationships between all of the tokens at a sequence so if you say the cat jumped over the dog that's a sequence it's Computing the relationship between the tokens in that sequence so self attention is going to have mathematically speaking what we would call quadratic complexity in other words it's very very expensive to compute for long sequences because you're multiplying across all of the relationships and Transformers struggle and don't explicitly distinguish between short-term and long-term memory it all works like that every token interacts with all of the others and so Titans is different because Titans introduces something that's closer to our own brains there's a dual memory system in Titans a Titans architecture apparently has a short-term memory which is very similar to how Transformers work today and it focuses on local dependencies it also has a long-term memory which is a separate net new neural module that's explicitly designed to store and retrieve information from past context now what's interesting is it apparently works over longer context windows so titans's long-term memory can handle context lengths exceeding 2 million tokens it does that by efficiently retrieving information without recomputing the dependencies for the entire sequence so it can look at Ultra long range dependencies like uh relationships between genes and a genome now the nice thing is it enables you to get to linear scaling versus sort of the computational cost of quadratic scaling and I know that sounds mathematical but basically if you're not Computing all the relationships all the time then you're able to scale farther and so that's really exciting I'm still digging in I'm still trying to figure out what all is in here but potentially it seems to enable long range needle in a hstack type memory retrieval that's what the authors claimed they did they claimed they tested it versus Baseline trans Transformer architecture on what's called a needle and a Hast stack task a classic example is you change one word in Moby Dick and you tell the Transformer to find it and you see if it can look through the entire context window and find it and they claim that their long-term uh Titan's memory architecture does better at that than Baseline um and they think that by explicitly differentiating what requires immediate attention in short-term memory versus long-term attention it's going to mimic human abilities better and allow us to exceed traditional context Windows that's what I've got so far I'm still reading the paper I think it's potentially very important uh and I wanted to share it with you and see what you guys think",
  "youtube_metadata": {
    "source": "youtube-transcript-api"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"Titans architecture: long-term memory and linear scaling in AI\",\n  \"tags\": [\"long-term-memory\", \"transformer\", \"self-attention\", \"linear-scaling\"],\n  \"summary\": \"Explains Google's Titans architecture, a dual-memory AI model with a short-term local memory and a separate long-term memory module that enables ultra-long context and linear-scaling computation.\"\n}",
      "generated_at": "2025-11-10T00:30:43.495993",
      "model": "claude-3-5-haiku-20241022",
      "cost_usd": 0.001,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "processing_history": []
}