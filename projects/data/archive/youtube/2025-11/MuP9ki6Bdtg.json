{
  "video_id": "MuP9ki6Bdtg",
  "url": "https://www.youtube.com/watch?v=MuP9ki6Bdtg",
  "fetched_at": "2025-11-09T19:23:28.505686",
  "source": "youtube-transcript-api",
  "raw_transcript": "Okay, so while everyone is waiting for Gemini 3.0 to come out, the Gemini API team introduced a really cool new feature this week. And this is basically a sort of automated or built-in rag system that you can use with the Gemini API. So, it's actually called the file search tool. And what it actually does is it allows you to basically upload a whole variety of different documents, whether they're PDFs, whether they're code, whether they're text markdown files, even things like logs and JSON files. And then what it will do is it will provision a file store for them and actually process them and chunk them and do the embeddings for you and then give you a system that you can basically just call Gemini and use this vector store and rag database as grounding for use with the actual Gemini APIs. So if we come in here and look at how this actually works and they're actually calling it file search. I'm not sure why they're not just calling it something like Gemini rag, but fundamentally this really is just a streamlined entire rag process here. So, what happens is you upload some files and you upload them in a way that they're going to become part of a file store. Once those files are up there, they're then going to be split and chunked and then each chunk is going to have an embedding made for it using the Gemini embedding model. And then at query time, you basically just call the Gemini API, passing in this file search tool, and in the back end, it will handle embedding your query, doing the vector store lookup, getting the relevant information, putting it into a natural language response, and even giving you a set of citations that you can return to the user to actually use in your UI, etc. So, they've actually made a demo app of this using the Gemini Build sort of vibe coding tool in here and you can come in here and see that. Okay, you basically click to upload something or we can use one of the examples. In this case, I'm going to use the example here because after this I'm going to go through both a simple version with code and then a more advanced version with code to show you some of the details of what's actually going on under the hood. But you can see that if we come out here, we upload this, it's going to go through the process of generating a set of embeddings, putting them together in this file store so that we can do vector lookups when we run our query and it gets embedded as well. Okay. And then when it's finished processing, you can see now we've got this dock here. It will even suggest some questions that we can ask it. And I can just click on some of those suggestions. We'll actually look at the code for this in a second. And you can see that it will go through. It's not loading that full document into the context window. It's just loaded our query and then it will go off and get a certain number of responses back from the chunks and then put them together. And you can see it's gone through and it's actually given us an answer in here. And then on top of that, for each of these, we can see we've actually got the source chunks in here. So, if we wanted to, we'd probably need to write some extra code for highlighting things in here specifically or those sorts of things. But we can see that this is what the raw system is actually getting out and using as the five sources for creating the answer in here. Now, I don't even actually know what the Hyundai i10 is. So, let's ask it what it is. So, we can see from this what it is. It's been able to go and find out those things about the car. And we can come in here and we can see the actual sources here. If you want to look at the code of how they've done this, it's quite nice because we can come in here and actually look at this code. We don't really care about the icons. What we do care about is in the services coming down here and seeing the actual Gemini calls. So we can see obviously in this case it's basically a TypeScript file. If we want to see what actually is in here, it's going to take our query and it's going to append to that. Do not ask the user to read the manual. It's a bit embarrassing if you have your rag system that is all about a manual for something telling you to go and find the answer in the manual. It's basically saying pinpoint the relevant sections. So that's being appended to the actual prompt that's going in there. And then we can see if we wanted to know how they're generating the actual sample questions, the prompt for that is in here as well. So it's using Gemini Flash for both of these. Often people are using Gemini flashlight for these kind of suggested prompt things in here. But we can see here for the suggested prompt getting you are provided some user manuals for some products. Figure out what product each manual is based on the cover page. Do not guess or hallucinate the product. Then for each product, generate four short and practical questions a user might ask in English. And then we can see that it's actually telling it return those questions as a JSON array of objects. And it's got some examples of how the actual JSON should be. And you can see in here for both queries, the actual rag store name is being provided in here. And so that's got the chunks, right, that have been embedded, etc. for going through this. So at the simplest level, we can see that there's not a lot to this. It really is just you upload the files. It will take care of the embeddings at the back end. It will take care of the vector store at the back end. And then we just call Gemini and basically pass this in as a tool for Gemini to use. So I think the best thing is let's jump in and have a look at a code example and see both sort of the simplest example of what it can do, but then also look at maybe how we could combine it with some other things to make a more advanced example. All right, let's jump in. Okay, so before we jump into the code, let's just take a look at actually what's going on here. So the high-level process of where we're indexing we're basically uploading our documents to file storage. Once that is done, they will go through an embedding model. So they'll be chunked down into chunks. We can actually set that. I'll show you that in the advanced code walk through. But then once we've done this, they'll be put into a vector store and then we can actually use it. So at query time, what happens is the user will just call the Gemini API or our app will call the Gemini API. It will then decide does it need the grounded knowledge to answer the question or not. Right? If not, it will just use normal Gemini to answer. If the answer is yes, it will then come over to the actual sort of rag part and it will generate some queries, right? It could be one, it could be multiple. Then for each of these queries will be embedded. It will then be passed to the database or the vector store where it will get the most relevant chunks, bring them back. If there are multiple queries, this could actually go through a number of different times for doing a multihop answer. It'll actually go through and do multiple queries here. It then brings them all back to Gemini and then combines this with our original query to create the final answer and generates the final answer that way. Let's jump into the code and have a look at actually how it's done in code. Okay, so you need to make sure that you've got a recent version of the Gen AI SDK installed. You want to make sure you've got a Google AI Studio key in here. And in this case, I'm basically just bringing in a PDF file. So the actual thing I'm bringing in is from the court documents. This came out this week where it's actually a deposition I think of Ilia Saskava and he talked about what led up to them sort of firing Sam Waltman for a few days and why he made the decisions he did etc. I think this is part of the ongoing lawsuit between Elon Musk and OpenAI or Sam Alman. And my thinking here was rather than just take everyone else's interpretation of this, I want to look at it myself, but it's quite long. I'm just going to ask particular questions and then see what the answers are back and then have a look at perhaps some of the grounding documents or the citations. So once we've got that document, the first thing I'm going to do is make a Genai client. And then I'm going to set up this file search store. And so this is basically our vector store in here. So we can give it a display name. In this case, I'm just calling it SAM basic. And you can see that once that's created, we've got a timestamp. We've got the display name. We've got the actual sort of underlying name that it gets referred to in the file search stores. And then we've got the last update in there. One of the things that's really important is you want to be able to see all your file search stores. So while the original documents that you upload get deleted after 48 hours, the file search stores stay there until you delete them. So you want to make sure that you can at any point have a look at all the file search stores that are actually on your account. Now in this is I'm listing it out. It's basically only one. I will show you later on actually how you at the end can delete it so that you're not going to be paying for having that up there. And speaking of the pricing and the rate limits etc. You can see that maximum file size per document is 100 meg. If you've got a free tier you can get 1GB of file search stores. If you've got tier one etc. goes up to 10x that then 10x that again then 10x that again. And pricing for this is pretty reasonable. We're just paying for the embedding calls for when we actually upload it at the normal price for Gemini embeddings. The storage, i.e. the vector storage there seems to be free for now. I don't know if that's going to change in the future. And then the actual embeddings for query time are also free. And then finally, you're just charged for the normal tokens just like you would be with any other Gemini API call. All right. So once you've got your file search store set up, you want to actually upload something. So here you can see I'm uploading this document in there. I'm giving it a display name of Ilia testimony01. I pass this in. I pass in the actual file search store name which you can see this is it here for where we're going to upload this. And I'll show you later on in the advanced one uploading multiple documents. But here we can kick this off and we've got this operation of really where it's more than just uploading. This is uploading and importing it in there and sort of processing it where it's going to be chunking, it's going to be doing embeddings, etc. So, we actually get this operation back and we can see what's going on. And at any point, we can see if it's done. So, one of the things you want to perhaps do if you're going to make a UI for this is you want to have sort of something where you're checking if it's done and then show you know that perhaps it's not done. In this case, print waiting. And if it is done, just print done. So in this case it uploaded pretty quickly and it's done now. And at this point we're done with the ingesting and the setting up of the actual vector store embeddings all those sorts of things. So now we can just call it and you can see that here I'm just going to start off by saying tell me about this document. Now I've added some stuff to the prompt just like I showed you in the other one of where we basically I want it to be returned in markdown. I want it as sections and bullet points. So I'm just adding that to whatever the sort of user prompt is going in there. It's really important in here that we basically have our tools. So we've got a list of tools and this one that we're passing in is type tool. It's the file search types file search and we're passing in the actual name of that. Now remember that's not the display name. That's the sort of long name in there. You see when I mouse over it here that you can actually see that it's Sam basic luh etc. We will then get a response back. We can just print out the response.ext. You can see sure enough it's given me this back as metadata in here. It's obviously got some things from the sort of front of the document about who's speaking in it, the date of the deposition, the case where we got Elon Musk versus Sam Olman. We've got the confidentially is marked highly confidential. As far as I know, from what I'm seeing online, it's actually been released in court documents. So, anyone can just download it. That's how I got the PDF. And then we can see sort of brief summary or brief what's going on. So, this is all about a memo that I created about Sam Alman. And it's got a lot of things in that. And then it's also got other people's comments, other people's reactions. Now, how do we know that this is true? Right? we can come in here and look at this metadata. So, as part of the response for each candidate we get back, we get this grounding metadata that it's been grounded on. And in this case, if we scroll through, it's got lots of parts of the dock in there, but we've also got like these grounding chunks. So, we can see bits where it's given us an answer back. Where what chunks do they refer to going through here? So just to show you this, we can actually go through and just look at the dictionary keys in there. And we can see that the grounding metadata is not just for the file search tool. It's for Google Maps stuff. It's for different things with web search with a bunch of different things. What we're interested in is these grounding chunks and the grounding supports. And so you can see if we come along here and we look at this fourth grounding support and grounding chunks, we can see that okay, we've got this grounding support here. So this is basically got the segment, you know, where it's coming from. And we can see with looking at that grounding chunk, the corresponding grounding chunk, where the answer actually comes from. So we've got the raw text in here. This text has lines like this. So we can see if we look in here, this is actually how the document is. It's not a nice neatly formatted document or anything like that. We can see what's actually going on in there. And then we come back to our thing. And we can see that okay, we've got all the grounding and stuff like that in there. Now, if you want to run another chunk, what did I see? I will let you go through and run this one yourself. You can look at the answer that comes back. And if you want to go through the grounding and stuff. So, finally, if we want to delete this and basically just get rid of the store, I've basically put together a little script here. Will delete all the stores on your keys. You can either get the store by the name, this is one of the things that you can do, and then just delete that one. or in this case it just loops through and deletes all the stores in here. So next up, let's look at a more advanced example where we can perhaps combine a few things from documents and some other things going on here as well. All right, so in this one it's a little bit more advanced. We're just going to add in a few different features that we didn't have. Mostly things around custom chunking, adding some metadata, multiple files, that kind of thing. So we start off just the same. The thing I've done here is I've brought in a set of transcripts of my last eight videos in here. And you can see that they're quite varied topics. So that was sort of like the goal here. I'm going to make a new vector store this time. So it's going to be called the Sam YouTube transcripts vector store. We can see when we look at this, sure enough, it's been made fine and stuff like that. And we can just list out to see what's there. All right. Now the next up is we want to set up our chunking for uploading. Chunking is basically just passed in as a config. We don't have a huge amount of options at the moment. This might change in the future, but we can go in here and set the maximum tokens per chunk to be 250. We can have overlap tokens to be 50 in this case. And then we're going to basically just pass in when we do the uploading. All right. So, if we're doing the multiple files, I'm just going to glob the folder of transcripts. And you can see we've just got a a path of where each of these is a markdown file. Now, what we want in there is if we look at one of those markdown files, we'll see that it's basically got a transcript where the title is the first line, the URL is the second line, and then the rest has got some timestamps in it, and it's got the transcript as we go along. You can see here I'm just making a really simple function to extract out that title and URL. So this will basically just get it where we can pass something in. It's going to return back the title and the URL which we're going to use for metadata. Now we could even add things like the date. You know there's lots of metadata that we could apply to this. We could actually have another function where we actually pass in the whole sort of article to the Gemini flash light model and ask it to create the metadata. So that's a good thing that I've done in the past is where you can use those really small fast models and ask it to give me a set of 10 keywords to give me a variety of different information that you want out of it and even to do things like to classify it. that is this video about a new model, a new product, a technique, a tutorial, that kind of thing. And that can work really well at being able to get it so you get a whole bunch of sort of custom metadata that you can then pass in for doing searches later on. For example, if I wanted to search all the videos that were just tutorials or all the videos that were just about OCR models, that kind of thing. And just to test that it's working, we're going to pass back our title, our URL, and our file name. And sure enough, it's working. If we just go through that list and print them out, we can see we're getting the data in there. All right. Next up, we want to create our upload function. So here, I'm going to basically just pass in the file path. From that, we will extract out the title, the URL, we will get the file name, and then we'll pass these in. And you can see that, okay, we're going to have the display name be the title name. We're going to have the chunking config be what we defined earlier on for the chunking config. And then we're going to have our custom data is just going to for now basically use the title, the URL, the file name. Like I said before, you could actually have a whole bunch of sort of keywords and other things in there as your metadata. Going through each time it's going to basically check is the upload done. When it's done, it's going to print out that it's uploaded and indexed. All right, we now run that and we can see sure enough it goes through and it uploads each of our docs up there. Now, if you wanted to, you can actually upload things to the files API. So, this is a different API. This is not straight to the vector store. At this point, we can actually then import them in. So, we could do something very similar to this except for rather than upload to file search store, it would just be import them into the files store. And that's if you've got files already uploaded there. All right. In this case, we then want to try it out and see how it goes. So you can see here we're basically saying, right, what is the name of the browser? OpenAI launched. You can see I'm passing in the prompt. I'm also passing in some instructions in there just to return in markdown. Be concise. One of the things that I could actually do here, and you can play around with this, is that I could actually get it to return the timestamp that's closest because you'll see that when it goes through it, sure enough, it gets the answer right pretty easily. So, just going through those documents, it can find out that the atlas is the name of the browser. And if we look at the grounding data that it's actually grounding on, you'll see that it's found the right video transcript. That's not a problem. It's got the URL in there. It's got this transcript, the sort of timestamps in here. So, we could actually get something where it returned back the timestamp and we could actually ask it to do that in JSON or in something like that. And then we could actually convert that to a time for the number of seconds and then pass that in a link so that when somebody clicked it, it would actually take it to the spot in the video where this was talked about. So that's something that we could look at doing as well if we want to see all the actual citations. You can see that okay just going through we've got these different citations where it's being talked about and just like before I'm just showing you that okay you do get a bunch of things in here. You get this grounding chunks and grounding support. Unfortunately the queries stuff like that this is seems to be for Google search. It's not for doing this. So it would be nice if they actually returned what were the queries that they ran and stuff like that and how many of them were there. But you can see that if we look at the grounding chunks and grounding supports we get a much more in-depth of what we were just looking at before going through. Right. Next up just to show you this we can also do things like where we ask it for multiple videos. So okay two of the eight one was about Claude skills one was about haiku. Sure enough, if I ask it what videos does Sam make about Claude, please give URLs, it's coming back, giving the correct URLs, giving the names of the videos, and then a little bit about them going through here. What about if we wanted to actually use meta data to do the search? So, you can see here where I'm basically saying, what does Sam talk about in this video? Now, this video could mean any of those eight videos. is now if we didn't provide any metadata in there, it would be confused and it would just tell us I don't know what video you're talking about. But if we pass in the meta filter of title equals and then we've put this title in there now it can actually work out that oh okay this is the video that you're talking about and it filters it out. Another good example of that is what if we wanted to do a search just on a URL. So you can see that the URL here was for the Claude skills. So, I just copied that down. And now we've got the same thing. What does Sam talk about in this video? But now we've got the metadata being the filter of the URL passing in there. And sure enough, we get the stuff about Claude skills. We get a breakdown of that. Remember, you could play around with these. In this one's you can see that I haven't put the extra instructions in there, but this is something that you can do just like you would do with any of your calls to the Gemini API. So just showing you there we've basically now got multiple documents. We've got metadata. We can filter responses back based on that metadata. And we could really expand this now by putting in a few hundred different text files in there. And it would be able to go through and we could put in things like date as a meta filter and stuff like that in there. All right. So just to finish up, if we want to list the docs that are in the vector store to actually see what's in there, we can do that so that we can see what's in there. And then finally, if we want to actually delete it all, we can just delete our store and we've basically taken everything out. So hopefully this video has shown you that you can use this file search tool in the Gemini API to build very quick rag solutions that you can try things out. Now, they're not going to be the fanciest rag solutions. Obviously, if you wanted to do lots of different things, you still need to basically build it perhaps from scratch. But if you want to get something up really quickly where you can have users just dump in a bunch of documents themselves and then be able to query them, this is something that you can get going really quickly and you can get really good results. Anyway, let me know in the comments if you're interested in more stuff about Rag. I haven't done rag stuff for ages. I've just been returning to it recently for some work stuff. And it really is one of the killer applications of using LLM today, especially things like Agentic Rag. And you could imagine using this to have an agent spin up its own rag as it goes out and searches and finds documents on the internet. It can just upload these and make its own rag, which it can then use to do questioning by itself later on. Anyway, as always, let me know in the comments what you think. If you found the video useful, please click like and subscribe, and I will talk to you in the next video.",
  "youtube_metadata": {
    "source": "exported_from_qdrant",
    "original_collection": "cached_content"
  },
  "llm_outputs": [
    {
      "output_type": "tags",
      "output_value": "{\n  \"video_title\": \"Gemini API file search tool and rag system overview\",\n  \"tags\": [\"retrieval-augmented-generation\", \"vector-store\", \"embeddings\", \"prompt-engineering\"],\n  \"summary\": \"An overview of Gemini API's file search tool for building a retrieval-augmented generation workflow with a vector store, embeddings, and grounded citations.\"\n}",
      "generated_at": "2025-11-09T19:23:28.513743",
      "model": "unknown",
      "cost_usd": null,
      "prompt_tokens": null,
      "completion_tokens": null
    }
  ],
  "processing_history": [
    {
      "version": "v1_full_embed",
      "processed_at": "2025-11-09T19:23:28.525084",
      "collection_name": "cached_content",
      "notes": "Migrated from Qdrant"
    }
  ]
}